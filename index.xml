<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zen 3.1</title>
    <link>http://zhen.org/</link>
    <description>Recent content on Zen 3.1</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Jian Zhen. All Rights Reserved.</copyright>
    <lastBuildDate>Sun, 14 Feb 2016 10:57:56 -0800</lastBuildDate>
    <atom:link href="http://zhen.org/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Modern App Developer and An Old-Timer System Developer Walk Into a Bar</title>
      <link>http://zhen.org/blog/two-developers-walk-into-a-bar/</link>
      <pubDate>Sun, 14 Feb 2016 10:57:56 -0800</pubDate>
      
      <guid>http://zhen.org/blog/two-developers-walk-into-a-bar/</guid>
      <description>

&lt;p&gt;&lt;i class=&#34;fa fa-heart&#34;&gt;&lt;/i&gt; &lt;i class=&#34;fa fa-heart&#34;&gt;&lt;/i&gt; Happy Valentine&amp;rsquo;s Day! &lt;i class=&#34;fa fa-heart&#34;&gt;&lt;/i&gt; &lt;i class=&#34;fa fa-heart&#34;&gt;&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;A modern app developer and an old-timer system developer walk into a bar. They had a couple drinks and started talking about the current state of security on the Internet. In a flash of genius, they both decided it would be useful to map the Internet and see what IPs have vulnerable ports open. After some discussion, they decided on the following&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;They will port scan all of the IPv4 address (2^32=4,294,967,296) on a monthly basis&lt;/li&gt;
&lt;li&gt;They will focus on a total of 20 ports including some well-known ports such as FTP (20, 21), telnet (23), ssh (22), SMTP (25), etc&lt;/li&gt;
&lt;li&gt;They will use nmap to scan the IPs and ports&lt;/li&gt;
&lt;li&gt;They need to store the port states as &lt;a href=&#34;https://nmap.org/book/man-port-scanning-basics.html&#34;&gt;open, closed, filtered, unfiltered, openfiltered and closedfiltered&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;They also need to store whether the host is up or down. One of the following two conditions must be met for a host to be &amp;ldquo;up&amp;rdquo;:

&lt;ul&gt;
&lt;li&gt;If the host has any of the ports open, it would be considered up&lt;/li&gt;
&lt;li&gt;If the host responds to ping, it would be considered up&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;They will store the results so they can post process to generate reports, e.g.,

&lt;ul&gt;
&lt;li&gt;Count the Number of &amp;ldquo;Up&amp;rdquo; Hosts&lt;/li&gt;
&lt;li&gt;Determine the Up/Down State of a Specific Host&lt;/li&gt;
&lt;li&gt;Determine Which Hosts are &amp;ldquo;Up&amp;rdquo; in a Particular /24 Subnet&lt;/li&gt;
&lt;li&gt;Count the Number of Hosts That Have Each of the Ports Open&lt;/li&gt;
&lt;li&gt;How Many Total Hosts Were Seen as &amp;ldquo;Up&amp;rdquo; in the Past 3 Months?&lt;/li&gt;
&lt;li&gt;How Many Hosts Changed State This Month (was &amp;ldquo;up&amp;rdquo; but now &amp;ldquo;down&amp;rdquo;, or was &amp;ldquo;down&amp;rdquo; but now &amp;ldquo;up&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;How Many Hosts Were &amp;ldquo;Down&amp;rdquo; Last Month But Now It&amp;rsquo;s &amp;ldquo;Up&amp;rdquo;&lt;/li&gt;
&lt;li&gt;How Many Hosts Were &amp;ldquo;Up&amp;rdquo; Last Month But Now It&amp;rsquo;s &amp;ldquo;Down&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;modern-app-developer-vs-old-timer-developer:81495eb0cd198863461f6e4666c91616&#34;&gt;Modern App Developer vs Old-Timer Developer&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s assume 300 million IPs are up, and has an average of 3 ports open.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;How would you architect this?&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Which approach is faster for each of these tasks?&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Which approach is easier to extend, e.g., add more ports?&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Which approach is more resource (cpu, memory, disk) intensive?&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Disclaimer: I don&amp;rsquo;t know ElasticSearch all that well, so feel free to correct me on any of the following.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;choose-a-language:81495eb0cd198863461f6e4666c91616&#34;&gt;Choose a Language&lt;/h3&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I will use Python. It&amp;rsquo;s quick to get started and easy to understaind/maintain.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I will use Go. It&amp;rsquo;s fast, performant, and easy to understand/maintain!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;store-the-host-and-port-states:81495eb0cd198863461f6e4666c91616&#34;&gt;Store the Host and Port States&lt;/h3&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I will use JSON! It&amp;rsquo;s human-readable, easy to parse (i.e., built in libraries), and everyone knows it!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;{
	&amp;quot;ip&amp;quot;: &amp;quot;10.1.1.1&amp;quot;,
	&amp;quot;state&amp;quot;: &amp;quot;up&amp;quot;,
	&amp;quot;ports&amp;quot;: {
		&amp;quot;20&amp;quot;: &amp;quot;closed&amp;quot;,
		&amp;quot;21&amp;quot;: &amp;quot;closed&amp;quot;,
		&amp;quot;22&amp;quot;: &amp;quot;open&amp;quot;,
		&amp;quot;23&amp;quot;: &amp;quot;closed&amp;quot;,
		.
		.
		.
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;For each host, I will need approximately 400 bytes to represent the host, the up/down state and the 20 port states.&lt;/p&gt;

&lt;p&gt;For 300 million IPs, it will take me about 112GB of space to store all host and port states.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer System Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I will use one bit array (memory mapped file) to store the host state, with 1 bit per host. If the bit is 1, then the host up; if it&amp;rsquo;s 0, then the host is down.&lt;/p&gt;

&lt;p&gt;Given there are 2^32 IPv4 addresses, the bit array will be 2^32 / 8=536,870,912 or 512MBs&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t need to store the IP address separately since the IPv4 address will convert into a number, which can then be used to index into the bit array.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;I will then use a second bit array (memory mapped file) to store the port states. Given there are 6 port states, I will use 3 bits to represent each port state, and 60 bits to represent the 20 port states. I will basically use one uint64 to represent the port states for each host.&lt;/p&gt;

&lt;p&gt;For all 4B IPs, I will need approximately 32GB of space to store the port states. Together, it will take me about 33GB of space to store all host and port states.&lt;/p&gt;

&lt;p&gt;I can probably use &lt;a href=&#34;http://localhost:1313/blog/bitmap-compression-using-ewah-in-go/&#34;&gt;EWAH bitmap compression&lt;/a&gt;
 to gain some space efficiency, but let&amp;rsquo;s assume we are not compressing for now. Also if I do EWAH bitmap compression, I may lose out on the ability to do population counting (see below).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;count-the-number-of-up-hosts:81495eb0cd198863461f6e4666c91616&#34;&gt;Count the Number of &amp;ldquo;Up&amp;rdquo; Hosts&lt;/h3&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a big data problem. Let&amp;rsquo;s use Hadoop!&lt;/p&gt;

&lt;p&gt;I will write a map/reduce hadoop job to process all 300 million host JSON results (documents), and count all the IPs that are &amp;ldquo;up&amp;rdquo;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Maybe this is a search problem. Let&amp;rsquo;s use ElasticSearch!&lt;/p&gt;

&lt;p&gt;I will index all 300M JSON documents with ElasticSearch (ES) on the &amp;ldquo;state&amp;rdquo; field. Then I can just run a query that counts the results of the search where &amp;ldquo;state&amp;rdquo; is &amp;ldquo;up&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;I do realize there&amp;rsquo;s additional storage required for the ES index. Let&amp;rsquo;s assume it&amp;rsquo;s &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;8&lt;/sub&gt; of the original document sizes. This means there&amp;rsquo;s possibly another 14GB of index data, bringing the total to 126GB.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer System Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a &lt;a href=&#34;https://github.com/zhenjl/bitmap/blob/master/ewah/bitcounter.go#L69&#34;&gt;bit counting&lt;/a&gt;, or &lt;em&gt;popcount()&lt;/em&gt;, problem. It&amp;rsquo;s just simple math. I can iterate through the array of uint64&amp;rsquo;s (~8.4M uint64&amp;rsquo;s), count the bits for each, and add them up!&lt;/p&gt;

&lt;p&gt;I can also split the work by creating multiple goroutines (assuming Go), similar to map/reduce, to gain faster calculation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;determine-the-up-down-state-of-a-specific-host:81495eb0cd198863461f6e4666c91616&#34;&gt;Determine the Up/Down State of a Specific Host&lt;/h3&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I know, this is a search problem. Let&amp;rsquo;s use ElasticSearch!&lt;/p&gt;

&lt;p&gt;I will have ElasticSearch index the &amp;ldquo;ip&amp;rdquo; field, in addition to the &amp;ldquo;state&amp;rdquo; field from earlier. Then for any IP, I can search for the document where &amp;ldquo;ip&amp;rdquo; equals the requested IP. From that document, I can then find the value of the &amp;ldquo;state&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer System Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This should be easy. I just need to index into the bit array using the integer value of the IPv4, and find out if the bit value is 1 or 0.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;determine-which-hosts-are-up-in-a-particular-24-subnet:81495eb0cd198863461f6e4666c91616&#34;&gt;Determine Which Hosts are &amp;ldquo;Up&amp;rdquo; in a Particular /24 Subnet&lt;/h3&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is similar to searching for a single IP. I will search for documents where IP is in the subnet (using CIDR notation search in ES) AND the &amp;ldquo;state&amp;rdquo; is &amp;ldquo;up&amp;rdquo;. This will return a list of search results which I can then iterate and retrieve the host IP.&lt;/p&gt;

&lt;p&gt;Or&lt;/p&gt;

&lt;p&gt;This is a map reduce job that I can write to process the 300 million JSON documents and return all the host IPs that are &amp;ldquo;up&amp;rdquo; in that /24 subnet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer System Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is just another bit iteration problem. I will use the first IP address of the subnet to determine where in the bit array I should start. Then I calculate the number of IPs in that subnet. From there, I just iterate through the bit array and for every bit that&amp;rsquo;s 1, I convert the index of that bit into an IPv4 address and add to the list of &amp;ldquo;Up&amp;rdquo; hosts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;count-the-number-of-hosts-that-have-each-of-the-ports-open:81495eb0cd198863461f6e4666c91616&#34;&gt;Count the Number of Hosts That Have Each of the Ports Open&lt;/h3&gt;

&lt;p&gt;For example, the report could simply be:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;20: 3,023
21: 3,023
22: 1,203,840
.
.
.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a big data problem. I will use Hadoop and write a map/reduce job. The job will return the host count for each of the port.&lt;/p&gt;

&lt;p&gt;This can probably also be done with ElasticSearch. It would require the port state to be index, which will increase the index size. I can then count the results for the search for ports 22 = &amp;ldquo;open&amp;rdquo;, and repeat for each port.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer System Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a simple counting problem. I will walk through the host state bit array, and for every host that&amp;rsquo;s up, I will use the bit index to index into the port state uint64 array and get the uint64 that represents all the port states for that host. I will then walk through each of the 3-bit bundles for the ports, and add up the counts if the port is &amp;ldquo;open&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Again, this can easily be paralleized by creating multiple goroutines (assuming Go).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;how-many-total-hosts-were-seen-as-up-in-the-past-3-months:81495eb0cd198863461f6e4666c91616&#34;&gt;How Many Total Hosts Were Seen as &amp;ldquo;Up&amp;rdquo; in the Past 3 Months&lt;/h3&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I can retrieve the &amp;ldquo;Up&amp;rdquo; host list for each month, and then go through all 3 lists and dedup into a single list. This would require quite a bit of processing and iteration.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer System Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I can perform a simple AND operation on the 3 monthly bit arrays, and then count the number of &amp;ldquo;1&amp;rdquo; bits.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;how-many-hosts-changed-state-this-month-was-up-but-now-down-or-was-down-but-now-up:81495eb0cd198863461f6e4666c91616&#34;&gt;How Many Hosts Changed State This Month (was &amp;ldquo;up&amp;rdquo; but now &amp;ldquo;down&amp;rdquo;, or was &amp;ldquo;down&amp;rdquo; but now &amp;ldquo;up&amp;rdquo;)&lt;/h3&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Hm&amp;hellip;I am not sure how to do that easily. I guess I can just iterate through last month&amp;rsquo;s hosts, and for each host check to see if it changed state this month. Then for each host that I haven&amp;rsquo;t checked this month, iterate and check that list against last month&amp;rsquo;s result.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer System Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I can perform a simple XOR operation on the bit arrays from this and last month. Then count the number of &amp;ldquo;1&amp;rdquo; bits of the resulting bit array.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;how-many-hosts-were-up-last-month-but-now-it-s-down:81495eb0cd198863461f6e4666c91616&#34;&gt;How Many Hosts Were &amp;ldquo;Up&amp;rdquo; Last Month But Now It&amp;rsquo;s &amp;ldquo;Down&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I can retrieve the &amp;ldquo;Up&amp;rdquo; hosts from last month from ES, then for each &amp;ldquo;Up&amp;rdquo; host, search for it with the state equals to &amp;ldquo;Down&amp;rdquo; this month, and accumulate the results.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer System Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I can perform this opeartion: &lt;code&gt;(this_month XOR last_month) XOR last_month&lt;/code&gt;. This will return a bit array that has the bit set if the host was &amp;ldquo;up&amp;rdquo; last month but now it&amp;rsquo;s &amp;ldquo;down&amp;rdquo;. Then count the number of &amp;ldquo;1&amp;rdquo; bits of the resulting bit array.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;how-many-hosts-were-down-last-month-but-now-it-s-up:81495eb0cd198863461f6e4666c91616&#34;&gt;How Many Hosts Were &amp;ldquo;Down&amp;rdquo; Last Month But Now It&amp;rsquo;s &amp;ldquo;Up&amp;rdquo;&lt;/h3&gt;

&lt;p&gt;Modern App Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I can retrieve the &amp;ldquo;Down&amp;rdquo; hosts from last month from ES, then for each &amp;ldquo;Down&amp;rdquo; host, search for it with the state equals to &amp;ldquo;Up&amp;rdquo; this month, and accumulate the results.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Old-Timer System Developer:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I can perform this opeartion: &lt;code&gt;(this_month XOR last_month) XOR this_month&lt;/code&gt;. This will return a bit array that has the bit set if the host was &amp;ldquo;down&amp;rdquo; last month but now it&amp;rsquo;s &amp;ldquo;up&amp;rdquo;. Then count the number of &amp;ldquo;1&amp;rdquo; bits of the resulting bit array.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>2016: Analyzing Security Trends Using RSA Exhibitor Descriptions</title>
      <link>http://zhen.org/blog/2016-analyzing-security-trends-using-rsa-exhibitor-descriptions/</link>
      <pubDate>Fri, 05 Feb 2016 10:57:56 -0800</pubDate>
      
      <guid>http://zhen.org/blog/2016-analyzing-security-trends-using-rsa-exhibitor-descriptions/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;The data used for this post is available &lt;a href=&#34;https://github.com/zhenjl/rsaconf&#34;&gt;here&lt;/a&gt;. A word of warning, I only have complete data set for 2014-2016. For 2008-2013, I have what I consider to be representative samples. So please take the result set with a big bucket of salt.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Continuing my &lt;a href=&#34;http://zhen.org/blog/analyzing-security-trends-using-rsa-exhibitor-descriptions/&#34;&gt;analysis from last year&lt;/a&gt;, this post analyzes the exhibitors&amp;rsquo; descriptions from the annual security conference, &lt;a href=&#34;http://www.rsaconference.com/events/us16&#34;&gt;RSA 2016&lt;/a&gt;. Intuitively, the vendor marketing messages should have a high degree of correlation to what customers care about, even if the messages trail the actual pain points slightly.&lt;/p&gt;

&lt;p&gt;Some interesting findings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The word &lt;em&gt;hunt&lt;/em&gt; has appeared for the first time since 2008. It only appeared 6 times and ranked pretty low, but that&amp;rsquo;s a first nonetheless.&lt;/li&gt;
&lt;li&gt;The word &lt;em&gt;iot&lt;/em&gt; jumped 881 spots to 192 in 2016, after showing up for the first time in 2015. This may indicate the strong interest in IoT security.&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s no mention of &lt;em&gt;docker&lt;/em&gt; in any of the years, and only 5 mentions of container in 2016. This is a bit of a surprise given the noise docker/container is making. This is likely due to most customers care more about management for new technologies than security.&lt;/li&gt;
&lt;li&gt;The word &lt;em&gt;firewall&lt;/em&gt; dropped 151 spots in 2016. I want to speculate that it is due to the dissolving of perimeters, but I can&amp;rsquo;t be sure of that.&lt;/li&gt;
&lt;li&gt;As much as noise as &lt;em&gt;blockchain&lt;/em&gt; is making, there&amp;rsquo;s no mention of the word.&lt;/li&gt;
&lt;li&gt;The word &lt;em&gt;behavior&lt;/em&gt; (as in behavioral analysis) has also gained drastically over the past few years, going from #370 in 2012 to #78 in 2016.&lt;/li&gt;
&lt;li&gt;See other findings below.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;top-words:bb7656c12180a6a78a8c54c60f6a51ce&#34;&gt;Top Words&lt;/h3&gt;

&lt;p&gt;The top words that vendors use to describe themselves haven&amp;rsquo;t changed much. The following table shows the top 10 words used in RSA conference exhibitor descriptions since 2008. You can find the complete word list &lt;a href=&#34;https://github.com/zhenjl/rsaconf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;#&lt;/th&gt;
&lt;th&gt;2008&lt;/th&gt;
&lt;th&gt;2009&lt;/th&gt;
&lt;th&gt;2010&lt;/th&gt;
&lt;th&gt;2011&lt;/th&gt;
&lt;th&gt;2012&lt;/th&gt;
&lt;th&gt;2013&lt;/th&gt;
&lt;th&gt;2014&lt;/th&gt;
&lt;th&gt;2015&lt;/th&gt;
&lt;th&gt;2016&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;protect&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;threat&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;information&lt;/td&gt;
&lt;td&gt;more&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;protect&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;company&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;information&lt;/td&gt;
&lt;td&gt;software&lt;/td&gt;
&lt;td&gt;software&lt;/td&gt;
&lt;td&gt;protect&lt;/td&gt;
&lt;td&gt;threat&lt;/td&gt;
&lt;td&gt;protect&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td&gt;product&lt;/td&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;td&gt;organization&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;information&lt;/td&gt;
&lt;td&gt;threat&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td&gt;technology&lt;/td&gt;
&lt;td&gt;software&lt;/td&gt;
&lt;td&gt;information&lt;/td&gt;
&lt;td&gt;technology&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td&gt;application&lt;/td&gt;
&lt;td&gt;busy&lt;/td&gt;
&lt;td&gt;risk&lt;/td&gt;
&lt;td&gt;product&lt;/td&gt;
&lt;td&gt;more&lt;/td&gt;
&lt;td&gt;customer&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here&amp;rsquo;s a word cloud that shows the &lt;a href=&#34;http://zhen.org/images/rsaconf/rsa2016.png&#34;&gt;2016&lt;/a&gt; top words. You can also find word clouds for
&lt;a href=&#34;http://zhen.org/images/rsaconf/rsa2008.png&#34;&gt;2008&lt;/a&gt;,
&lt;a href=&#34;http://zhen.org/images/rsaconf/rsa2009.png&#34;&gt;2009&lt;/a&gt;,
&lt;a href=&#34;http://zhen.org/images/rsaconf/rsa2010.png&#34;&gt;2010&lt;/a&gt;,
&lt;a href=&#34;http://zhen.org/images/rsaconf/rsa2011.png&#34;&gt;2011&lt;/a&gt;,
&lt;a href=&#34;http://zhen.org/images/rsaconf/rsa2012.png&#34;&gt;2012&lt;/a&gt;,
&lt;a href=&#34;http://zhen.org/images/rsaconf/rsa2013.png&#34;&gt;2013&lt;/a&gt;,
&lt;a href=&#34;http://zhen.org/images/rsaconf/rsa2014.png&#34;&gt;2014&lt;/a&gt;,
&lt;a href=&#34;http://zhen.org/images/rsaconf/rsa2015.png&#34;&gt;2015&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/rsa2016.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;endpoint-vs-network:bb7656c12180a6a78a8c54c60f6a51ce&#34;&gt;Endpoint vs. Network&lt;/h3&gt;

&lt;p&gt;While the word &lt;em&gt;network&lt;/em&gt; has mostly maintained its top 10 position (except 2013 when it fell to #11), the big gainer is the word &lt;em&gt;endpoint&lt;/em&gt;, which improved drastically from #266 in 2012 to 2016&amp;rsquo;s #50. This may indicate that enterprises are much more accepting of endpoint technologies.&lt;/p&gt;

&lt;p&gt;I also speculate that there might be a correlation between the increase in &lt;em&gt;cloud&lt;/em&gt; and the increase in &lt;em&gt;endpoint&lt;/em&gt;. As the perimeters get dissolved due to the move to cloud, it&amp;rsquo;s much more difficult to use network security technologies. So enterprises are looking at &lt;em&gt;endpoint&lt;/em&gt; technologies to secure their critical assets.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/2016/endpoint-network.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;compliance-vs-threat:bb7656c12180a6a78a8c54c60f6a51ce&#34;&gt;Compliance vs. Threat&lt;/h3&gt;

&lt;p&gt;Not surprisingly, the use of the word &lt;em&gt;compliance&lt;/em&gt; continues to go down, and the word &lt;em&gt;threat&lt;/em&gt; continues to go up.&lt;/p&gt;

&lt;p&gt;The number of mentions for &lt;em&gt;threat intelligence&lt;/em&gt; remained at 22 for both 2015 and 2016, after jumping from 12 in 2014.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/2016/compliance-threat.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;mobile-cloud-virtual-and-iot:bb7656c12180a6a78a8c54c60f6a51ce&#34;&gt;Mobile, Cloud, Virtual and IoT&lt;/h3&gt;

&lt;p&gt;While the words &lt;em&gt;mobile&lt;/em&gt; and &lt;em&gt;cloud&lt;/em&gt; maintained their relative positioning in 2016, we can also see &lt;em&gt;virtual&lt;/em&gt; continues its slight downward trend.&lt;/p&gt;

&lt;p&gt;Interestingly, the word &lt;em&gt;iot&lt;/em&gt; made a big jump, going from position #1073 in 2015 to #193 in 2016. This potentially indicates a strong interest in security for internet of things. In general, the IoT space has seen some major activities, including &lt;a href=&#34;http://techcrunch.com/2016/02/03/cisco-buys-jasper-technologies-for-1-4-billion/&#34;&gt;Cisco&amp;rsquo;s recent acquisition of Jasper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/2016/mobile-cloud.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;cyber-malware-and-phishing:bb7656c12180a6a78a8c54c60f6a51ce&#34;&gt;Cyber, Malware and Phishing&lt;/h3&gt;

&lt;p&gt;The word &lt;em&gt;cyber&lt;/em&gt; continues to gain popularity in the past 4 years; however, the word &lt;em&gt;malware&lt;/em&gt; has fell below the top 100, a position it maintained since 2010.&lt;/p&gt;

&lt;p&gt;The word &lt;em&gt;phishing&lt;/em&gt; made drastic gains since 2014, jumping from #807 to #193 in 2016. This may indicate that enterprises are seeing more attacks from phishing, and vendors are targeting that specific attack vector.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/2016/cyber-malware.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;it-s-all-about-behavior:bb7656c12180a6a78a8c54c60f6a51ce&#34;&gt;It&amp;rsquo;s all about Behavior!&lt;/h3&gt;

&lt;p&gt;The word &lt;em&gt;behavior&lt;/em&gt; (as in behavioral analysis) has also gained drastically over the past few years, going from #370 in 2012 to #78 in 2016.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/2016/behavior.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;credits:bb7656c12180a6a78a8c54c60f6a51ce&#34;&gt;Credits&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The word clouds and word rankings are generated using &lt;a href=&#34;http://timdream.org/wordcloud&#34;&gt;Word Cloud&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The actual vendor descriptions are gathered from the RSA web site as well as press releases from Business Wire and others.&lt;/li&gt;
&lt;li&gt;Charts are generated using Excel, which continues to be one of the best friends for data analysts (not that I consider myself one).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Installing Windows 7 on Macbook Late 2008</title>
      <link>http://zhen.org/blog/installing-windows7-on-macbook-late-2008/</link>
      <pubDate>Mon, 20 Apr 2015 21:32:57 -0700</pubDate>
      
      <guid>http://zhen.org/blog/installing-windows7-on-macbook-late-2008/</guid>
      <description>&lt;p&gt;Over the weekend I wanted to install Windows in a bootcamp partition so the kids can use it to do their Chinese homework. The Chinese homework CD unfortunately only works in Windows so I had no choice!! I guess I could have taken other routes, like installing Windows in a VM or something, but I figure that Mac has this awesome tool called bootcamp, why not use that?&lt;/p&gt;

&lt;p&gt;Well, how wrong I was! I went through a whole day of head-scratching, temper-inducing, word-cussing, USB-swapping and machine-rebooting exercise of getting Windows installed in the bootcamp partition. I almost went as far as buying a replacement superdrive for the macbook, but at the end I finally was able to get Windows 7 onto the Macbook.&lt;/p&gt;

&lt;p&gt;To start, my laptop is a Macbook, Aluminum, Late 2008 (MB467LL/A) with a busted optical drive (superdrive). I originally had Mavericks running on it but before this exercise I wiped it clean and installed Yosemite on it. Because the optical drive is busted, I cannot use the Windows 7 DVD, so I had to do this using a USB flash drive.&lt;/p&gt;

&lt;p&gt;Below are the steps I took to make this work. I can&amp;rsquo;t guarantee that these steps will work for you, but it&amp;rsquo;s probably good as a reference. Having seen a ton of articles on the problems people had with bootcamp, I hope no one has to go through the troubles I went through.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It took me a while to figure this out (after reading numerous online posts), if your Mac has an optical drive, Boot Camp Assistant will NOT create a USB flash drive-based install disk. The only way to trick the system to do that is to do the following: (Though it turns out at the end that this step is quite useless, since the USB install disk created by Boot Camp Assistant couldn&amp;rsquo;t boot! So you could really skip this step.)

&lt;ol&gt;
&lt;li&gt;Modify Boot Camp Assistant&amp;rsquo;s Info.plist as described &lt;a href=&#34;http://forums.macrumors.com/showthread.php?t=1488322&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;After the modification, you need to resign Boot Camp Assistant, or else it will keep crashing. To do that, following the instructions &lt;a href=&#34;https://discussions.apple.com/thread/5479879?start=30&#34;&gt;here&lt;/a&gt;. For the impatient, run the command &lt;code&gt;sudo codesign -fs - /Applications/Utilities/Boot\ Camp\ Assistant.app&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Start &amp;ldquo;Boot Camp Assistant&amp;rdquo;, and select the options &amp;ldquo;Download the latest Windows Support&amp;rdquo;, and &amp;ldquo;Install Windows 7 or later versions&amp;rdquo;.

&lt;ul&gt;
&lt;li&gt;Note I am not selecting the option to create a Windows install disk. It turned out the USB install disk didn&amp;rsquo;t boot. I keep getting the &amp;ldquo;non-system disk, press any key to continue&amp;rdquo; error, and basically that&amp;rsquo;s the end.&lt;/li&gt;
&lt;li&gt;In any case, these two tasks should download the bootcamp drivers onto a USB drive, and also partition the Mac&amp;rsquo;s HD into two partitions. One of the parititions is the BOOTCAMP partition, which will be used to install Windows 7.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Once that&amp;rsquo;s done, I needed to create a bootable Windows 7 USB Flash drive.

&lt;ul&gt;
&lt;li&gt;If you search the web, you will find that most people run into two problems. The first is the bootcamp-created flash drive giving the &amp;ldquo;non-system disk&amp;rdquo; error, and the second is the boot up hangs with a blank screen and a flash underscore cursor at the top left corner. I&amp;rsquo;ve ran into both. You will also find some articles that explain how to make the flash drives bootable using fdisk, but that didn&amp;rsquo;t work for me either.&lt;/li&gt;
&lt;li&gt;Finally I found &lt;a href=&#34;http://www.tonymacx86.com/windows/97075-windows-7-usb-install-stuck-black-screen.html&#34;&gt;a post online&lt;/a&gt; that pointed to the &lt;a href=&#34;http://wudt.codeplex.com/&#34;&gt;Windows USB/DVD Download Tool&lt;/a&gt;. It&amp;rsquo;s a Windows program that can create a bootable USB flash drive from a Windows 7 or 8 ISO file.&lt;/li&gt;
&lt;li&gt;Note though, not all the USB flash drives are created equal. The PNY 16GB drive I used didn&amp;rsquo;t work. WUDT ended with an error that says it couldn&amp;rsquo;t run bootsect to create the boot sectors on the flash drive. The one that worked for me was Kingston Data Traveler 4GB.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Now that I have the bootable USB flash drive, I plugged that into the Mac and started it up. This time the installation process got started.&lt;/li&gt;
&lt;li&gt;When Boot Camp Assistant created the BOOTCAMP partition, it did not format it to NTFS. So the first thing I noticed was that when I select the BOOTCAMP partition, the installer said it cannot be used because it&amp;rsquo;s not NTFS.

&lt;ul&gt;
&lt;li&gt;The option to format the partition is not immediately obvious, but I had to click on &amp;ldquo;Drive options (advanced)&amp;rdquo; and select the option to format the partition.&lt;/li&gt;
&lt;li&gt;Once that&amp;rsquo;s done, I encountered another error that says the drive may not be bootable and I need to change the BIOS setting. Yeah at this point I was pretty ticked and the computer heard a few choice words from me. Doesn&amp;rsquo;t matter what I do it doesn&amp;rsquo;t seem to let me pass this point.&lt;/li&gt;
&lt;li&gt;I did a bunch more readings and research, but nothing seem to have worked. I finally decided to turn the computer off and come back to it. Magically it worked the second time I tried to install it. I was no longer getting the non-bootable disk error. My guess is that after the NTFS formatting, the installer needs to be completely restarted.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In any case, at this point, it was fairly smooth sailing. The installation process took a bit of time but overall everything seemed to have worked.&lt;/li&gt;
&lt;li&gt;After the installation, I plugged int the bootcamp flash drive with the WindowsSupport files, and installed them.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am still not a 100% yet. The trackpad still doesn&amp;rsquo;t behave like when it&amp;rsquo;s on the Mac. For example, I can&amp;rsquo;t use the two finger drag to scroll the windows, and for the life of me, I cannot figure out how to easily (and correctly) set the brightness of display. But at least now I have a working Windows 7 laptop!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing Security Trends Using RSA Exhibitor Descriptions</title>
      <link>http://zhen.org/blog/analyzing-security-trends-using-rsa-exhibitor-descriptions/</link>
      <pubDate>Sun, 22 Mar 2015 09:57:56 -0800</pubDate>
      
      <guid>http://zhen.org/blog/analyzing-security-trends-using-rsa-exhibitor-descriptions/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;The data used for this post is available &lt;a href=&#34;https://github.com/topclouds/rsaconf&#34;&gt;here&lt;/a&gt;. A word of warning, I only have complete data set for 2014 and 2015. For 2008-2013, I have what I consider to be representative samples. So please take the result set with a big bucket of salt.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;After going through this analysis, the big question I wonder out loud is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;How can vendors differentiate from each other and stand above the crowd when everyone is using the same words to describe themselves?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;The annual security conference, &lt;a href=&#34;http://www.rsaconference.com/events/us15&#34;&gt;RSA 2015&lt;/a&gt;, is right around the corner. Close to 30,000 attendees will descend into &lt;a href=&#34;http://moscone.com&#34;&gt;San Francisco Moscone Center&lt;/a&gt; to attend 400+ sessions, listen to 600+ speakers and talk to close to 600 vendors and exhibitors.&lt;/p&gt;

&lt;p&gt;For me, the most interesting aspect of RSA is walking the expo floor, and listening to how vendors describe their products. Intuitively, the vendor marketing messages should have a high degree of correlation to what customers care about, even if the messages trail the actual pain points slightly.&lt;/p&gt;

&lt;p&gt;This post highlights some of the unsurprising findings from analyzing 8 years worth of RSA Conference exihibitor descriptions.&lt;/p&gt;

&lt;p&gt;It is interesting how almost all vendor descriptions use the same set of words to describe themselves, and these words mostly haven&amp;rsquo;t changed over the past 8 years. For example, the following table shows the top 10 words used in RSA conference exhibitor descriptions for the past 8 years. You can find the complete word list at &amp;hellip;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;right&#34;&gt;#&lt;/th&gt;
&lt;th&gt;2008&lt;/th&gt;
&lt;th&gt;2009&lt;/th&gt;
&lt;th&gt;2010&lt;/th&gt;
&lt;th&gt;2011&lt;/th&gt;
&lt;th&gt;2012&lt;/th&gt;
&lt;th&gt;2013&lt;/th&gt;
&lt;th&gt;2014&lt;/th&gt;
&lt;th&gt;2015&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;td&gt;secure&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;td&gt;solution&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;protect&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;network&lt;/td&gt;
&lt;td&gt;provide&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;information&lt;/td&gt;
&lt;td&gt;more&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;protect&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;company&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;information&lt;/td&gt;
&lt;td&gt;software&lt;/td&gt;
&lt;td&gt;software&lt;/td&gt;
&lt;td&gt;protect&lt;/td&gt;
&lt;td&gt;threat&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td&gt;product&lt;/td&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;td&gt;organization&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;information&lt;/td&gt;
&lt;td&gt;threat&lt;/td&gt;
&lt;td&gt;manage&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;9&lt;/td&gt;
&lt;td&gt;technology&lt;/td&gt;
&lt;td&gt;software&lt;/td&gt;
&lt;td&gt;information&lt;/td&gt;
&lt;td&gt;technology&lt;/td&gt;
&lt;td&gt;data&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td&gt;application&lt;/td&gt;
&lt;td&gt;busy&lt;/td&gt;
&lt;td&gt;risk&lt;/td&gt;
&lt;td&gt;product&lt;/td&gt;
&lt;td&gt;more&lt;/td&gt;
&lt;td&gt;customer&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;td&gt;enterprise&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Here&amp;rsquo;s a word cloud that shows the &lt;a href=&#34;https://github.com/topclouds/rsaconf/blob/master/rsa2015.png&#34;&gt;2015&lt;/a&gt; top words. You can also find word clouds for
&lt;a href=&#34;https://github.com/topclouds/rsaconf/blob/master/rsa2008.png&#34;&gt;2008&lt;/a&gt;,
&lt;a href=&#34;https://github.com/topclouds/rsaconf/blob/master/rsa2009.png&#34;&gt;2009&lt;/a&gt;,
&lt;a href=&#34;https://github.com/topclouds/rsaconf/blob/master/rsa2010.png&#34;&gt;2010&lt;/a&gt;,
&lt;a href=&#34;https://github.com/topclouds/rsaconf/blob/master/rsa2011.png&#34;&gt;2011&lt;/a&gt;,
&lt;a href=&#34;https://github.com/topclouds/rsaconf/blob/master/rsa2012.png&#34;&gt;2012&lt;/a&gt;,
&lt;a href=&#34;https://github.com/topclouds/rsaconf/blob/master/rsa2013.png&#34;&gt;2013&lt;/a&gt;,
&lt;a href=&#34;https://github.com/topclouds/rsaconf/blob/master/rsa2014.png&#34;&gt;2014&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/rsa2015.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;compliance-down-threats-up:b082a5c20b5449f4372a8955a4b5d0dd&#34;&gt;Compliance Down, Threats Up&lt;/h3&gt;

&lt;p&gt;While the macro trend has not changed dramatically for the exhibitor descriptions, there have been some micro trends. Here are a couple of examples.&lt;/p&gt;

&lt;p&gt;First, the use of the word &lt;em&gt;compliance&lt;/em&gt; has gone down over the years, while the word &lt;em&gt;threat&lt;/em&gt; has gone up. After 2013, they changed places with each other.&lt;/p&gt;

&lt;p&gt;This finding is probably not surprising. At the end of 2013, one of the biggest breaches, Target, happened. And over the next two years we&amp;rsquo;ve seen major breaches of Sony, Anthem, Home Depot, Premera and many others. Threats to both the corporate infrastructure as well as top executive jobs (just ask &lt;a href=&#34;http://www.forbes.com/sites/ericbasu/2014/06/15/target-ceo-fired-can-you-be-fired-if-your-company-is-hacked/&#34;&gt;Target’s CEO Gregg Steinhafel&lt;/a&gt;, or &lt;a href=&#34;http://abcnews.go.com/Entertainment/wireStory/sony-chief-amy-pascal-acknowledges-fired-28918607&#34;&gt;Sony&amp;rsquo;s Co-Chairwoman Amy Pascal&lt;/a&gt;) are becoming real. So it seems natural for the marketers to start using the word &lt;em&gt;threat&lt;/em&gt; to highlight their solutions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Compliance&lt;/em&gt; was a big use case in security for many years, and many vendors have leveraged the need for compliance to build their company and revenue pipeline since the mid-2000s. However, use cases can only remain in fashion for so long before customers get sick of hearing about them, and vendors need new ways of selling their wares to customers. So it looks like &lt;em&gt;compliance&lt;/em&gt; is finally out of fashion around 2011 and started declining in exhibitor descriptions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/compliance-threat.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;mobile-and-cloud-up:b082a5c20b5449f4372a8955a4b5d0dd&#34;&gt;Mobile and Cloud Up&lt;/h3&gt;

&lt;p&gt;The words &lt;em&gt;mobile&lt;/em&gt; and &lt;em&gt;cloud&lt;/em&gt; has gained dramatically in rankings over past 8 years. In fact, it&amp;rsquo;s been consistently one of the top words used in the last 4. For anyone who hasn&amp;rsquo;t been hiding under a rock in the past few years, this is completely unsurprising.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;cloud&lt;/em&gt; war started to heat up back in 2009 when most major service providers have felt the Amazon Web Services threat and all wanted to build their own clouds. In fact, I joined VMware in 2009 to build out their emerging cloud infrastructure group to specifically help service providers build their cloud infrastructures. Eventually, in 2011, VMware decided to get into the game and I built the initial product and engineering team that developed what it&amp;rsquo;s now known as vCloud Air (still have no idea why this name is chosen).&lt;/p&gt;

&lt;p&gt;As more and more workloads move to the cloud, requirements for protecting cloud workloads quickly appeared, and vendors natually started to position their products for the cloud. So the rise in &lt;em&gt;cloud&lt;/em&gt; rankings matches what I&amp;rsquo;ve experiened.&lt;/p&gt;

&lt;p&gt;About the same time (2010, 2011 or so), more and more corporations are providing their employees smartphones, and workers are becoming more and more mobile. The need for mobile security became a major requirement, and a whole slueth of mobile security startup came into the scene. So natually the &lt;em&gt;mobile&lt;/em&gt; word rose in rankings.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/mobile-cloud.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;virtual-and-real-time-regaining-ground:b082a5c20b5449f4372a8955a4b5d0dd&#34;&gt;Virtual and Real-Time Regaining Ground&lt;/h3&gt;

&lt;p&gt;The words &lt;em&gt;virtual&lt;/em&gt; and &lt;em&gt;real-time&lt;/em&gt; dropped dramatically in rankings for a couple of years (2010, 2011) but have since regained all the lost ground and more. I have no precise reasons on why that&amp;rsquo;s the case but I have some theories. These theories are probably completely wrong, and if you have better explanations I would love to hear from you.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Virtual&lt;/em&gt; lost to &lt;em&gt;cloud&lt;/em&gt; during that timeframe as every vendor is trying to position their products for the cloud era. However, &lt;em&gt;virtual&lt;/em&gt; infrastructures haven&amp;rsquo;t gone away and in fact continue to experience strong growth. So in the past couple of years, marketers are covering their basis and starting to message both &lt;em&gt;virtual&lt;/em&gt; and &lt;em&gt;cloud&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The drop in rankings for &lt;em&gt;real-time&lt;/em&gt; is potentially due to the peak of &lt;em&gt;compliance&lt;/em&gt; use case, which is usually report-based and does not have &lt;em&gt;real-time&lt;/em&gt; requirements. Also, I suspect another reason is that SIEM, which &lt;em&gt;real-time&lt;/em&gt; is critical, is going out of fashion somewhat due to the high cost of ownership and lack of trust in the tools. However, given the recent rise of &lt;em&gt;threats&lt;/em&gt;, natually &lt;em&gt;real-time&lt;/em&gt; becomes critical again.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/virtual-real-time.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;other-findings:b082a5c20b5449f4372a8955a4b5d0dd&#34;&gt;Other Findings&lt;/h3&gt;

&lt;p&gt;The word &lt;em&gt;cyber&lt;/em&gt; gained huge popularity in the past 3 years, likely due to the U.S. government&amp;rsquo;s focus on &lt;em&gt;cyber&lt;/em&gt; security. The word &lt;em&gt;malware&lt;/em&gt; has been fairly consistently at the top 100 words since 2010.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/cyber-malware.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The words &lt;em&gt;product&lt;/em&gt; and &lt;em&gt;service&lt;/em&gt; switched places in 2013, likely due to the increase in number of security software-as-a-service plays.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/rsaconf/product-service.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;credits:b082a5c20b5449f4372a8955a4b5d0dd&#34;&gt;Credits&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The word clouds and word rankings are generated using &lt;a href=&#34;http://timdream.org/wordcloud&#34;&gt;Word Cloud&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The actual vendor descriptions are gathered from the RSA web site as well as press releases from Business Wire and others.&lt;/li&gt;
&lt;li&gt;Charts are generated using Excel, which continues to be one of the best friends for data analysts (not that I consider myself one).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Papers I Read: 2015 Week 8</title>
      <link>http://zhen.org/blog/papers-i-read-2015-week-8/</link>
      <pubDate>Sun, 22 Feb 2015 19:57:56 -0800</pubDate>
      
      <guid>http://zhen.org/blog/papers-i-read-2015-week-8/</guid>
      <description>

&lt;h3 id=&#34;random-ramblings:29724b4ec2d333ad12e89fd98257dccd&#34;&gt;Random Ramblings&lt;/h3&gt;

&lt;p&gt;Another week, another report of hacks. This time, &lt;a href=&#34;http://securelist.com/blog/research/68732/the-great-bank-robbery-the-carbanak-apt/&#34;&gt;The Great Bank Robbery&lt;/a&gt;, where up to 100 financial institutions have been hit.Total financial losses could be as a high as $1bn. You can download the &lt;a href=&#34;http://25zbkz3k00wn2tp5092n6di7b5k.wpengine.netdna-cdn.com/files/2015/02/Carbanak_APT_eng.pdf&#34;&gt;full report&lt;/a&gt; and learn all about it.&lt;/p&gt;

&lt;p&gt;Sony spent $15M to clean up and remediate their hack. I wonder how much these banks are going to spend on tracing the footsteps of their intruders and trying to figure out exactly where they have gone, what they have done and what they have taken.&lt;/p&gt;

&lt;p&gt;I didn&amp;rsquo;t make much progress this week on either &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; or &lt;a href=&#34;https://github.com/surgemq/surgemq&#34;&gt;surgemq&lt;/a&gt; because of busy work schedule and my son getting sick AGAIN!! But I did merge the few surgemq &lt;a href=&#34;https://github.com/surgemq/surgemq/pulls?q=is%3Apr+is%3Aclosed&#34;&gt;pull requests&lt;/a&gt; that the community has graciously contributed. One of them actually got it tested on Raspberry! That&amp;rsquo;s pretty cool.&lt;/p&gt;

&lt;p&gt;I also did manage to finish up the &lt;a href=&#34;https://github.com/strace/sequence/commit/713979f70d6025308e434205973249aa3138e58e&#34;&gt;experimental json scanner&lt;/a&gt; that I&amp;rsquo;ve been working on for the past couple of weeks. I will write more about it in the next &lt;a href=&#34;http://strace.io/sequence&#34;&gt;sequence&lt;/a&gt; article.&lt;/p&gt;

&lt;p&gt;Actually I am starting to feel a bit overwhelmed by having both projects. Both of them are very interesting and I can see both move forward in very positive ways. Lots of ideas in my head but not enough time to do them. Now that I am getting feature requests, issues and pull requests, I feel even worse because I haven&amp;rsquo;t spent enough time on them. &amp;lt;sigh&amp;gt;&lt;/p&gt;

&lt;h3 id=&#34;papers-i-read:29724b4ec2d333ad12e89fd98257dccd&#34;&gt;Papers I Read&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pdl.cmu.edu/PDL-FTP/HECStorage/git-cercs-12-08.pdf&#34;&gt;Memory-Efficient GroupBy-Aggregate using Compressed Buffer Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Memory is rapidly becoming a precious resource in many data processing environments. This paper introduces
a new data structure called a Compressed Buffer Tree (CBT). Using a combination of buffering, compression,
and lazy aggregation, CBTs can improve the memoryefficiency of the GroupBy-Aggregate abstraction which
forms the basis of many data processing models like MapReduce and databases. We evaluate CBTs in the
context of MapReduce aggregation, and show that CBTs can provide significant advantages over existing hashbased
aggregation techniques: up to 2× less memory and 1.5× the throughput, at the cost of 2.5× CPU.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.usenix.org/system/files/conference/atc14/atc14-paper-hu.pdf&#34;&gt;ELF: Efficient Lightweight Fast Stream Processing at Scale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Stream processing has become a key means for gaining rapid insights from webserver-captured data. Challenges
include how to scale to numerous, concurrently running streaming jobs, to coordinate across those jobs to share
insights, to make online changes to job functions to adapt to new requirements or data characteristics, and for each job, to efficiently operate over different time windows. The ELF stream processing system addresses these new challenges. Implemented over a set of agents enriching the web tier of datacenter systems, ELF obtains scalability by using a decentralized “many masters” architecture where for each job, live data is extracted directly from webservers, and placed into memory-efficient compressed buffer trees (CBTs) for local parsing and temporary storage, followed by subsequent aggregation using shared reducer trees (SRTs) mapped to sets of worker processes. Job masters at the roots of SRTs can dynamically customize worker actions, obtain aggregated results for end user delivery and/or coordinate with other jobs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html&#34;&gt;Is Parallel Programming Hard, And, If So, What Can You Do About It?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not just a paper, it&amp;rsquo;s a whole book w/ 800+ pages.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The purpose of this book is to help you program shared-memory parallel machines without risking your sanity.1 We hope that this book’s design principles will help you avoid at least some parallel-programming pitfalls. That said, you should think of this book as a foundation on which to build, rather than as a completed cathedral. Your mission, if you choose to accept, is to help make further progress in the exciting field of parallel programming—progress that will in time render this book obsolete. Parallel programming is not as hard as some say, and we hope that this book makes your parallel-programming projects easier and more fun.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Papers I Read: 2015 Week 7</title>
      <link>http://zhen.org/blog/papers-i-read-2015-week-7/</link>
      <pubDate>Sun, 15 Feb 2015 19:57:56 -0800</pubDate>
      
      <guid>http://zhen.org/blog/papers-i-read-2015-week-7/</guid>
      <description>

&lt;h2 id=&#34;random-ramblings:10248ddcf524aa32198bc5cded1aa098&#34;&gt;Random Ramblings&lt;/h2&gt;

&lt;p&gt;Well, another week, &lt;a href=&#34;http://www.nytimes.com/2015/02/05/business/hackers-breached-data-of-millions-insurer-says.html&#34;&gt;another big data breach&lt;/a&gt;. This time is Anthem, one of the nation’s largest health insurers. Ok, maybe it was last week that it happend. But this week they revealed that &lt;a href=&#34;http://consumerist.com/2015/02/13/anthem-says-data-from-as-far-back-as-2004-exposed-during-hack-offering-free-identity-theft-protection/&#34;&gt;hackers had access &amp;hellip; going back as far as 2004&lt;/a&gt;. WSJ blamed Anthem for &lt;a href=&#34;http://www.wsj.com/articles/investigators-eye-china-in-anthem-hack-1423167560&#34;&gt;not encrypting the data&lt;/a&gt;. Though I have to agree with Rich Mogull over at Securosis that &amp;ldquo;&lt;a href=&#34;https://securosis.com/blog/even-if-anthem-encrypted-it-probably-wouldnt-have-mattered&#34;&gt;even if Anthem had encrypted, it probably wouldn’t have helped&lt;/a&gt;&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;I feel bad for saying this but there&amp;rsquo;s one positive side effect from all these data breaches. Security is now officially a boardroom topic. Anthem&amp;rsquo;s CEO, Joseph Swedish, is now &lt;a href=&#34;http://www.latimes.com/business/la-fi-anthem-hack-ceo-20150213-story.html#page=1&#34;&gt;under the gun&lt;/a&gt; because top level executives are no longer immune to major security breaches that affect the company&amp;rsquo;s top line. Just ask &lt;a href=&#34;http://www.forbes.com/sites/ericbasu/2014/06/15/target-ceo-fired-can-you-be-fired-if-your-company-is-hacked/&#34;&gt;Target’s CEO Gregg Steinhafel&lt;/a&gt;, or &lt;a href=&#34;http://abcnews.go.com/Entertainment/wireStory/sony-chief-amy-pascal-acknowledges-fired-28918607&#34;&gt;Sony&amp;rsquo;s Co-Chairwoman Amy Pascal&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Brian Krebs wrote a &lt;a href=&#34;http://krebsonsecurity.com/2015/02/anthem-breach-may-have-started-in-april-2014/&#34;&gt;detailed piece&lt;/a&gt; analyzing the various pieces of information available relating to the Anthem hack. Quite an interesting read.&lt;/p&gt;

&lt;p&gt;One chart in the artile that Brian referred to is the time difference between the “time to compromise” and the “time to discovery&amp;rdquo;, taken from &lt;a href=&#34;http://www.verizonenterprise.com/DBIR/2014/&#34;&gt;Verizon’s 2014 Data Breach Investigations Report&lt;/a&gt;. As Brian summaries, &amp;ldquo;TL;DR: That gap is not improving, but instead is widening.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;What this really says is that, &lt;strong&gt;you will get hacked&lt;/strong&gt;. So how do you shorten the time between getting hacked, and finding out that you are hacked so you can quickly remediate the problem before worse things happen?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://krebsonsecurity.com/wp-content/uploads/2015/02/timetocompromise.png&#34; alt=&#34;The time difference between the “time to compromise” and the “time to discovery.”&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With all these data breaches as backdrop, this week we also saw &amp;ldquo;President Barack Obama signed an executive order on Friday designed to spur businesses and the Federal Government to share with each other information related to cybersecurity, hacking and data breaches for the purpose of safeguarding U.S. infrastructure, economics and citizens from cyber attacks.&amp;rdquo; (&lt;a href=&#34;https://gigaom.com/2015/02/13/obamas-executive-order-calls-for-sharing-of-security-data/&#34;&gt;Gigaom&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;In general I don&amp;rsquo;t really think government mandates like this will work. The industry has to feel the pain enough that they are willing to participate, otherwise it&amp;rsquo;s just a waste of paper and ink. Facebook seems to be taking a lead in security information sharing and &lt;a href=&#34;https://www.facebook.com/notes/protect-the-graph/threatexchange-sharing-for-a-safer-internet/1566584370248375&#34;&gt;launched their ThreatExchange security framework&lt;/a&gt; this week. along with Pinterest, Tumblr, Twitter, and Yahoo. Good for them! I hope this is not a temporary PR thing, and that they keep funding and supporting the framework.&lt;/p&gt;

&lt;h2 id=&#34;papers-i-read:10248ddcf524aa32198bc5cded1aa098&#34;&gt;Papers I Read&lt;/h2&gt;

&lt;p&gt;Another great resource of computer science papers is Adrian Coyler&amp;rsquo;s &lt;a href=&#34;http://blog.acolyer.org/&#34;&gt;the morning paper&lt;/a&gt;. He selects and summarizes &amp;ldquo;an interesting/influential/important paper from the world of CS every weekday morning&amp;rdquo;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.put.poznan.pl/dweiss/site/publications/download/fsacomp.pdf&#34;&gt;Smaller Representation of Finite State Automata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I read this paper when I was trying to figure out how to make the FSAs smaller for the &lt;a href=&#34;https://github.com/surge/xparse/tree/master/etld&#34;&gt;Effective TLD matcher&lt;/a&gt; I created. The FSM I generated is 212,294 lines long. That&amp;rsquo;s just absolutely crazy. This paper seems to present an interesting way of compressing them.&lt;/p&gt;

&lt;p&gt;I am not exactly sure if &lt;a href=&#34;https://godoc.org/golang.org/x/net/publicsuffix&#34;&gt;PublicSuffix&lt;/a&gt; uses a similar representation but it basically represents a FSA as an array of bytes, and then walk the bytes like a binary search tree. It&amp;rsquo;s interesting for sure.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This paper is a follow-up to Jan Daciuk’s experiments on space-efficient finite state automata representation that can be used directly for traversals in main memory [4]. We investigate several techniques of reducing the memory footprint of minimal automata, mainly exploiting the fact that transition labels and transition pointer offset values are not evenly distributed and so are suitable for compression. We achieve a size gain of around 20–30% compared to the original representation given in [4]. This result is comparable to the state-of-the-art dictionary compression techniques like the LZ-trie [12] method, but remains
memory and CPU efficient during construction.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://arxiv.org/pdf/1409.5942v1.pdf&#34;&gt;IP Tracing and Active Network Response&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;This work presents integrated model for active security response model. The proposed model introduces Active Response Mechanism (ARM) for tracing anonymous attacks in the network back to their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or “spoofed”, source addresses. This paper presents within the proposed model two tracing approaches based on:
• Sleepy Watermark Tracing (SWT) for unauthorized access attacks.
• Probabilistic Packet Marking (PPM) in the network for Denial of Service
(DoS) and Distributed Denial of Service (DDoS) attacks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36356.pdf&#34;&gt;Dapper, a Large-Scale Distributed Systems Tracing Infrastructure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Here we introduce the design of Dapper, Google’s production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design
choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/&#34;&gt;STREAM PROCESSING, EVENT SOURCING, REACTIVE, CEP… AND MAKING SENSE OF IT ALL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not a paper, but a good write up nonetheless.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Some people call it stream processing. Others call it Event Sourcing or CQRS. Some even call it Complex Event Processing. Sometimes, such self-important buzzwords are just smoke and mirrors, invented by companies who want to sell you stuff. But sometimes, they contain a kernel of wisdom which can really help us design better systems. In this talk, we will go in search of the wisdom behind the buzzwords. We will discuss how event streams can help make your application more scalable, more reliable and more maintainable.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Sequence: Optimizing Go For the High Performance Log Scanner</title>
      <link>http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/</link>
      <pubDate>Fri, 13 Feb 2015 01:03:08 -0800</pubDate>
      
      <guid>http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/</guid>
      <description>

&lt;p&gt;Information here maybe outdated. Please visit &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;http://sequence.trustpath.com&lt;/a&gt; for latest.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This is part 3 of the &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;sequence&lt;/a&gt; series.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/&#34;&gt;Part 1&lt;/a&gt; is about the high performance parser that can parse 100,000-200,000 MPs.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/&#34;&gt;Part 2&lt;/a&gt; is about automating the process of reducing 100 of 1000&amp;rsquo;s of log messages down to dozens of unique patterns.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/&#34;&gt;Part 3&lt;/a&gt; is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size and core count) for scanning and tokenizing log messages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I would love to learn more about the state-of-the-art approaches that log vendors are using. These attempts are about scratching my own itch and trying to realize ideas I&amp;rsquo;ve had in my mind. Given some of these ideas are 5 to 10 years old, they may already be outdated. Personally I just haven&amp;rsquo;t heard of any groundbreaking approaches.&lt;/p&gt;

&lt;p&gt;In any case, if you know of some of the more innovative ways people are approaching these problems, please please please comment below as I would love to hear from you.&lt;/p&gt;

&lt;h3 id=&#34;tl-dr:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;sequence&lt;/code&gt; scanner is designed to tokenize free-form log messages.

&lt;ul&gt;
&lt;li&gt;It can scan between 200K to 500K log messages per second depending on message size and core count.&lt;/li&gt;
&lt;li&gt;It recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.&lt;/li&gt;
&lt;li&gt;The design is based mostly on finite-state machines.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The performance was achieved by the following techniques:

&lt;ol&gt;
&lt;li&gt;Go Through the String Once and Only Once&lt;/li&gt;
&lt;li&gt;Avoid Indexing into the String&lt;/li&gt;
&lt;li&gt;Reduce Heap Allocation&lt;/li&gt;
&lt;li&gt;Reduce Data Copying&lt;/li&gt;
&lt;li&gt;Mind the Data Struture&lt;/li&gt;
&lt;li&gt;Avoid Interfaces If Possible&lt;/li&gt;
&lt;li&gt;Find Ways to Short Circuit Checks&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;background:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Background&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;In computer science, lexical analysis is the process of converting a sequence of characters into a sequence of tokens, i.e. meaningful character strings. A program or function that performs lexical analysis is called a lexical analyzer, lexer, tokenizer, or scanner. - &lt;a href=&#34;http://en.wikipedia.org/wiki/Lexical_analysis&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of the most critical functions in the &lt;code&gt;sequence&lt;/code&gt; parser is the message tokenization. At a very high level, message tokenization means taking a single log message and breaking it into a list of tokens.&lt;/p&gt;

&lt;h3 id=&#34;functional-requirements:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Functional Requirements&lt;/h3&gt;

&lt;p&gt;The challenge is knowing where the token break points are. Most log messages are free-form text, which means there&amp;rsquo;s no common structure to them.&lt;/p&gt;

&lt;p&gt;As an example, the following log message can be tokenized into the sequence of tokens below. As you can see, one cannot depend on white spaces to tokenize, as the timestamp would be broken into 3 parts; nor can one use punctuations like &amp;ldquo;;&amp;rdquo; or &amp;ldquo;:&amp;ldquo;, as they would break the log mesage into useless parts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream

  #   0: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 14 10:15:56&amp;quot; }
  #   1: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;testserver&amp;quot; }
  #   2: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sudo&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   4: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;gonner&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;tty&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pts/3&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;pwd&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/home/gonner&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  14: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  15: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;root&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  18: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;command&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/bin/su&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;-&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;ustream&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So a log message &lt;em&gt;scanner&lt;/em&gt; or &lt;em&gt;tokenizer&lt;/em&gt; (we will use these terms interchangeably) must understand common components such as timestamp, URL, hex strings, IP addresses (v4 or v6), and mac addresses, so it can break the messages into &lt;em&gt;meaningful components&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;performance-requirements:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Performance Requirements&lt;/h3&gt;

&lt;p&gt;From a performance requirements perspective, I really didn&amp;rsquo;t start out with any expectations. However, after achieving 100-200K MPS for parsing (not just tokenizing), I have a strong desire to keep the performance at that level. So the more I can optimize the scanner to tokenize faster, the more head room I have for parsing.&lt;/p&gt;

&lt;p&gt;One may ask, who can POSSIBLY use such performance? Many organizations that I know are generating between 50-100M messages per second (MPS), that&amp;rsquo;s only 1,200 MPS. Some larger organizations I know are generating 60GB of Bluecoat logs per day, &lt;strong&gt;8 years ago&lt;/strong&gt;!! That&amp;rsquo;s a good 3,000 MPS assuming an average of 250 bytes per message. Even if log rate grows at 15%, that&amp;rsquo;s still only 10K MPS today.&lt;/p&gt;

&lt;p&gt;To run through an example, &lt;a href=&#34;http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf&#34;&gt;at EMC, 1.4 billion log messages are generated daily on average, at a rate of one terabyte a day&lt;/a&gt;. That&amp;rsquo;s 16,200 messages per second, and about 714 bytes per message. (Btw, what system can possibly generate messages that are 714 bytes long? That&amp;rsquo;s crazy and completely inefficient!) These EMC numbers are from 2013, so they have likely increased by now.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; parser, with a single CPU core, can process about 270,000 MPS for messages averaging 98 bytes. Assuming the performance is linear compare to the message size (which is pretty close to the truth), we can process 37,000 MPS for messages averaging 714 bytes. That&amp;rsquo;s just enough to parse the 16,2000 MPS, with a little head room to do other types of analysis or future growth.&lt;/p&gt;

&lt;p&gt;Obviously one can throw more hardware at solving the scale problem, but then again, why do that if you don&amp;rsquo;t need to. Just because you have the hardware doesn&amp;rsquo;t mean you should waste the money! Besides, there are much more interesting analytics problems your hardware can be used for than just tokenizing a message.&lt;/p&gt;

&lt;p&gt;In any case, I want to squeeze every oz of performance out of the scanner so I can have more time in the back to parse and analyze. So let&amp;rsquo;s set a goal of keeping at least 200,000 MPS for 100 bytes per message (BPM).&lt;/p&gt;

&lt;p&gt;Yes, go ahead and tell me I shouldn&amp;rsquo;t worry about micro-optimization, because this post is all about that. :)&lt;/p&gt;

&lt;h2 id=&#34;sequence-scanner:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Sequence Scanner&lt;/h2&gt;

&lt;p&gt;In the &lt;code&gt;sequence&lt;/code&gt; package, we implemented a general log message scanner, called &lt;a href=&#34;https://github.com/strace/sequence/blob/master/scanner.go&#34;&gt;GeneralScanner&lt;/a&gt;. GeneralScanner is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.&lt;/p&gt;

&lt;p&gt;This implementation was able to achieve both the functional and performance requirements. The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro, although the tests were only using 1 or 2 cores. The first file is a bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message. Last is a mix of ASA, sshd and sudo logs, averaging 136 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272869.35 msgs/sec

  $ ./sequence bench scan -i ../../data/allasa.log
  Scanned 234815 messages in 1.43 secs, ~ 163827.61 msgs/sec

  $ ./sequence bench scan -i ../../data/allasassh.log
  Scanned 447745 messages in 2.27 secs, ~ 197258.42 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/sshd.all -w 2
  Scanned 212897 messages in 0.43 secs, ~ 496961.52 msgs/sec

  $ GOMAXPROCS=2 ./sequenceo bench scan -i ../../data/allasa.log -w 2
  Scanned 234815 messages in 0.80 secs, ~ 292015.98 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/allasassh.log -w 2
  Scanned 447745 messages in 1.20 secs, ~ 373170.45 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;concepts:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Concepts&lt;/h3&gt;

&lt;p&gt;To understand the scanner, you have to understand the following concepts that are part of the package.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;Token&lt;/em&gt; is a piece of information extracted from the original log message. It is a struct that contains fields for &lt;em&gt;TokenType&lt;/em&gt;, &lt;em&gt;FieldType&lt;/em&gt;, and &lt;em&gt;Value&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;TokenType&lt;/em&gt; indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;FieldType&lt;/em&gt; indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Sequence&lt;/em&gt; is a list of Tokens. It is the key data structure consumed and returned by the &lt;em&gt;Scanner&lt;/em&gt;, &lt;em&gt;Analyzer&lt;/em&gt;, and the &lt;em&gt;Parser&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Basically, the scanner takes a log message string, tokenizes it and returns a &lt;em&gt;Sequence&lt;/em&gt; with the recognized &lt;em&gt;TokenType&lt;/em&gt; marked. This &lt;em&gt;Sequence&lt;/em&gt; is then fed into the analyzer or parser, and the analyzer or parser in turn returns another &lt;em&gt;Sequence&lt;/em&gt; that has the recognized &lt;em&gt;FieldType&lt;/em&gt; marked.&lt;/p&gt;

&lt;h3 id=&#34;design:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Design&lt;/h3&gt;

&lt;p&gt;Tokenizers or scanners are usually implemented using finite-state machines. Each FSM (or FSA, finite state automata) understands a specific sequences of characters that make up a type of token.&lt;/p&gt;

&lt;p&gt;In the &lt;code&gt;sequence&lt;/code&gt; scanner, there are three FSMs: Time, HexString and General.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Time FSM understands a list of &lt;a href=&#34;https://github.com/strace/sequence/blob/master/time.go&#34;&gt;time formats&lt;/a&gt;. This list of time formats are commonly seen in log messages. It is also fairly easy to add to this list if needed.&lt;/li&gt;
&lt;li&gt;The HexString FSM is designed to understand IPv6 addresses (dead:beef:1234:5678:223:32ff:feb1:2e50 or f0f0:f::1), MAC addresses (00:04:c1:8b:d8:82), fingerprints or signatures (de:ad:be:ef:74:a6:bb:45:45:52:71:de:b2:12:34:56).&lt;/li&gt;
&lt;li&gt;The General FSM that recognizes URLs, IPv4 addresses, and any literal or strings.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each character in the log string are run through all three FSMs.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If a time format is matched, that&amp;rsquo;s what it will be returned.&lt;/li&gt;
&lt;li&gt;Next if a hex string is matched, it is also returned.

&lt;ul&gt;
&lt;li&gt;We mark anything with 5 colon characters and no successive colons like &amp;ldquo;::&amp;rdquo; to be a MAC address.&lt;/li&gt;
&lt;li&gt;Anything that has 7 colons and no successive colons are marked as IPv6 address.&lt;/li&gt;
&lt;li&gt;Anything that has less than 7 colons but has only 1 set of successive colons like &amp;ldquo;::&amp;rdquo; are marked as IPv6 address.&lt;/li&gt;
&lt;li&gt;Everything else is just a literal.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Finally if neither of the above matched, we return what the general FSM has matched.

&lt;ul&gt;
&lt;li&gt;The general FSM recognizes these quote characters: &amp;ldquo;, &amp;lsquo; and &amp;lt;. If these characters are encountered, then it will consider anything between the quotes to be a single token.&lt;/li&gt;
&lt;li&gt;Anything that starts with http:// or https:// are considered URLs.&lt;/li&gt;
&lt;li&gt;Anything that matches 4 integer octets are considered IP addresses.&lt;/li&gt;
&lt;li&gt;Anything that matches two integers with a dot in between are considered floats.&lt;/li&gt;
&lt;li&gt;Anything that matches just numbers are considered integers.&lt;/li&gt;
&lt;li&gt;Everything else are literals.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;performance:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;To achieve the performance requirements, the following rules and optimizations are followed. Some of these are Go specific, and some are general recommendations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Go Through the String Once and Only Once&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a hard requirement, otherwise we can&amp;rsquo;t call this project a &lt;em&gt;sequential&lt;/em&gt; parser. :)&lt;/p&gt;

&lt;p&gt;This is probably a pretty obvious technique. The more times you loop through loop through a string, the lower the performance. If you used regular expressions to parse logs, you will likely go through parts of the log message multiple times due to back tracking or look forward, etc.&lt;/p&gt;

&lt;p&gt;I took great pain to ensure that I don&amp;rsquo;t need to look forward or look backward in the log string to determine the current token type, and I think the effort paid off.&lt;/p&gt;

&lt;p&gt;In reality though, while I am only looping through the log string once, and only once, I do run each character through three different FSMs. However, it is still much less expensive than looping through three times, each time checking a single FSM. However, the more FSMs I run the characters through, the slower it gets.&lt;/p&gt;

&lt;p&gt;This was apparently when I &lt;a href=&#34;https://github.com/strace/sequence/commit/a5447814f43b4b9b7e804b14dde38e88fd53e6d0&#34;&gt;updated the scanner to support IPv6 and hex strings&lt;/a&gt;. I tried a couple of different approaches. First, I added an IPv6 specific FSM. So in addition to the original time, mac and general FSMs, there are now 4. That dropped performance by like 15%!!! That&amp;rsquo;s just unacceptable.&lt;/p&gt;

&lt;p&gt;The second approach, which is the one I checked in, combines the MAC, IPv6 and general hex strings into a single FSM. That helped somewhat. I was able to regain about 5% of the performance hit. However, because I can no longer short circuit the MAC address check (by string length and colon positions), I was still experiencing a 8-10% hit.&lt;/p&gt;

&lt;p&gt;What this means is that for most tokens, instead of checking just 2 FSMs because I can short circuit the MAC check pretty early, I have to now check all 3 FSMs.&lt;/p&gt;

&lt;p&gt;So the more FSMs, the more comlicated the FSMs, the more performance hits there will be.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Avoid Indexing into the String&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is really a Go-specific recommentation. Each time you index into a slice or string, Go will perform bounds checking for you, which means there&amp;rsquo;s extra operations it&amp;rsquo;s doing, and also means lower performance. As an example, here are results from two benchmark runs. The first is with bounds checking enabled, which is default Go behavior. The second disables bounds checking.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.79 secs, ~ 268673.91 msgs/sec

  $ go run -gcflags=-B ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 277479.58 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The performance difference is approximately 3.5%! However, while it&amp;rsquo;s fun to see the difference, I would never recommend disable bounds checking in production. So the next best thing is to remove as many operations that index into a string or slice as possible. Specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use &amp;ldquo;range&amp;rdquo; in the loops, e.g. &lt;code&gt;for i, r := range str&lt;/code&gt; instead of &lt;code&gt;for i := 0; i &amp;lt; len(str); i++ { if str[i] == ... }&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you are checking a specific character in the string/slice multiple times, assign it to a variable and use the variable instead. This will avoid indexing into the slice/string multiple times.&lt;/li&gt;
&lt;li&gt;If there are multiple conditions in an &lt;code&gt;if&lt;/code&gt; statement, try to move (or add) the non-indexing checks to the front of the statement. This sometimes will help short circuit the checks and avoid the slice-indexing checks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One might question if this is worth optimizing, but like I said, I am trying to squeeze every oz of performance so 3.5% is still good for me. Unfornately I do know I won&amp;rsquo;t get 3.5% since I can&amp;rsquo;t remove every operation that index into slice/string.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Reduce Heap Allocation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is true for all languages (where you can have some control of stack vs heap allocation), and it&amp;rsquo;s even more true in Go. Mainly in Go, if you allocate a new slice, Go will &amp;ldquo;zero&amp;rdquo; out the allocated memory. This is great from a safety perspective, but it does add to the overhead.&lt;/p&gt;

&lt;p&gt;As an example, in the scanner I originally allocated a new &lt;em&gt;Sequence&lt;/em&gt; (slice of &lt;em&gt;Token&lt;/em&gt;) for every new message. However, when i changed it to re-use the existing slice, the performance increased by over 10%!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.87 secs, ~ 246027.12 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 275038.83 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The best thing to do is to run Go&amp;rsquo;s builtin CPU profiler, and look at the numbers for Go internal functions such as &lt;code&gt;runtime.makeslice&lt;/code&gt;, &lt;code&gt;runtime.markscan&lt;/code&gt;, and &lt;code&gt;runtime.memclr&lt;/code&gt;. Large percentages and numbers for these internal functions are dead giveaway that you are probably allocating too much stuff on the heap.&lt;/p&gt;

&lt;p&gt;I religiously go through the SVGs generated from the Go profiler to help me identify hot spots where I can optimize.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s also a couple of tips I picked up from the &lt;a href=&#34;https://groups.google.com/forum/#!topic/golang-nuts/baU4PZFyBQQ&#34;&gt;go-nuts mailing list&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Maps are bad&amp;ndash;even if they&amp;rsquo;re storing integers or other non-pointer structs. The implementation appears to have lots of pointers inside which must be evaluated and followed during mark/sweep GC.  Using structures with pointers magnifies the expense.&lt;/li&gt;
&lt;li&gt;Slices are surprisingly bad (including strings and substrings of existing strings). A slice is a pointer to the backing array with a length and capacity. It would appear that the internal pointer that causes the trouble because GC wants to inspect it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. Reduce Data Copying&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Data copying is expensive. It means the run time has to allocate new space and copy the data over. It&amp;rsquo;s even more expensive when you can&amp;rsquo;t have do &lt;code&gt;memcpy&lt;/code&gt; of a slice in Go like you can in C. Again, direct memory copying is not Go&amp;rsquo;s design goal. It is also much safer if you can prevent users from playing with memory directly too much. However, it is still a good idea to avoid any copying of data, whether it&amp;rsquo;s string or slice.&lt;/p&gt;

&lt;p&gt;As much as I can, I try to do in place processing of the data. Every &lt;em&gt;Sequence&lt;/em&gt; is worked on locally and I try not to copy &lt;em&gt;Sequence&lt;/em&gt; or string unless I absolutely have to.&lt;/p&gt;

&lt;p&gt;Unfortunately I don&amp;rsquo;t have any comparison numbers for this one, because I learned from &lt;a href=&#34;http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/&#34;&gt;previous projects&lt;/a&gt; that I should avoid copying as much as possible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Mind the Data Struture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If there&amp;rsquo;s one thing I learned over the past year, is to use the right data structure for the right job. I&amp;rsquo;ve written about other data structures such as &lt;a href=&#34;http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/&#34;&gt;ring buffer&lt;/a&gt;, &lt;a href=&#34;http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/&#34;&gt;bloom filters&lt;/a&gt;, and &lt;a href=&#34;http://zhen.org/blog/go-skiplist/&#34;&gt;skiplist&lt;/a&gt; before.&lt;/p&gt;

&lt;p&gt;However, &lt;a href=&#34;http://en.wikipedia.org/wiki/Finite-state_machine&#34;&gt;finite-state automata or machine&lt;/a&gt; is my latest love and I&amp;rsquo;ve been using it at various projects such as my &lt;a href=&#34;http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/&#34;&gt;porter2&lt;/a&gt; and &lt;a href=&#34;https://github.com/surge/xparse/tree/master/etld&#34;&gt;effective TLD&lt;/a&gt;. Ok, technical FSM itself is not a data structure and can be implemented in different ways. In the &lt;code&gt;sequence&lt;/code&gt; project, I used both a tree representation as well as a bunch of switch-case statements. For the &lt;a href=&#34;http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/&#34;&gt;porter2&lt;/a&gt; FSMs, I used switch-case to implement them.&lt;/p&gt;

&lt;p&gt;Interestingly, swtich-case doesn&amp;rsquo;t always win. I tested the time FSM using both tree and switch-case implementations, and the tree actually won out. (Below, 1 is tree, 2 is switch-case.) So guess which one is checked in?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BenchmarkTimeStep1   2000000         696 ns/op
BenchmarkTimeStep2   2000000         772 ns/op
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Writing this actually reminds me that in the parser, I am currently using a tree to parse the sequences. While parsing, there could be multiple paths that the sequence will match. Currently I walk all the matched paths fully, before choosing one that has the highest score. What I should do is to do a weighted walk, and always walk the highest score nodes first. If at the end I get a perfect score, I can just return that path and not have to walk the other paths. (Note to self, more parser optimization to do).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6. Avoid Interfaces If Possible&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is probably not a great advice to give to Go developers. Interface is probably one of the best Go features and everyone should learn to use it. However, if you want high performane, avoid interfaces as it provides additional layers of indirection. I don&amp;rsquo;t have performance numbers for the &lt;code&gt;sequence&lt;/code&gt; project since I tried to avoid interfaces in high performance areas from the start. However, previous in the &lt;a href=&#34;http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/&#34;&gt;ring buffer&lt;/a&gt; project, the version that uses interface is 140% slower than the version that didn&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t have the direct link but someone on the go-nuts mailing list also said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you really want high performance, I would suggest avoiding interfaces and, in general, function calls like the plague, since they are quite expensive in Go (compared to C). We have implemented basically the same for our internal web framework (to be released some day) and we&amp;rsquo;re almost 4x faster than encoding/json without doing too much optimization. I&amp;rsquo;m sure we could make this even faster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;7. Find Ways to Short Circuit Checks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Find ways to quickly eliminate the need to run a section of the code has been tremendously helpful to improve performance. For example, here are a couple of place where I tried to do that.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;https://github.com/strace/sequence/blob/master/scanner.go#L223&#34;&gt;this first example&lt;/a&gt;, I simply added &lt;code&gt;l == 1&lt;/code&gt; before the actual equality check of the string values. The first output is before the add, the second is after. The difference is about 2% performance increase.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272303.79 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 278433.34 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In &lt;a href=&#34;https://github.com/strace/sequence/blob/master/scanner.go#L282&#34;&gt;the second example&lt;/a&gt;, I added a quick check to make sure the remaining string is at least as long as the shortest time format. If there&amp;rsquo;s not enough characters, then don&amp;rsquo;t run the time FSM. The performance difference is about 2.5%.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272059.04 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 279388.47 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So by simply adding a couple of checks, I&amp;rsquo;ve increased perfromance by close to 5%.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At this point I think I have squeezed every bit of performance out of the scanner, to the extend of my knowledge. It&amp;rsquo;s performing relatively well and it&amp;rsquo;s given the parser plenty of head room to do other things. I hope some of these lessons are helpful to whatever you are doing.&lt;/p&gt;

&lt;p&gt;Feel free to take a look at the &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; project and try it out if you. If you have any issues/comments, please don&amp;rsquo;t hestiate to &lt;a href=&#34;https://github.com/strace/sequence/issues&#34;&gt;open a github issue&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns</title>
      <link>http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/</link>
      <pubDate>Tue, 10 Feb 2015 06:40:20 -0800</pubDate>
      
      <guid>http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/</guid>
      <description>

&lt;p&gt;Information here maybe outdated. Please visit &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;http://sequence.trustpath.com&lt;/a&gt; for latest.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This is part 2 of the &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;sequence&lt;/a&gt; series.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/&#34;&gt;Part 1&lt;/a&gt; is about the high performance parser that can parse 100,000-200,000 MPs.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/&#34;&gt;Part 2&lt;/a&gt; is about automating the process of reducing 100 of 1000&amp;rsquo;s of log messages down to dozens of unique patterns.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/&#34;&gt;Part 3&lt;/a&gt; is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size) for scanning and tokenizing log messages&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;background:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;This post really takes me down the memory lane. Back in 2005, while I was at LogLogic, we envisioned an automated approach to tagging, or labeling, log messages. More specifically, we wanted to automatically tag specific components within the log messages with their semantic label, such as a source IP address, or a target user.&lt;/p&gt;

&lt;p&gt;At the time, much like it is still today, the message parsing process is performed manually. This means someone has to manually look at the object and decided that the object should be labeled “user” or “targetUser.” An  analyst has to go through the log data, create a regular expression that extracts the useful strings out, and then finally assigning these to a specific label. This is extremely time consuming and error-prone.&lt;/p&gt;

&lt;p&gt;At that time, the vision was to provide an automated approach to universally parse and analyze ANY log data. The key phrase being “automated approach.” This means the users should only need to provide minimum guidance to the system, if any, for the platforms to be able to analyze the log data. LogLogic never did much with this, unfortunately.&lt;/p&gt;

&lt;p&gt;However, the tagging concept was later on adopted by (and I know how this got into CEE :) the &lt;a href=&#34;http://cee.mitre.org/&#34;&gt;Common Event Expression, or CEE&lt;/a&gt; effort by Mitre. This idea of tags also inspired &lt;a href=&#34;http://www.liblognorm.com/&#34;&gt;liblognorm&lt;/a&gt; to develop their &lt;a href=&#34;http://www.libee.org/&#34;&gt;libee&lt;/a&gt; library and &lt;a href=&#34;http://www.liblognorm.com/news/log-classification-with-liblognorm/&#34;&gt;tagging system&lt;/a&gt;. Rsyslog&amp;rsquo;s &lt;a href=&#34;http://www.rsyslog.com/doc/mmnormalize.html&#34;&gt;mmnormalize&lt;/a&gt; module is based on liblognorm.&lt;/p&gt;

&lt;p&gt;And then there&amp;rsquo;s Fedora&amp;rsquo;s &lt;a href=&#34;https://fedorahosted.org/lumberjack/&#34;&gt;Project Lumberjack&lt;/a&gt;, which &amp;ldquo;is an open-source project to update and enhance the event log architecture&amp;rdquo; and &amp;ldquo;aims to improve the creation and standardize the content of event logs by implementing the concepts and specifications proposed by the ​Common Event Expression (CEE).&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Then finally &lt;a href=&#34;http://logstash.net/&#34;&gt;logstash&lt;/a&gt; has their &lt;a href=&#34;http://logstash.net/docs/1.4.2/filters/grok&#34;&gt;grok filter&lt;/a&gt; that basically does similar extraction of unstructured data into a structured and queryable format. However, it seems like there might be some &lt;a href=&#34;http://ghost.frodux.in/logstash-grok-speeds/&#34;&gt;performance bottlenecks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, none of these efforts attempted to solve the automated tagging/labeling problem. They mostly just try to provide a parser for log messages.&lt;/p&gt;

&lt;p&gt;Also, it looks like many of these efforts have all been abandoned or put in hibernation, and haven&amp;rsquo;t been updated since 2012 or 2013. liblognrom did put out &lt;a href=&#34;http://www.liblognorm.com/news/&#34;&gt;a couple of updates&lt;/a&gt; in the past couple of years. Logstash&amp;rsquo;s grok obviously is being maintained and developed with the &lt;a href=&#34;http://www.elasticsearch.com/&#34;&gt;Elasticsearch&lt;/a&gt; backing.&lt;/p&gt;

&lt;p&gt;It is understandable, unfortunately. Log parsing is &lt;strong&gt;BORING&lt;/strong&gt;. I mean, who wants to sit there and stare at logs all day and try to come up with regular expressions or other types of parsing rules? LogLogic used to have a team of LogLabs analysts that did that, and I have to say I truly appreciated their effort and patience, because I cannot do that.&lt;/p&gt;

&lt;h3 id=&#34;the-end-result:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;The End Result&lt;/h3&gt;

&lt;p&gt;So instead of writing rules all day long, I decided to create an analyzer that can help us get at least 75% of the way there. The end result is the &lt;code&gt;Analyzer&lt;/code&gt;, written in &lt;a href=&#34;http://golang.org&#34;&gt;Go&lt;/a&gt;, in the &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; project I created. Here are some preliminary results. Below, we analyzed 2 files. The first is a file with over 200,000 sshd messages. The second is a file with a mixture of ASA, sshd, sudo and su log messages. It contains almost 450,000 messages.&lt;/p&gt;

&lt;p&gt;By running the analyzer over these logs, the pure sshd log file returned 45 individual patterns, and the second returned 103 unique patterns.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run sequence.go analyze -i ../../data/sshd.all -o sshd.analyze
Analyzed 212897 messages, found 45 unique patterns, 45 are new.

$ go run sequence.go analyze -i ../../data/asasshsudo.log -o asasshsudo.analyze
Analyzed 447745 messages, found 103 unique patterns, 103 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the output file has entries such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%msgtime% %apphost% %appname% [ %sessionid% ] : %status% %method% for %srcuser% from %srcipv4% port %srcport% ssh2
# Jan 15 19:39:26 irc sshd[7778]: Accepted password for jlz from 108.61.8.124 port 57630 ssh2

%msgtime% %appipv4% %appname% : %action% outbound %protocol% connection %sessionid% for %string% : %srcipv4% / %srcport% ( %ipv4% / %integer% ) to %string% : %dstipv4% / %dstport% ( %ipv4% / %integer% )
# 2012-04-05 18:46:18   172.23.0.1  %ASA-6-302013: Built outbound TCP connection 1424575 for outside:10.32.0.100/80 (10.32.0.100/80) to inside:172.23.73.72/2522 (10.32.0.1/54702)

%msgtime% %apphost% %appname% : %string% : tty = %string% ; pwd = %string% ; user = %srcuser% ; command = %command% - %string%
# Jan 15 14:09:11 irc sudo:    jlz : TTY=pts/1 ; PWD=/home/jlz ; USER=root ; COMMAND=/bin/su - irc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the output is not 100%, but it gets us pretty close. Once the analyst goes through and updates the rules, he/she can re-run the analyzer anytime with any file to determine if there&amp;rsquo;s new patterns. For example, below, we ran the sshd log file with an existing pattern file, and got 4 new log patterns.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run sequence.go analyze -i ../../data/sshd.all -p ../../patterns/sshd.txt -o sshd.analyze
Analyzed 212897 messages, found 39 unique patterns, 4 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;parser-quick-review:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Parser - Quick Review&lt;/h3&gt;

&lt;p&gt;I wrote about the &lt;a href=&#34;http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/&#34;&gt;sequence parser&lt;/a&gt; a couple of weeks back. It is a &lt;em&gt;high performance sequential log parser&lt;/em&gt;. It &lt;em&gt;sequentially&lt;/em&gt; goes through a log message, &lt;em&gt;parses&lt;/em&gt; out the meaningful parts, without the use regular expressions. It can achieve &lt;em&gt;high performance&lt;/em&gt; parsing of &lt;strong&gt;100,000 - 200,000 messages per second (MPS)&lt;/strong&gt; without the need to separate parsing rules by log source type. Underneath the hood, the &lt;code&gt;sequence&lt;/code&gt; parser basically constructs a tree based on the sequential rules, walks the tree to identify all the possible paths, and returns the path that has the best match (highest weight) for the message.&lt;/p&gt;

&lt;p&gt;While the analyzer is about reducing a large corupus of raw log messages down to a small set of unique patterns, the parser is all about matching log messages to an existing set of patters and determining whether a specific pattern has matched. Based on the pattern, it returns a sequence of tokens that basically extracts out the important pieces of information from the logs. The analysts can then take this sequence and perform other types of analysis.&lt;/p&gt;

&lt;p&gt;The approach taken by the &lt;code&gt;sequence&lt;/code&gt; parser is pretty much the same as liblognorm or other tree-based approaches.&lt;/p&gt;

&lt;h2 id=&#34;sequence-analyzer:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Sequence Analyzer&lt;/h2&gt;

&lt;p&gt;In the following section I will go through additional details of how the &lt;code&gt;sequence&lt;/code&gt; analyzer reduces 100 of 1000&amp;rsquo;s of raw log messages down to just 10&amp;rsquo;s of unique patterns, and then determining how to label the individual tokens.&lt;/p&gt;

&lt;h3 id=&#34;identifying-unique-patterns:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Identifying Unique Patterns&lt;/h3&gt;

&lt;p&gt;Analyzer builds an analysis tree that represents all the Sequences from messages. It can be used to determine all of the unique patterns for a large body of messages.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s based on a single basic concept, that for multiple log messages, if tokens in the same position shares one same parent and one same child, then the tokens in that position is likely variable string, which means it&amp;rsquo;s something we can extract. For example, take a look at the following two messages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jan 12 06:49:42 irc sshd[7034]: Accepted password for root from 218.161.81.238 port 4228 ssh2
Jan 12 14:44:48 jlz sshd[11084]: Accepted publickey for jlz from 76.21.0.16 port 36609 ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first token of each message is a timestamp, and the 3rd token of each message is the literal &amp;ldquo;sshd&amp;rdquo;. For the literals &amp;ldquo;irc&amp;rdquo; and &amp;ldquo;jlz&amp;rdquo;, they both share a common parent, which is a timestamp. They also both share a common child, which is &amp;ldquo;sshd&amp;rdquo;. This means token in between these, the 2nd token in each message, likely represents a variable token in this message type. In this case, &amp;ldquo;irc&amp;rdquo; and &amp;ldquo;jlz&amp;rdquo; happens to
represent the syslog host.&lt;/p&gt;

&lt;p&gt;Looking further down the message, the literals &amp;ldquo;password&amp;rdquo; and &amp;ldquo;publickey&amp;rdquo; also share a common parent, &amp;ldquo;Accepted&amp;rdquo;, and a common child, &amp;ldquo;for&amp;rdquo;. So that means the token in this position is also a variable token (of type TokenString).&lt;/p&gt;

&lt;p&gt;You can find several tokens that share common parent and child in these two messages, which means each of these tokens can be extracted. And finally, we can determine that the single pattern that will match both is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%time% %string% sshd [ %integer% ] : Accepted %string% for %string% from %ipv4% port %integer% ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If later we add another message to this mix:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jan 12 06:49:42 irc sshd[7034]: Failed password for root from 218.161.81.238 port 4228 ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Analyzer will determine that the literals &amp;ldquo;Accepted&amp;rdquo; in the 1st message, and &amp;ldquo;Failed&amp;rdquo; in the 3rd message share a common parent &amp;ldquo;:&amp;rdquo; and a common child &amp;ldquo;password&amp;rdquo;, so it will determine that the token in this position is also a variable token. After all three messages are analyzed, the final pattern that will match all three
messages is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%time% %string% sshd [ %integer% ] : %string% %string% for %string% from %ipv4% port %integer% ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By applying this concept, we can effectively identify all the unique patterns in a log file.&lt;/p&gt;

&lt;h3 id=&#34;determining-the-correct-labels:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Determining the Correct Labels&lt;/h3&gt;

&lt;p&gt;Now that we have the unique patterns, we will scan the tokens to determine which labels we should apply to them.&lt;/p&gt;

&lt;p&gt;System and network logs are mostly free form text. There&amp;rsquo;s no specific patterns to any of them. So it&amp;rsquo;s really difficult to determine how to label specific parts of the log message automatically. However, over the years, after looking at so many system and network log messages, some patterns will start to emerge.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no &amp;ldquo;machine learning&amp;rdquo; here. This section is all about codifying these human learnings. I&amp;rsquo;ve created the following 6 rules to help label tokens in the log messages. By no means are these rules perfect. They are at best just guesses on how to label. But hopefully they can get us 75% of the way there and we human can just take it the rest of the way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;0. Parsing Email and Hostname Formats&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is technically not a labeling step. Before we actually start the labeling process, we wanted to first parse out a couple more formats like email and host names. The message tokenizer doesn&amp;rsquo;t recognize these because they are difficult to parse and will slow down the tokenizer. These specific formats are also not needed by the parser. So because the analyzer doesn&amp;rsquo;t care about performance as much, we can do this as post-processing step.&lt;/p&gt;

&lt;p&gt;To recognize the hostname, we try to match the &amp;ldquo;effective TLD&amp;rdquo; using the &lt;a href=&#34;https://github.com/surge/xparse/tree/master/etld&#34;&gt;xparse/etld&lt;/a&gt; package. It is an effective TLD matcher that returns the length of the effective domain name for the given string. It uses the data set from &lt;a href=&#34;https://www.publicsuffix.org/list/effective_tld_names.dat&#34;&gt;https://www.publicsuffix.org/list/effective_tld_names.dat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Recognizing Syslog Headers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First we will try to see if we can regonize the syslog headers. We try to recogize both RFC5424 and RFC3164 syslog headers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	// RFC5424
	// - &amp;quot;1 2003-10-11T22:14:15.003Z mymachine.example.com evntslog - ID47 ...&amp;quot;
	// - &amp;quot;1 2003-08-24T05:14:15.000003-07:00 192.0.2.1 myproc 8710 - ...&amp;quot;
	// - &amp;quot;1 2003-10-11T22:14:15.003Z mymachine.example.com su - ID47 ...&amp;quot;
	// RFC3164
	// - &amp;quot;Oct 11 22:14:15 mymachine su: ...&amp;quot;
	// - &amp;quot;Aug 24 05:34:00 CST 1987 mymachine myproc[10]: ...&amp;quot;
	// - &amp;quot;jan 12 06:49:56 irc last message repeated 6 times&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the sequence pattern matches any of the above sequence, then we assume the first few tokens belong to the syslog header.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Marking Key and Value Pairs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The next step we perform is to mark known &amp;ldquo;keys&amp;rdquo;. There are two types of keys. First, we identify any token before the &amp;ldquo;=&amp;rdquo; as a key. For example, the message &lt;code&gt;fw=TOPSEC priv=6 recorder=kernel type=conn&lt;/code&gt; contains 4 keys: &lt;code&gt;fw&lt;/code&gt;, &lt;code&gt;priv&lt;/code&gt;, &lt;code&gt;recorder&lt;/code&gt; and &lt;code&gt;type&lt;/code&gt;. These keys should be considered string literals, and should not be extracted. However, they can be used to determine how the value part should be labeled.&lt;/p&gt;

&lt;p&gt;The second types of keys are determined by keywords that often appear in front of other tokens, I call these &lt;strong&gt;prekeys&lt;/strong&gt;. For example, we know that the prekey &lt;code&gt;from&lt;/code&gt; usually appears in front of any source host or IP address, and the prekey &lt;code&gt;to&lt;/code&gt; usually appears in front of any destination host or IP address. Below are some examples of these prekeys.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from 		= [ &amp;quot;%srchost%&amp;quot;, &amp;quot;%srcipv4%&amp;quot; ]
port 		= [ &amp;quot;%srcport%&amp;quot;, &amp;quot;%dstport%&amp;quot; ]
proto		= [ &amp;quot;%protocol%&amp;quot; ]
sport		= [ &amp;quot;%srcport%&amp;quot; ]
src 		= [ &amp;quot;%srchost%&amp;quot;, &amp;quot;%srcipv4%&amp;quot; ]
to 			= [ &amp;quot;%dsthost%&amp;quot;, &amp;quot;%dstipv4%&amp;quot;, &amp;quot;%dstuser%&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To help identify these prekeys, I wrote a quick program that goes through many of the logs I have to help identify what keywords appears before IP address, mac addresses, and other non-literal tokens. The result is put into the &lt;a href=&#34;https://github.com/strace/sequence/blob/master/keymaps.go&#34;&gt;keymaps.go&lt;/a&gt; file. It&amp;rsquo;s not comprehensive, but it&amp;rsquo;s also not meant to be. We just need enough hints to help with labeling.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Labeling &amp;ldquo;Values&amp;rdquo; by Their Keys&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once the keys are labeled, we can label the values based on the mapping described above. For key/value pairs, we try to recognize both &lt;code&gt;key=value&lt;/code&gt; or &lt;code&gt;key=&amp;quot;value&amp;quot;&lt;/code&gt; formats (or other quote characters like &amp;lsquo; or &amp;lt;).&lt;/p&gt;

&lt;p&gt;For the prekeys, we try to find the value token within 2 tokens of the key token. That means sequences such as &lt;code&gt;from 192.168.1.1&lt;/code&gt; and &lt;code&gt;from ip 192.168.1.1&lt;/code&gt; will identify &lt;code&gt;192.168.1.1&lt;/code&gt; as the &lt;code&gt;%srcipv4%&lt;/code&gt; based on the above mapping, but we will miss &lt;code&gt;from ip address 192.168.1.1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Identifying Known Keywords&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Within most log messages, there are certain keywords that would indicate what actions were performed, what the state/status of the action was, and what objects the actions were performed on. CEE had a list that it identified, so I copied the list and added some of my own.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;action = [
	&amp;quot;access&amp;quot;,
	&amp;quot;alert&amp;quot;,
	&amp;quot;allocate&amp;quot;,
	&amp;quot;allow&amp;quot;,
	.
	.
	.
]

status = [
	&amp;quot;accept&amp;quot;,
	&amp;quot;error&amp;quot;,
	&amp;quot;fail&amp;quot;,
	&amp;quot;failure&amp;quot;,
	&amp;quot;success&amp;quot;
]

object = [
	&amp;quot;account&amp;quot;,
	&amp;quot;app&amp;quot;,
	&amp;quot;bios&amp;quot;,
	&amp;quot;driver&amp;quot;,
	.
	.
	.
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In our labeling process, we basically goes through and identify all the string literals that are NOT marked as keys, and perform a &lt;a href=&#34;https://github.com/surge/porter2&#34;&gt;porter2 stemming operation&lt;/a&gt; on the literal, then compare to the above list (which is also porter2 stemmed).&lt;/p&gt;

&lt;p&gt;If a literal matches one of the above lists, then the corresponding label (&lt;code&gt;action&lt;/code&gt;, &lt;code&gt;status&lt;/code&gt;, &lt;code&gt;object&lt;/code&gt;, &lt;code&gt;srcuser&lt;/code&gt;, &lt;code&gt;method&lt;/code&gt;, or &lt;code&gt;protocol&lt;/code&gt;) is applied.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Determining Positions of Specific Types&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this next step, we are basically looking at the position of where some of the token types appear. Specifically, we are looking for &lt;code&gt;%time%&lt;/code&gt;, &lt;code&gt;%url%&lt;/code&gt;, &lt;code&gt;%mac%&lt;/code&gt;, &lt;code&gt;%ipv4%&lt;/code&gt;, &lt;code&gt;%host%&lt;/code&gt;, and &lt;code&gt;%email%&lt;/code&gt; tokens. Assuming the labels have not already been taken with the previous rules, the rules are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first %time% token is labeled as %msgtime%&lt;/li&gt;
&lt;li&gt;The first %url% token is labeled as %object%&lt;/li&gt;
&lt;li&gt;The first %mac% token is labeled as %srcmac% and the second is labeld as %dstmac%&lt;/li&gt;
&lt;li&gt;The first %ipv4% token is labeled as %srcipv4% and the second is labeld as %dstipv4%&lt;/li&gt;
&lt;li&gt;The first %host% token is labeled as %srchost% and the second is labeld as %dsthost%&lt;/li&gt;
&lt;li&gt;The first %email% token is labeled as %srcemail% and the second is labeld as %dstemail%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;6. Scanning for ip/port or ip:port Pairs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Finally, after all that, we scan through the sequence again, and identify any numbers that follow an IP address, but separated by either a &amp;ldquo;/&amp;rdquo; or &amp;ldquo;:&amp;ldquo;. Then we label these numbers as either &lt;code&gt;%srcport%&lt;/code&gt; or &lt;code&gt;%dstport%&lt;/code&gt; based on how the previous IP address is labeled.&lt;/p&gt;

&lt;h3 id=&#34;summary:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;There are some limitations to the &lt;code&gt;sequence&lt;/code&gt; parser and analyzer. For example, currently &lt;code&gt;sequence&lt;/code&gt; does not handle multi-line logs. Each log message must appear as a single line. So if there&amp;rsquo;s multi-line logs, they must be first be converted into a single line. Also, &lt;code&gt;sequence&lt;/code&gt; has been only tested with a limited set of system (Linux, AIX, sudo, ssh, su, dhcp, etc etc), network (ASA, PIX, Neoteris, CheckPoint, Juniper Firewall) and infrastructure application (apache, bluecoat, etc) logs.&lt;/p&gt;

&lt;p&gt;Documentation is available at godoc: &lt;a href=&#34;http://godoc.org/github.com/strace/sequence&#34;&gt;package&lt;/a&gt;, &lt;a href=&#34;http://godoc.org/github.com/strace/sequence/cmd/sequence&#34;&gt;command&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are some pattern files developed for ASA, Sudo and SSH in the &lt;code&gt;patterns&lt;/code&gt; directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages.&lt;/p&gt;

&lt;p&gt;If you have a set of logs you would like me to test out, please feel free to &lt;a href=&#34;https://github.com/strace/sequence/issues&#34;&gt;open an issue&lt;/a&gt; and we can arrange a way for me to download and test your logs.&lt;/p&gt;

&lt;p&gt;Stay tuned for more log patterns&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Papers I Read: 2015 Week 6</title>
      <link>http://zhen.org/blog/papers-i-read-2015-week-6/</link>
      <pubDate>Sun, 08 Feb 2015 19:57:56 -0800</pubDate>
      
      <guid>http://zhen.org/blog/papers-i-read-2015-week-6/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://paperswelove.org/&#34;&gt;Papers We Love&lt;/a&gt; has been making rounds lately and a lot of people are excited about it. I also think it&amp;rsquo;s kind of cool since I&amp;rsquo;ve been reading a lot of research papers over the past year or so. I have been killing some trees because of that.&lt;/p&gt;

&lt;p&gt;My interests have been mostly around data analytics, but the specific focus areas have changed a few times. I have read papers on data structures (bloom filters, skiplist, bitmap compression, etc), security analytics, consumer behavioral analysis, loyalty analytics, and now back to security analytics. In fact, recently I started reading a few security research papers that I found on &lt;a href=&#34;http://www.covert.io/&#34;&gt;covert.io&lt;/a&gt;, put together by Jason Trost.&lt;/p&gt;

&lt;p&gt;In any case, I thought it might be an interesting idea to share some of the papers I read/scan/skim on weekly basis. This way I can also track what I read over time.&lt;/p&gt;

&lt;h3 id=&#34;random-ramblings:e2137c343047358d9912399632750231&#34;&gt;Random Ramblings&lt;/h3&gt;

&lt;p&gt;This week has been a disaster. I was the last one in the family to catch the cold, but probably lasted the longest. In fact I am still only about 50%. This whole week I have been having headaches, body aches, and haven&amp;rsquo;t been able to concentrate. My body must be trying to catch up on sleep or something. For a couple days I actually slept for almost 12 hours a night!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been meaning to work on &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; and finish updating the analyzer, but really had a hard time concentrating. Any non-working hours are basically spent in bed if I could.&lt;/p&gt;

&lt;p&gt;So this is probably the worst week to start the &amp;ldquo;Papers I Read&amp;rdquo; series since I only technically read 1 paper. But I am going to cheat a little, and list the papers I read over the past couple of weeks, pretty much all in my spare time.&lt;/p&gt;

&lt;p&gt;This week we also saw Sony&amp;rsquo;s accouncement that last year&amp;rsquo;s hack cost them &lt;a href=&#34;http://www.sony.net/SonyInfo/IR/financial/fr/150204_sony.pdf&#34;&gt;$15 million&lt;/a&gt; to investigate and remediate. It&amp;rsquo;s pretty crazy if you think about it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s assume that they hired a bunch of high-priced consultants, say $250/hour, to help comb through the logs and clean the systems. And let&amp;rsquo;s say &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; of the $15m is spent on these consultants. That&amp;rsquo;s &lt;code&gt;$10m / $250 = 40,000 hours&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say these consultants worked full time, non-stop, no weekends, no breaks, for 2 months since the announcement on Nov 24, 2014, that would be a team of 56 people (&lt;code&gt;40,000 hours / 60 days / 12 hours/day = 56&lt;/code&gt;) working 12 hour days!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll tell ya, these security guys are raking it in. They make money upfront by selling products/services to protect the company, then they make money in the back by selling forensic services to clean up after the hack.&lt;/p&gt;

&lt;p&gt;[Disclaimer: any mistake in my calculations/assumptions I blame on my drugged brain cells.]&lt;/p&gt;

&lt;h3 id=&#34;papers-i-read:e2137c343047358d9912399632750231&#34;&gt;Papers I Read&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf&#34;&gt;Beehive: Large-Scale Log Analysis for Detecting Suspicious Activity in Enterprise Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://minds.cs.umn.edu/publications/chapter.pdf&#34;&gt;Data Mining for Cyber Security&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;This chapter provides an overview of the Minnesota Intrusion Detection System (MINDS), which uses a suite of data mining based algorithms to address different aspects of cyber security. The various components of MINDS such as the scan detector, anomaly detector and the profiling module detect different types of attacks
and intrusions on a computer network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/VAST-%20Network%20Visibility%20Across%20Space%20and%20Time.pdf&#34;&gt;VAST: Network Visibility Across Space and Time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Key operational networking tasks, such as troubleshooting and defending against attacks, greatly benefit from attaining views of network activity that are unified across space and time. This means that data from heterogeneous devices and systems is treated in a uniformfashion, and that analyzing past activity and detecting future instances follow the same procedures. Based on previous ideas that formulated principles for comprehensive
network visibility [AKP+08], we present the design and architecture of Visibility Across Space and Time (VAST), an intelligent database that serves as a single vantage point into the network. The system is based on a generic event model to handle network data from disparate sources and provides a query architecture that allows operators or remote applications to extract events matching a given condition. We implemented a proof-of-principle prototype that can archive and index events from a wide range of sources. Moreover, we conducted a preliminary performance evaluation to verify that our implementation works efficient and as expected.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/Finding%20The%20Needle-%20Suppression%20of%20False%20Alarms%20in%20Large%20Intrusion%20Detection%20Data%20Sets.pdf&#34;&gt;Finding The Needle: Suppression of False Alarms in Large Intrusion Detection Data Sets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Managed security service providers (MSSPs) must manage and monitor thousands of intrusion detection sensors.
The sensors often vary by manufacturer and software version, making the problem of creating generalized tools to separate true attacks from false positives particularly difficult. Often times it is useful from an operations perspective to know if a particular sensor is acting out of character. We propose a solution to this problem using anomaly detection techniques over the set of alarms produced by the sensors. Similar to the manner in which an anomaly based sensor detects deviations from normal user or system behavior, we establish the baseline
behavior of a sensor and detect deviations from this baseline. We show that departures from this profile by a sensor have a high probability of being artifacts of genuine attacks. We evaluate a set of time-based Markovian heuristics against a simple compression algorithm and show that we are able to detect the existence of all attacks which were manually identified by security personnel, drastically reduce the number of false positives, and identify attacks which were overlooked during manual evaluation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://user.informatik.uni-goettingen.de/~krieck/docs/2013a-aisec.pdf&#34;&gt;A Close Look on n-Grams in Intrusion Detection: Anomaly Detection vs. Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Detection methods based on n-gram models have been widely studied for the identification of attacks and malicious software. These methods usually build on one of two learning schemes: anomaly detection, where a model of normality is constructed from n-grams, or classification, where a discrimination between benign and malicious n-grams is learned. Although successful in many security domains, previous work falls short of explaining why a particular scheme is used and more importantly what renders one favorable over the other for a given type of data. In this paper we provide a close look on n-gram models for intrusion detection. We specifically study anomaly detection and classification using n-grams and develop criteria for data being used in one or the other
scheme. Furthermore, we apply these criteria in the scope of web intrusion detection and empirically validate their effectiveness with different learning-based detection methods for client-side and service-side attacks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.scalyr.com/2014/05/searching-20-gbsec-systems-engineering-before-algorithms/&#34;&gt;Searching 20 GB/sec: Systems Engineering Before Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ok, this is a blog post, not a research paper, but it&amp;rsquo;s somewhat interesting nonetheless.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This article describes how we met that challenge using an “old school”, brute-force approach, by eliminating layers and avoiding complex data structures. There are lessons here that you can apply to your own engineering challenges.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS</title>
      <link>http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/</link>
      <pubDate>Sun, 01 Feb 2015 10:40:20 -0800</pubDate>
      
      <guid>http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/</guid>
      <description>

&lt;p&gt;Information here maybe outdated. Please visit &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;http://sequence.trustpath.com&lt;/a&gt; for latest.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This is part 1 of the &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;sequence&lt;/a&gt; series.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/&#34;&gt;Part 1&lt;/a&gt; is about the high performance parser that can parse 100,000-200,000 MPs.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/&#34;&gt;Part 2&lt;/a&gt; is about automating the process of reducing 100 of 1000&amp;rsquo;s of log messages down to dozens of unique patterns.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/&#34;&gt;Part 3&lt;/a&gt; is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size) for scanning and tokenizing log messages&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;background:d4419a1968098b45153921045f06326c&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;sequence&lt;/code&gt; is a &lt;em&gt;high performance sequential log parser&lt;/em&gt;. It &lt;em&gt;sequentially&lt;/em&gt; goes through a log message, &lt;em&gt;parses&lt;/em&gt; out the meaningful parts, without the use regular expressions. It can achieve &lt;em&gt;high performance&lt;/em&gt; parsing of &lt;strong&gt;100,000 - 200,000 messages per second (MPS)&lt;/strong&gt; without the need to separate parsing rules by log source type.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;sequence&lt;/code&gt; is currently under active development and should be considered unstable until further notice.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you have a set of logs you would like me to test out, please feel free to &lt;a href=&#34;https://github.com/strace/sequence/issues&#34;&gt;open an issue&lt;/a&gt; and we can arrange a way for me to download and test your logs.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;motivation:d4419a1968098b45153921045f06326c&#34;&gt;Motivation&lt;/h3&gt;

&lt;p&gt;Log messages are notoriusly difficult to parse because they all have different formats. Industries (see Splunk, ArcSight, Tibco LogLogic, Sumo Logic, Logentries, Loggly, LogRhythm, etc etc etc) have been built to solve the problems of parsing, understanding and analyzing log messages.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you have a bunch of log files you like to parse. The first problem you will typically run into is you have no way of telling how many DIFFERENT types of messages there are, so you have no idea how much work there will be to develop rules to parse all the messages. Not only that, you have hundreds of thousands, if not  millions of messages, in front of you, and you have no idea what messages are worth parsing, and what&amp;rsquo;s not.&lt;/p&gt;

&lt;p&gt;The typical workflow is develop a set of regular expressions and keeps testing against the logs until some magical moment where all the logs you want parsed are parsed. Ask anyone who does this for a living and they will tell you this process is long, frustrating and error-prone.&lt;/p&gt;

&lt;p&gt;Even after you have developed a set of regular expressions that match the original set of messages, if new messages come in, you will have to determine which of the new messages need to be parsed. And if you develop a new set of regular expressions to parse those new messages, you still have no idea if the regular expressions will conflict with the ones you wrote before. If you write your regex parsers too liberally, it can easily parse the wrong messages.&lt;/p&gt;

&lt;p&gt;After all that, you will end up finding out the regex parsers are quite slow. It can typically parse several thousands messages per second. Given enough CPU resources on a large enough machine, regex parsers can probably parse tens of thousands of messages per second. Even to achieve this type of performance, you will likely need to limit the number of regular expressions the parser has. The more regex rules, the slower the parser will go.&lt;/p&gt;

&lt;p&gt;To work around this performance issue, companies have tried to separate the regex rules for different log message types into different parsers. For example, they will have a parser for Cisco ASA logs, a parser for sshd logs, a parser for Apache logs, etc etc. And then they will require the users to tell them which parser to use (usually by indicating the log source type of the originating IP address or host.)&lt;/p&gt;

&lt;p&gt;Sequence is developed to make analyzing and parsing log messages a lot easier and faster.&lt;/p&gt;

&lt;h3 id=&#34;performance:d4419a1968098b45153921045f06326c&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro. The first file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 1.69 secs, ~ 126319.27 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 2.89 secs, ~ 81323.41 msgs/sec

  $ ./sequence bench -d ../patterns -i ../data/asasshsudo.log
  Parsed 447745 messages in 4.47 secs, ~ 100159.65 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.00 secs, ~ 212711.83 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 1.56 secs, ~ 150769.68 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -d ../patterns -i ../data/asasshsudo.log -w 2
  Parsed 447745 messages in 2.52 secs, ~ 177875.94 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;documentation:d4419a1968098b45153921045f06326c&#34;&gt;Documentation&lt;/h3&gt;

&lt;p&gt;Documentation is available at godoc: &lt;a href=&#34;http://godoc.org/github.com/strace/sequence&#34;&gt;package&lt;/a&gt;, &lt;a href=&#34;http://godoc.org/github.com/strace/sequence/sequence&#34;&gt;command&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;license:d4419a1968098b45153921045f06326c&#34;&gt;License&lt;/h3&gt;

&lt;p&gt;Copyright &amp;copy; 2014 Dataence, LLC. All rights reserved.&lt;/p&gt;

&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &amp;ldquo;License&amp;rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;ldquo;AS IS&amp;rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.&lt;/p&gt;

&lt;h3 id=&#34;roadmap-futures:d4419a1968098b45153921045f06326c&#34;&gt;Roadmap / Futures&lt;/h3&gt;

&lt;p&gt;There are some pattern files developed for ASA, Sudo and SSH in the &lt;code&gt;patterns&lt;/code&gt; directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages. So currently there&amp;rsquo;s not a set roadmap.&lt;/p&gt;

&lt;h2 id=&#34;concepts:d4419a1968098b45153921045f06326c&#34;&gt;Concepts&lt;/h2&gt;

&lt;p&gt;The following concepts are part of the package:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;Token&lt;/em&gt; is a piece of information extracted from the original log message. It is a struct that contains fields for &lt;em&gt;TokenType&lt;/em&gt;, &lt;em&gt;FieldType&lt;/em&gt;, &lt;em&gt;Value&lt;/em&gt;, and indicators of whether it&amp;rsquo;s a key or value in the key=value pair.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;TokenType&lt;/em&gt; indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;FieldType&lt;/em&gt; indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Sequence&lt;/em&gt; is a list of Tokens. It is returned by the &lt;em&gt;Tokenizer&lt;/em&gt;, and the &lt;em&gt;Parser&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Scanner&lt;/em&gt; is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, IPv4 addresses, URLs, MAC addresses,
integers and floating point numbers. It also recgonizes key=value or key=&amp;ldquo;value&amp;rdquo; or key=&amp;lsquo;value&amp;rsquo; or key=&lt;value&gt; pairs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Parser&lt;/em&gt; is a tree-based parsing engine for log messages. It builds a parsing tree based on pattern sequence supplied, and for each message sequence, returns the matching pattern sequence. Each of the message tokens will be marked with the semantic field types.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sequence-command:d4419a1968098b45153921045f06326c&#34;&gt;Sequence Command&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; command is developed to demonstrate the use of this package. You can find it in the &lt;code&gt;sequence&lt;/code&gt; directory. The &lt;code&gt;sequence&lt;/code&gt; command implements the &lt;em&gt;sequential semantic log parser&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   Usage:
     sequence [command]

   Available Commands:
     scan                      scan will tokenize a log file or message and output a list of tokens
     parse                     parse will parse a log file and output a list of parsed tokens for each of the log messages
     bench                     benchmark the parsing of a log file, no output is provided
     help [command]            Help about any command
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scan:d4419a1968098b45153921045f06326c&#34;&gt;Scan&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence scan [flags]

   Available Flags:
    -h, --help=false: help for scan
    -m, --msg=&amp;quot;&amp;quot;: message to tokenize
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence scan -m &amp;quot;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream&amp;quot;
  #   0: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 14 10:15:56&amp;quot; }
  #   1: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;testserver&amp;quot; }
  #   2: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sudo&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   4: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;gonner&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;tty&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pts/3&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;pwd&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/home/gonner&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  14: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  15: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;root&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  18: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;command&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/bin/su&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;-&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;ustream&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;parse:d4419a1968098b45153921045f06326c&#34;&gt;Parse&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence parse [flags]

   Available Flags:
    -h, --help=false: help for parse
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -o, --outfile=&amp;quot;&amp;quot;: output file, if empty, to stdout
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&amp;quot;&amp;quot;: initial pattern file, required
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command parses a file based on existing rules. Note that the
performance number (9570.20 msgs/sec) is mostly due to reading/writing to disk.
To get a more realistic performance number, see the benchmark section below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence parse -d ../../patterns -i ../../data/sshd.all  -o parsed.sshd
  Parsed 212897 messages in 22.25 secs, ~ 9570.20 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is an entry from the output file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
  #   0: { Field=&amp;quot;%createtime%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 15 19:39:26&amp;quot; }
  #   1: { Field=&amp;quot;%apphost%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;jlz&amp;quot; }
  #   2: { Field=&amp;quot;%appname%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;sshd&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;[&amp;quot; }
  #   4: { Field=&amp;quot;%sessionid%&amp;quot;, Type=&amp;quot;%integer%&amp;quot;, Value=&amp;quot;7778&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;]&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pam_unix&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;(&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sshd&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;session&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;)&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #  14: { Field=&amp;quot;%object%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;session&amp;quot; }
  #  15: { Field=&amp;quot;%action%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;opened&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;for&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  18: { Field=&amp;quot;%dstuser%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;jlz&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;by&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;(&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;uid&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  23: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%integer%&amp;quot;, Value=&amp;quot;0&amp;quot; }
  #  24: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;)&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;benchmark:d4419a1968098b45153921045f06326c&#34;&gt;Benchmark&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence bench [flags]

   Available Flags:
    -c, --cpuprofile=&amp;quot;&amp;quot;: CPU profile filename
    -h, --help=false: help for bench
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&amp;quot;&amp;quot;: pattern file, required
    -w, --workers=1: number of parsing workers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will benchmark the parsing of two files. First file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA
log file, averaging 180 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 1.69 secs, ~ 126319.27 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 2.89 secs, ~ 81323.41 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.00 secs, ~ 212711.83 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 1.56 secs, ~ 150769.68 msgs/sec
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Generating Porter2 FSM For Fun and Performance in Go</title>
      <link>http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/</link>
      <pubDate>Wed, 21 Jan 2015 20:48:44 -0800</pubDate>
      
      <guid>http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://godoc.org/github.com/surgebase/porter2&#34;&gt;&lt;img src=&#34;http://godoc.org/github.com/surgebase/porter2?status.svg&#34; alt=&#34;GoDoc&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;tl-dr:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;tl;dr&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This post describes the &lt;a href=&#34;https://github.com/surgebase/porter2&#34;&gt;Porter2&lt;/a&gt; package I implemented. It is written in Go (#golang).&lt;/li&gt;
&lt;li&gt;By using a &lt;a href=&#34;http://en.wikipedia.org/wiki/Finite-state_machine&#34;&gt;finite-state-machine&lt;/a&gt; approach to &lt;a href=&#34;http://snowball.tartarus.org/algorithms/english/stemmer.html&#34;&gt;Porter2&lt;/a&gt; &lt;a href=&#34;http://en.wikipedia.org/wiki/Stemming&#34;&gt;stemming&lt;/a&gt;, I was able to achieve 660% better performance compare to other Go implementations.&lt;/li&gt;
&lt;li&gt;FSM-based approach is great for known/fixed data set, but obviously not workable if the data set changes at runtime.&lt;/li&gt;
&lt;li&gt;Hand-coding FSM is a PITA!!! &lt;a href=&#34;https://github.com/surgebase/porter2/tree/master/cmd/suffixfsm&#34;&gt;Automate&lt;/a&gt; if possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;introduction:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In a personal project I am working on, I had the need to perform word stemming in two scenarios. First, I need to perform stemming for all the string literals in a LARGE corpus and then determine if the words are in a fixed set of literals. Second, I need to perform stemming for a subset of words in real-time, as messages stream in.&lt;/p&gt;

&lt;p&gt;In the first case, performance is important but not critical; in the second case, performance is a huge factor.&lt;/p&gt;

&lt;h3 id=&#34;stemming:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Stemming&lt;/h3&gt;

&lt;p&gt;To start, according to &lt;a href=&#34;http://en.wikipedia.org/wiki/Stemming&#34;&gt;wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Stemming is the term used in linguistic morphology and information retrieval to describe the process for reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As a quick example, the words &lt;em&gt;fail&lt;/em&gt;, &lt;em&gt;failed&lt;/em&gt;, and &lt;em&gt;failing&lt;/em&gt; all mean something has &lt;em&gt;failed&lt;/em&gt;. By stemming these three words, I will get a single form which is &lt;em&gt;fail&lt;/em&gt;. I can then just use &lt;em&gt;fail&lt;/em&gt; going forward instead of having to compare all three forms all the time.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://tartarus.org/martin/PorterStemmer/def.txt&#34;&gt;Porter&lt;/a&gt; stemming algorithm is by far the most commonly used stemmer and also considered to be one of the most gentle stemmers. The Porter stemming algorithm (or ‘Porter stemmer’) works by removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems. (&lt;a href=&#34;http://tartarus.org/martin/PorterStemmer/&#34;&gt;ref&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://snowball.tartarus.org/algorithms/english/stemmer.html&#34;&gt;Porter2&lt;/a&gt; is universally considered to be an enhancement over the original Porter algorithm. Porter2 has an improved set of rules and it&amp;rsquo;s widely used as well.&lt;/p&gt;

&lt;h2 id=&#34;implementation:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;This package, &lt;a href=&#34;https://github.com/surgebase/porter2&#34;&gt;Porter2&lt;/a&gt;, implements the Porter2 stemmer. It is written completely using finite state machines to perform suffix comparison, rather than the usual string-based or tree-based approaches. As a result, it is 660% faster compare to string comparison-based approach written in the same (Go) language.&lt;/p&gt;

&lt;p&gt;This implementation has been successfully validated with the dataset from &lt;a href=&#34;http://snowball.tartarus.org/algorithms/english/&#34;&gt;http://snowball.tartarus.org/algorithms/english/&lt;/a&gt;, so it should be in a usable state. If you encounter any issues, please feel free to &lt;a href=&#34;https://github.com/surgebase/porter2/issues&#34;&gt;open an issue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Usage is fairly simple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import &amp;quot;github.com/surgebase/porter2&amp;quot;

fmt.Println(porter2.Stem(&amp;quot;seaweed&amp;quot;)) // should get seawe
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;performance:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;This implementation by far has the highest performance of the various Go-based implementations, AFAICT. I tested a few of the implementations and the results are below.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Implementation&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Algorithm&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/surgebase/porter2&#34;&gt;surgebase&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;319.009358ms&lt;/td&gt;
&lt;td&gt;Porter2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/dchest/stemmer&#34;&gt;dchest&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.106912401s&lt;/td&gt;
&lt;td&gt;Porter2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kljensen/snowball&#34;&gt;kljensen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;5.725917198s&lt;/td&gt;
&lt;td&gt;Porter2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To run the test again, you can run &lt;a href=&#34;https://github.com/surgebase/porter2/tree/master/cmd/compare&#34;&gt;compare.go&lt;/a&gt; (&lt;code&gt;go run compare.go&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&#34;state-machines:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;State Machines&lt;/h3&gt;

&lt;p&gt;Most of the implementations, like the ones in the table above, rely completely on suffix string comparison. Basically there&amp;rsquo;s a list of suffixes, and the code will loop through the list to see if there&amp;rsquo;s a match. Given most of the time you are looking for the longest match, so you order the list so the longest is the first one. So if you are luckly, the match will be early on the list. But regardless that&amp;rsquo;s a huge performance hit.&lt;/p&gt;

&lt;p&gt;This implementation is based completely on finite state machines to perform suffix comparison. You compare each chacter of the string starting at the last character going backwards. The state machines will determine what the longest suffix is.&lt;/p&gt;

&lt;p&gt;As an example, let&amp;rsquo;s look at the 3 suffixes from step0 of the porte2 algorithm. The goal, and it&amp;rsquo;s the same for all the other steps, it&amp;rsquo;s to find the longest matching suffix.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;
&#39;s
&#39;s&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you were to build a non-space-optimized &lt;a href=&#34;http://en.wikipedia.org/wiki/Suffix_tree&#34;&gt;suffix tree&lt;/a&gt;, you would get this, where R is the root of the tree, and any node with * is designated as a final state:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        R
       / \
      &#39;*  s
     /     \
    s       &#39;*
   /
  &#39;*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a fairly easy tree to build, and we actually did that in the FSM generator we will talk about later. However, to build a working suffix tree in Go, we would need to use a &lt;code&gt;map[rune]*node&lt;/code&gt; structure at each of the nodes. And then search the map for each rune we encounter.&lt;/p&gt;

&lt;p&gt;To test the performance of using a switch statement vs using a map, I wrote a &lt;a href=&#34;https://github.com/surgebase/porter2/tree/master/cmd/switchvsmap&#34;&gt;quick test&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;switch: 4.956523ms
   map: 10.016601ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The test basically runs a switch statement and a map each for 1,000,000 times. So it seems like using a switch statement is faster than a map. Though I think the compiler basically builds a map for all the switch case statements.  (Maybe we should call this post &lt;em&gt;Microbenchmarking for fun and performance&lt;/em&gt;?)&lt;/p&gt;

&lt;p&gt;In any case, let&amp;rsquo;s go with the switch approach. We basically need to unroll the suffix tree into a finite state machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        R0
       / \
      &#39;1* s2
     /     \
    s3      &#39;4*
   /
  &#39;5*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To do that, we need to assign a state number to each of the nodes in the suffix tree, and output each of the states and the transitions based on the rune encountered. The tree above is the same as the one before, but now has a state number assigned to each node.&lt;/p&gt;

&lt;h3 id=&#34;generator:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Generator&lt;/h3&gt;

&lt;p&gt;I actually started building all the porter2 FSMs manually with a completely different approach than what I am describing here. I won&amp;rsquo;t go into details here but needless to say, it was disastrous. Not only was hand coding state machines extremely error-prone, the approach I was taking also had a lot of potential for bugs. It took me MANY HOURS to hand build those FSMs but at the end, I was happy to abandon all of them for the approach I am taking now.&lt;/p&gt;

&lt;p&gt;To reduce errors and make updating the FSM easier, I wrote a quick tool called &lt;a href=&#34;https://github.com/surgebase/porter2/tree/master/cmd/suffixfsm&#34;&gt;suffixfsm&lt;/a&gt; to generate the FSMs. The tool basically takes a list of suffixes, creates a suffix tree as described above, and unrolls the tree into a set of states using the &lt;code&gt;switch&lt;/code&gt; statement.&lt;/p&gt;

&lt;p&gt;It took me just a couple hours to write and debug the tool, and I was well on my way to fixing other bugs!&lt;/p&gt;

&lt;p&gt;For example, running the command &lt;code&gt;go run suffixfsm.go step0.txt&lt;/code&gt; generated the following code. This is a complete function for step0 of the porter2 algorithm. The only thing missing is what to do with each of the final states, which are in the last &lt;code&gt;switch&lt;/code&gt; statement.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
		l int = len(rs) // string length
		m int			// suffix length
		s int			// state
		f int			// end state of longgest suffix
		r rune			// current rune
	)

loop:
	for i := 0; i &amp;lt; l; i++ {
		r = rs[l-i-1]

		switch s {
		case 0:
			switch r {
			case &#39;\&#39;&#39;:
				s = 1
				m = 1
				f = 1
				// &#39; - final
			case &#39;s&#39;:
				s = 2
			default:
				break loop
			}
		case 1:
			switch r {
			case &#39;s&#39;:
				s = 4
			default:
				break loop
			}
		case 2:
			switch r {
			case &#39;\&#39;&#39;:
				s = 3
				m = 2
				f = 3
				// &#39;s - final
			default:
				break loop
			}
		case 4:
			switch r {
			case &#39;\&#39;&#39;:
				s = 5
				m = 3
				f = 5
				// &#39;s&#39; - final
			default:
				break loop
			}
		default:
			break loop
		}
	}

	switch f {
	case 1:
		// &#39; - final

	case 3:
		// &#39;s - final

	case 5:
		// &#39;s&#39; - final

	}

	return rs
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;finally:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Finally&lt;/h2&gt;

&lt;p&gt;This is a technique that can probably be applied to any fixed data set. The performance may vary based on the size of the state machine so test it with both maps and FSM to see what works best.&lt;/p&gt;

&lt;p&gt;Happy Go&amp;rsquo;ing!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go: From a Non-Programmer&#39;s Perspective</title>
      <link>http://zhen.org/blog/golang-from-a-non-programmers-perspective/</link>
      <pubDate>Tue, 13 Jan 2015 13:30:00 -0800</pubDate>
      
      <guid>http://zhen.org/blog/golang-from-a-non-programmers-perspective/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Warning: Long Post. Over 3900 words according to &lt;code&gt;wc&lt;/code&gt;. So read at your own risk. :)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://golang.org/&#34;&gt;Go&lt;/a&gt; is a fairly recent programming language &lt;a href=&#34;http://golang.org/doc/faq#history&#34;&gt;created&lt;/a&gt; by Robert Griesemer, Rob Pike and Ken Thompson of Google. It has risen in popularity over the the past few years, especially since Go 1.0 was released.&lt;/p&gt;

&lt;p&gt;There are a ton of posts out there that talks about the pros and cons of Go, and why one would use it or not. In addition, there&amp;rsquo;s a bunch of posts out there written by different developers coming from different perspectives, such as Python, Ruby, Node, Rust, etc, etc. Recently I even read a couple of Chinese blog posts on why Go is popular in China, and why some of the Chinese developers have abandoned Go, which are quite interesting as well.&lt;/p&gt;

&lt;p&gt;This post is my perspective of Go, how I picked it up, and what I think of it after using it for a while. It is not a post about why Go is better or worse than other languages.&lt;/p&gt;

&lt;p&gt;In short, I like Go. It&amp;rsquo;s the first programming language I&amp;rsquo;ve used in recent years that I can actually build some interesting projects, e.g., &lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt; (&lt;a href=&#34;http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/&#34;&gt;detailed post&lt;/a&gt;), in my limited spare time.&lt;/p&gt;

&lt;h2 id=&#34;my-background:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;My Background&lt;/h2&gt;

&lt;p&gt;I am not a programmer/developer. Not full-time, not part-time, not moonlight. I tell my colleagues and teams that &amp;ldquo;I am not technical.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;But I do have a technical background. I have a MSCS degree from way back when, and have spent the first 6-7 years of my career performing security audits and penetration tests, and building one of the world&amp;rsquo;s largest managed security services (at least at the time).&lt;/p&gt;

&lt;p&gt;My programming langauge progression, when I was technical, has been BASIC (high school), Pascal and C (college), Perl, PHP, Java, and Javascript (during my technical career). I can&amp;rsquo;t claim to be an &amp;ldquo;expert&amp;rdquo; in any of these languages, but I consider myself quite proficient in each at the time I was using them.&lt;/p&gt;

&lt;p&gt;I was also reasonably network and system savvy, in the sense that I can get myself in and around the Solaris and Linux (UN*X) systems pretty well, and understand the networking stack sufficiently. I consider myself fairly proficient with the various system commands and tools.&lt;/p&gt;

&lt;p&gt;For the past 12 years, however, I have not been a developer, nor a systems guy, nor a networking guy. Instead, I have been running product management for various startups and large companies in the security and infrastructure space.&lt;/p&gt;

&lt;p&gt;Since the career change, I&amp;rsquo;ve not done any meaningful code development. I&amp;rsquo;ve written a script here and there, but nothing that I would consider to be &amp;ldquo;software.&amp;rdquo; However, I&amp;rsquo;ve managed engineering teams as part of my resonsibility, in addition to product management, to produce large scale software.&lt;/p&gt;

&lt;p&gt;In the past 12 years, my most used IDE is called Microsoft Office. So, in short, I am probably semi-technical, and know just enough to be dangerous.&lt;/p&gt;

&lt;h3 id=&#34;my-history-with-go:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;My History with Go&lt;/h3&gt;

&lt;p&gt;In &lt;sup&gt;2011&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2012&lt;/sub&gt;, I had the responsibility of building a brand new engineering team (I was already running product management) at VMware to embark on a new strategic initiative. The nature of the product/service is not important now. However, at the time, because the team is brand new, we had some leeway in choosing a language for the project. VMware at the time was heavily Java, and specifically Spring given the &lt;a href=&#34;http://www.vmware.com/company/news/releases/springsource&#34;&gt;2009 acquisition of SpringSource&lt;/a&gt;. While the new team had mostly Java experience, there was desire to choose something less bloated, and something that had good support for the emerging patterns of distributed software.&lt;/p&gt;

&lt;h4 id=&#34;first-touch:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;First Touch&lt;/h4&gt;

&lt;p&gt;Some of the team members had experience with Scala, so that became an obvious option. I did some research on the web, and found some discussions of Go. At the time, Go hasn&amp;rsquo;t reached 1.0 yet, but there was already a buzz around it. I looked on Amazon, and found &lt;a href=&#34;http://www.amazon.com/gp/product/B0083RVAJW/ref=docs-os-doi_0&#34;&gt;The Way to Go&lt;/a&gt;, which was probably the only Go book around at the time. For $3 on the Kindle, it was well worth it. However, due to the nascent nature of Go (pre 1.0), it was not a comfortable choice so I didn&amp;rsquo;t put that as an option. But this was my &lt;strong&gt;first touch of Go and it felt relatively painless&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;At the end, the team chose Scala because of existing experience, and that in theory, people with Java experience should move fairly easily to Scala. We were the first team in VMware to use Scala and we were pretty excited about it.&lt;/p&gt;

&lt;p&gt;However, to this date, I am still not sure we made the right decision to move to Scala (not that it&amp;rsquo;s wrong either.) The learning curve I believe was higher than we originally anticipated. Many of the developers wrote Java code w/ Scala syntax. And hiring also became an issue. Basically every new developer that came onboard must be sent to Typesafe for training. It was simply not easy for most developers who came from a non-functional mindset to jump into a totally functional mindset. Lastly, the knowledge differences of new Scala developers and experienced ones made it more difficult for them to collaborate.&lt;/p&gt;

&lt;p&gt;I also tried to read up Scala and at least understand the concept. I even tried to take the online course on Coursera offered by Martin Odersky. However, I just could not get my non-functional mind to wrap around the functional Scala. And since I really didn&amp;rsquo;t need to code (nor the developers want me to), I gave up on learning Scala.&lt;/p&gt;

&lt;h4 id=&#34;second-touch:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Second Touch&lt;/h4&gt;

&lt;p&gt;In any case, fast forward 2 years to Q3 of 2013. I had since left VMware and joined my current company, &lt;a href=&#34;http://jolata.com&#34;&gt;Jolata&lt;/a&gt;, to build a big data network analytics solutions for mobile carriers and high-frequency trading financial services firms. We are a small startup that&amp;rsquo;s trying to do a ton of things. So even though I run products, I have to get my hands dirty often.&lt;/p&gt;

&lt;p&gt;One of the things we had to do as a company is to build a repeatable demo environment. The goal is to have a prebuilt vagrant VM that we can run on our Macs, and we can demonstrate our product without connecting to the network. The requirement was that we had an interesting set of data in the database so we can walk through different scenarios.&lt;/p&gt;

&lt;p&gt;The data set we needed was network flow data. And to make the UI look realistic, interesting and non-blocky, we wanted to generate noisy data so the UI looks like it&amp;rsquo;s monitoring a real network. Because all of the developers are focused on feature development, I took on the task of building out the data set.&lt;/p&gt;

&lt;p&gt;By now, Go has released v1.1 and on its way to 1.2. It was then I started seriously considering Go as a candidate for this project. To build a tool that can generate the data set, we needed two libraries. The first is a &lt;a href=&#34;http://en.wikipedia.org/wiki/Perlin_noise&#34;&gt;Perlin Noise&lt;/a&gt; generator, and the second is &lt;a href=&#34;https://code.google.com/p/cityhash/&#34;&gt;Google&amp;rsquo;s Cityhash&lt;/a&gt;. Neither of these were available in Go (or not that I remember). I thought this would be a great opportunity to test out Go. The end results were my Go Learn Projects &lt;a href=&#34;https://github.com/dataence/perlinnoise&#34;&gt;#0 Perlin&lt;/a&gt;, and &lt;a href=&#34;https://github.com/dataence/cityhash&#34;&gt;#1 Cityhash&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Both of these projects were relatively simple since I didn&amp;rsquo;t have to spend a lot of time figuring out HOW to write them. Perlin Noise has well-established C libraries and algorithms, and Cityhash was written in C so it was easy to translate to Go. However, these projects gave me a good feel of how Go works.&lt;/p&gt;

&lt;p&gt;In the end, I wrote the data generator in Go (private repo) and got the first taste of goroutines. Again, &lt;strong&gt;this second touch with Go was also relatively painless&lt;/strong&gt;. The only confusion I had at the time was the Go source tree structure. Trying to understand $GOROOT, $GOPATH and other Go environment variables were all new to me. This was also the first time in 10 years that I really spent time writing a piece of software, so I just considered the confusion as my inexperience.&lt;/p&gt;

&lt;h4 id=&#34;third-touch-and-beyond:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Third Touch and Beyond&lt;/h4&gt;

&lt;p&gt;Today, I no longer code at work as we have more developers now. Also, the Jolata product is mostly C/C++, Java and Node, so Go is also no longer in the mix. However, After getting a taste of Go in the first couple of Go projects, I&amp;rsquo;ve since spent a tremendous amount of my limited personal spare time working with it.&lt;/p&gt;

&lt;p&gt;I have since written various libraries for &lt;a href=&#34;https://github.com/dataence/bitmap&#34;&gt;bitmap compression&lt;/a&gt;, &lt;a href=&#34;https://github.com/dataence/encoding&#34;&gt;integer compression&lt;/a&gt;, &lt;a href=&#34;https://github.com/dataence/bloom&#34;&gt;bloom filters&lt;/a&gt;, &lt;a href=&#34;https://github.com/dataence/skiplist&#34;&gt;skiplist&lt;/a&gt;, and &lt;a href=&#34;https://github.com/dataence&#34;&gt;many others&lt;/a&gt;. And I have &lt;a href=&#34;http://zhen.org/blog/&#34;&gt;blogged my journey&lt;/a&gt; along the way as I learn. With these projects, I&amp;rsquo;ve learned how to use the Go toolchain, how to write idiomatic Go, how to write tests with Go, and more importantly, how to optimize Go.&lt;/p&gt;

&lt;p&gt;Interestingly, one of my most popular posts is &lt;a href=&#34;http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/&#34;&gt;Go vs Java: Decoding Billions of Integers Per Second&lt;/a&gt;. This tells me that a lot of Java developers are potentially looking to adopt Go.&lt;/p&gt;

&lt;p&gt;All these have allowed me to learn Go enough to build a real project, &lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt;. It is by far my most popular project and one that I expect to continue developing.&lt;/p&gt;

&lt;h2 id=&#34;my-views-on-go:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;My Views on Go&lt;/h2&gt;

&lt;p&gt;Go is not just a langauge, it also has a very active community around it. The views are based on my observation over the past 1.5 years of using Go. My Go environment is primary Sublime Text 3 with GoSublime plugin.&lt;/p&gt;

&lt;h3 id=&#34;as-a-language:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;As a Language&amp;hellip;&lt;/h3&gt;

&lt;p&gt;I am not a language theorist, nor do I claim to be a language expert. In fact, prior to actually using Go, I&amp;rsquo;ve barely heard of generics, communicating sequential processes, and other &amp;ldquo;cool&amp;rdquo; and &amp;ldquo;advanced&amp;rdquo; concepts. I&amp;rsquo;ve heard of all the new cool programming languages such as Clojure and Rust, but have never looked at any of the code. So my view of Go is basically one of a developer n00b.&lt;/p&gt;

&lt;p&gt;In a way, I consider that to be an advantage coming in to a new programming language, in that I have no preconceived notion of how things &amp;ldquo;SHOULD&amp;rdquo; be. I can learn the language and use the constructs as they were intended, and not have to question WHY it was designed that way because it&amp;rsquo;s different than what I know.&lt;/p&gt;

&lt;p&gt;Others may consider this to be a huge disadvantage, since I don&amp;rsquo;t know any better. There maybe constructs in other languages that would make my work a lot easier, or make my code a lot simpler.&lt;/p&gt;

&lt;p&gt;However, as long as the language doesn&amp;rsquo;t slow me down, then I feel it&amp;rsquo;s serving my needs.&lt;/p&gt;

&lt;h4 id=&#34;go-is-simple:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Go is Simple&lt;/h4&gt;

&lt;p&gt;As a language for a new deverloper, Go was very easy to pick up. Go&amp;rsquo;s design is fairly simple and minimalistic. You can sit down and read through the &lt;a href=&#34;https://golang.org/ref/spec&#34;&gt;Language Specification&lt;/a&gt; fairly quickly in an idle afternoon. I actually didn&amp;rsquo;t find the language reference until later. My first touch of Go was by scanning through the book &lt;em&gt;The Way To Go&lt;/em&gt;. Regardless, there&amp;rsquo;s not a lot to the language so it&amp;rsquo;s relatively easy for someone like myself to pick up the basics. (Btw, I&amp;rsquo;ve also never gone through the &lt;a href=&#34;https://tour.golang.org/&#34;&gt;Go Tour&lt;/a&gt;. I know it&amp;rsquo;s highly recommended to all new Go developers. I just never did it.)&lt;/p&gt;

&lt;p&gt;There are more advanced concepts in Go, such as interface, channel, and goroutine. Channel in general is a fairly straightforward concept. Most new programmers should be able to understand that quickly. You write stuff in, you read stuff out. It&amp;rsquo;s that simple. From there, you can slowly expand on the concept as you &lt;em&gt;go&lt;/em&gt; along by adding buffered channels, or ranging over channels, or checking if the read is ok, or using quit channels.&lt;/p&gt;

&lt;p&gt;For anyone coming from a language with threads, goroutine is not a difficult concept to understand. It&amp;rsquo;s basically a light-weight thread that can be executed concurrently. You can run any function as a goroutine.&lt;/p&gt;

&lt;p&gt;The more difficult concept is interface. That&amp;rsquo;s because it&amp;rsquo;s a &lt;strike&gt;fairly new concept that doesn&amp;rsquo;t really exist in&lt;/strike&gt; concept that&amp;rsquo;s fairly different than other languages. Once you understand what interfaces are, it&amp;rsquo;s fairly easy to start using them. However, designing your own interfaces is a different story.&lt;/p&gt;

&lt;p&gt;The one thing I&amp;rsquo;ve seen most developers complain about Go is the lack of generics. Egon made a nice &lt;a href=&#34;https://docs.google.com/document/d/1vrAy9gMpMoS3uaVphB32uVXX4pi-HnNjkMEgyAHX4N4&#34;&gt;Summary of Go Generics Discussions&lt;/a&gt; that you can read through. For me personally, I don&amp;rsquo;t know any better. I have never used generics and I haven&amp;rsquo;t found a situation where I strongly require it.&lt;/p&gt;

&lt;p&gt;As a language a team, the simplicity of Go is &lt;em&gt;HUGE&lt;/em&gt;. It allows develoeprs to quickly come up to speed and be productive in the shortest period of time. And in this case, time is literally money.&lt;/p&gt;

&lt;h4 id=&#34;go-is-opinionated:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Go is Opinionated&lt;/h4&gt;

&lt;p&gt;Go is opinionated in many ways. For example, probably one of the most frustrating thing about Go is how to structure the code directory. Unlike other languages where you can just create a directory and get started, Go wants you to put things in $GOPATH. It took a few readings of &lt;a href=&#34;https://golang.org/doc/code.html&#34;&gt;How to Write Go Code&lt;/a&gt; for me to grasp what&amp;rsquo;s going on, and it took even longer for me to really get the hang of code organization, and how Go imports packages (e.g., &lt;code&gt;go get&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;If I go back and look at my first internal project, I would probably cry because it&amp;rsquo;s all organized in a non-idiomatic way. However, once I got the hang of how Go expects things to be organized, it no longer was a obstacle for me. &lt;strong&gt;Instead of fighting the way things should be organized in Go, I learned to &lt;em&gt;go&lt;/em&gt; with the flow.&lt;/strong&gt; At the end of the day, the $GOPATH organizational structure actually helps me track the different packages I import.&lt;/p&gt;

&lt;p&gt;Another way Go is opinionated is code formatting. Go, and Go developers, expect that all Go programs are formatted with &lt;a href=&#34;http://blog.golang.org/go-fmt-your-code&#34;&gt;&lt;code&gt;go fmt&lt;/code&gt;&lt;/a&gt;. A lot of developers hate it and some even listed it as a top reason for leaving Go. However, this is one of those things that you just have to learn to &lt;em&gt;go&lt;/em&gt; with the flow. Personally I love it.&lt;/p&gt;

&lt;p&gt;And as a team language it will save a ton of argument time. Again, time is money for a new team. When my new VMware team got started, we probably spent a good 30 person-hours debating code formatting. That&amp;rsquo;s $2700 at a $180K fully-burdened rate. And that&amp;rsquo;s not counting all the issues we will run into later trying to reformat code that&amp;rsquo;s not properly formatted.&lt;/p&gt;

&lt;p&gt;Go is also very opininated in terms of variable use and package import. If a variable is declared but not used, the Go compiler will complain. If a package is imported but not used, the Go compiler will complain. Personally, I like the compiler complaining about the unused variables. It keeps the code clean, and reduce the chance of unexpected bugs. I am less concerned about unused packages but have also learned to live with the compiler complains. I use &lt;a href=&#34;https://github.com/bradfitz/goimports&#34;&gt;goimports&lt;/a&gt; in Sublime Text 3 to effectively and quickly take care of the import statements. In fact, in 99% of the cases I don&amp;rsquo;t even need to type in the import statements myself.&lt;/p&gt;

&lt;h4 id=&#34;go-is-safe:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Go is Safe&lt;/h4&gt;

&lt;p&gt;Go is safe for a couple of reasons. For a new developer, Go does not make it easy for you to be lazy. For example, Go is a statically typed language, which means every variable must explicitly have a type associated with it. The Go compiler does infer types under certain situations, but regardless, there&amp;rsquo;s a type for every variable. This may feel uncomfortable for developers coming from dynamic languages, but the benefit definitely outweighs the cost. I&amp;rsquo;ve experience first hand, as a product person waiting for bugs to be fixed, how long it takes to troubleshoot problems in Node. Having static types gives you a feeling of &amp;ldquo;correctness&amp;rdquo; after you have written the code.&lt;/p&gt;

&lt;p&gt;Another example of Go not allowing you to be lazy is that Go&amp;rsquo;s error handling is through the return of &lt;code&gt;error&lt;/code&gt; from functions. There has been a ton of discussions and debates on the merit of &lt;code&gt;error&lt;/code&gt; vs exception handling so I won&amp;rsquo;t go through it here. However, for a new programmer, it really requires your explicit attention to handle the errors. And I consider that to be a good thing as you know what to expect at each step of the program.&lt;/p&gt;

&lt;p&gt;Making things explicit and making it harder for developers to be lazy are a couple of the reasons that make Go safe.&lt;/p&gt;

&lt;p&gt;Another reason is that &lt;a href=&#34;http://golang.org/doc/faq#garbage_collection&#34;&gt;Go has a garbage collector&lt;/a&gt;. This makes it different from C/C++ as it no longer require developers to perform memory management. The difficulty in memory management is the single biggest source of memory leaks in C/C++ programs. Having a GC removes that burden from developers and makes the overall program much safer. Having said that, there&amp;rsquo;s much improvement to be made to the GC given its nascent state. And, as I learned over the past 1.5 years, to write high performance programs in Go today, developers need to make serious efforts to reduce GC pressure.&lt;/p&gt;

&lt;p&gt;Again, as a team langauge, the safety aspect is very important. The team will likely end up spending much less time dealing with memory bugs and focus more on feature development.&lt;/p&gt;

&lt;h4 id=&#34;go-is-powerful:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Go is Powerful&lt;/h4&gt;

&lt;p&gt;What makes Go powerful are its simplicity, its high performance, and advanced concepts such as channels, goroutines, interfaces, type composition, etc. We have discussed all of these in previous sections.&lt;/p&gt;

&lt;p&gt;In addition to all that, one of the killer feature of Go is that all Go programs are statically compiled into a single binary. There&amp;rsquo;s no shared libraries to worry about. There&amp;rsquo;s no jar files to worry about. There&amp;rsquo;s no packages to bundle. It&amp;rsquo;s just a single binary. And that&amp;rsquo;s an extremely powerful feature from the deployment and maintenance perspectives. To deploy a Go program, you just need to copy a single Go binary over. To update it, copy a single Go binary over.&lt;/p&gt;

&lt;p&gt;In contrast, to deploy a Node.js application, you may end up downloading hundreds of little packages at deployment time. And you have to worry about whether all these packages are compatible. The Node community has obviously developed a lot of good tools to manage dependencies and version control. But still, every time I see a Node app get deployed on a new machine, and have to download literally hundreds of little packages, I die a little inside.&lt;/p&gt;

&lt;p&gt;Also, if you deploy C/C++ programs and depend on shared libraries, now you have to worry about OS and shared library version compatibility issues.&lt;/p&gt;

&lt;p&gt;Another powerful feature of Go is that you can mix C and assembly code with Go code in a single program. I haven&amp;rsquo;t used this extensively, but in my attempt to &lt;a href=&#34;http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/&#34;&gt;optimize&lt;/a&gt; the &lt;a href=&#34;https://github.com/dataence/encoding&#34;&gt;integer compression&lt;/a&gt; library, I added different C and assembly snippets to try to squeeze the last ounce of performance out of Go. It was fairly easy and straightforward to do.&lt;/p&gt;

&lt;p&gt;One last thing, Go has a very large and complete standard library. It enables developers to do most, if not all, of their work quickly and efficiently. As the language matures and the community grows, there will be more and more 3rd party open source libraries one can leverage.&lt;/p&gt;

&lt;h3 id=&#34;as-a-community:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;As a Community&lt;/h3&gt;

&lt;p&gt;Today, Go has a very active community behind it. Specifically, the information sources I&amp;rsquo;ve followed and gotten help from include &lt;a href=&#34;https://botbot.me/freenode/go-nuts/&#34;&gt;#go-nuts IRC&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/search?q=golang&amp;amp;sort=new&amp;amp;restrict_sr=on&amp;amp;t=all&#34;&gt;golang subreddit&lt;/a&gt;, and obviously the &lt;a href=&#34;https://groups.google.com/forum/#!forum/golang-nuts&#34;&gt;golang-nuts mailing list&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I spent quite a bit of time in IRC when I first started. I&amp;rsquo;ve gotten help from quite a few people such as dsal, tv42, and others, and I am grateful for that. I am spending less time there now because of the limited time I have (remember, my day job is not development. :)&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s been some sentiments in the developer community that Go developers (gophers) are Google worshippers, don&amp;rsquo;t accept any feedbacks on language changes, harsh to new comers who come from different languages,  difficult to ask questions because the sample code is not on play.golang.org, etc etc.&lt;/p&gt;

&lt;p&gt;To be clear, I&amp;rsquo;ve never really spent much time with the different language communites, even when I was technical. So I have nothing else to compare to. So I can only speak from a human interaction level.&lt;/p&gt;

&lt;p&gt;I can see it from both perspectives. For example, developers coming from different language backgrounds sometimes have experience with a different way of doing things. When they want to perfrom the same tasks in Go, they ask the question by saying here&amp;rsquo;s how I solved this problem in language X, how do I translate that to Go?&lt;/p&gt;

&lt;p&gt;In some cases I&amp;rsquo;ve definitely seen people responding by saying that&amp;rsquo;s not how Go works and you are doing it wrong. That type of response can quickly create negative sentiment and kill the conversation.&lt;/p&gt;

&lt;p&gt;Another type of response I&amp;rsquo;ve seen is some developers telling the original poster (OP) that they are not asking questions the right way, and then promptly sending the OP a link to some web page on how to properly ask questions. Again, I can see how the OP can have a negative view on the matter.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve expereinced some of this myself. When I implemented a &lt;a href=&#34;https://github.com/dataence/bloom&#34;&gt;Bloom Filter&lt;/a&gt; package last year, I did a bunch of performance tests and wrote a &lt;a href=&#34;http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/&#34;&gt;blog post&lt;/a&gt; about the it. As a newbie learning Go, I felt like I accomplished something and I was pretty happy with it. I posted the link to reddit, and the first comment I got was&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Downvoted because I dislike this pattern of learning a new language and then immediately publishing performance data about it, before you know how to write idiomatic or performant code in it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Ouch!!&lt;/strong&gt; As a new Go developer, this is not the response I expected. In the end though, the commenter also pointed out something that helped me improve the performance of the implementation. I was grateful for that. It was also then I realized how important it is to reduce the number of allocation in order to reduce the Go GC pressure.&lt;/p&gt;

&lt;p&gt;In hindsight, the comment has a very valid point. I can understand why some developers would feel annoyed about benchmarks from people who have no idea on what they are doing. Regardless, being nice is not a bad thing. Saying things like &amp;ldquo;WTF is wrong with you&amp;rdquo; (not related to the bloom filter post) will only push new developers away.&lt;/p&gt;

&lt;p&gt;I quickly got over the sting because I am just too old to care about what others think I should or should not do. I continued my learning process by writing and optimizing Go packages, and posting the results in my blog. In fact, the &lt;a href=&#34;http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/&#34;&gt;Go vs Java: Decoding Billions of Integers Per Second&lt;/a&gt; post has many of the optimization techniques I tried to increase the performance of Go programs.&lt;/p&gt;

&lt;p&gt;Overall though, I felt I&amp;rsquo;ve learned a ton from the Go community. People have generally been helpful and are willing to offer solutions to problems. I have nothing to compare to, but I feel that the positives of the Go community far outweighs any negatives.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In conclusion, it has been a tremendous 1.5 years of working with Go, and seeing Go grow both as a language and as a community has been very rewarding.&lt;/p&gt;

&lt;p&gt;My focus now, in my limited spare personal time, is to continue the development of &lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt;, which is a high performance MQTT broker and client library that aims to be fully compliant with MQTT 3.1 and 3.1.1 specs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PingMQ: A SurgeMQ-based ICMP Monitoring Tool</title>
      <link>http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/</link>
      <pubDate>Thu, 25 Dec 2014 00:00:33 -0800</pubDate>
      
      <guid>http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/surge/surgemq/tree/master/cmd/pingmq&#34;&gt;pingmq&lt;/a&gt; is developed to demonstrate the different use cases one can use &lt;a href=&#34;//surgemq.com&#34;&gt;SurgeMQ&lt;/a&gt;, a high performance MQTT server and client library. In this simplified use case, a network administrator can setup server uptime monitoring system by periodically sending ICMP ECHO_REQUEST to all the IPs in their network, and send the results to SurgeMQ.&lt;/p&gt;

&lt;p&gt;Then multiple clients can subscribe to results based on their different needs. For example, a client maybe only interested in any failed ping attempts, as that would indicate a host might be down. After a certain number of failures the client may then raise some type of flag to indicate host down.&lt;/p&gt;

&lt;p&gt;There are three benefits of using SurgeMQ for this use case.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First, with all the different monitoring tools out there that wants to know if hosts are up or down, they can all now subscribe to a single source of information. They no longer need to write their own uptime tools.&lt;/li&gt;
&lt;li&gt;Second, assuming there are 5 monitoring tools on the network that wants to ping each and every host, the small packets are going to congest the network. The company can save 80% on their uptime monitoring bandwidth by having a single tool that pings the hosts, and have the rest subscribe to the results.&lt;/li&gt;
&lt;li&gt;Third/last, the company can enhance their security posture by placing tighter restrictions on their firewalls if there&amp;rsquo;s only a single host that can send ICMP ECHO_REQUESTS to all other hosts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following commands will run pingmq as a server, pinging the 8.8.8.0/28 CIDR block, and publishing the results to /ping/success/{ip} and /ping/failure/{ip} topics every 30 seconds. &lt;code&gt;sudo&lt;/code&gt; is needed because we are using RAW sockets and that requires root privilege.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build
$ sudo ./pingmq server -p 8.8.8.0/28 -i 30
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./pingmq client -t /ping/failure/+
8.8.8.6: Request timed out for seq 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./pingmq client -t /ping/success/+
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One can also subscribe to a specific IP by using the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./pingmq client -t /ping/+/8.8.8.8
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;commands:2953f7aa0fe0aa3757fe6bf025e7fa57&#34;&gt;Commands&lt;/h3&gt;

&lt;p&gt;There are two builtin commands for &lt;code&gt;pingmq&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;pingmq server&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage:
  pingmq server [flags]

 Available Flags:
  -h, --help=false: help for server
  -i, --interval=60: ping interval in seconds
  -p, --ping=[]: Comma separated list of IPv4 addresses to ping
  -q, --quiet=false: print out ping results
  -u, --uri=&amp;quot;tcp://:5836&amp;quot;: URI to run the server on
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;pingmq client&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage:
  pingmq client [flags]

 Available Flags:
  -h, --help=false: help for client
  -s, --server=&amp;quot;tcp://127.0.0.1:5836&amp;quot;: PingMQ server to connect to
  -t, --topic=[]: Comma separated list of topics to subscribe to
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ip-addresses:2953f7aa0fe0aa3757fe6bf025e7fa57&#34;&gt;IP Addresses&lt;/h3&gt;

&lt;p&gt;To list IPs you like to use with &lt;code&gt;pingmq&lt;/code&gt;, you can use the following formats:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;10.1.1.1      -&amp;gt; 10.1.1.1
10.1.1.1,2    -&amp;gt; 10.1.1.1, 10.1.1.2
10.1.1,2.1    -&amp;gt; 10.1.1.1, 10.1.2.1
10.1.1,2.1,2  -&amp;gt; 10.1.1.1, 10.1.1.2 10.1.2.1, 10.1.2.2
10.1.1.1-2    -&amp;gt; 10.1.1.1, 10.1.1.2
10.1.1.-2     -&amp;gt; 10.1.1.0, 10.1.1.1, 10.1.1.2
10.1.1.1-10   -&amp;gt; 10.1.1.1, 10.1.1.2 ... 10.1.1.10
10.1.1.1-     -&amp;gt; 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-3.1    -&amp;gt; 10.1.1.1, 10.1.2.1, 10.1.3.1
10.1-3.1-3.1  -&amp;gt; 10.1.1.1, 10.1.2.1, 10.1.3.1, 10.2.1.1, 10.2.2.1, 10.2.3.1, 10.3.1.1, 10.3.2.1, 10.3.3.1
10.1.1        -&amp;gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-2      -&amp;gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1-2        -&amp;gt; 10.1.0.0, 10.1.0,1 ... 10.2.255.254, 10..2.255.255
10            -&amp;gt; 10.0.0.0 ... 10.255.255.255
10.1.1.2,3,4  -&amp;gt; 10.1.1.1, 10.1.1.2, 10.1.1.3, 10.1.1.4
10.1.1,2      -&amp;gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1.1/28     -&amp;gt; 10.1.1.0 ... 10.1.1.255
10.1.1.0/28   -&amp;gt; 10.1.1.0 ... 10.1.1.15
10.1.1.0/30   -&amp;gt; 10.1.1.0, 10.1.1.1, 10.1.1.2, 10.1.1.3
10.1.1.128/25 -&amp;gt; 10.1.1.128 ... 10.1.1.255
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;topic-format:2953f7aa0fe0aa3757fe6bf025e7fa57&#34;&gt;Topic Format&lt;/h3&gt;

&lt;p&gt;TO subscribe to the &lt;code&gt;pingmq&lt;/code&gt; results, you can use the following formats:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/ping/#&lt;/code&gt; will subscribe to both success and failed pings for all IP addresses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping/success/+&lt;/code&gt; will subscribe to success pings for all IP addresses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping/failure/+&lt;/code&gt; will subscribe to failed pings for all IP addresses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping/+/8.8.8.8&lt;/code&gt; will subscribe to both success and failed pings for all IP 8.8.8.8&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;building:2953f7aa0fe0aa3757fe6bf025e7fa57&#34;&gt;Building&lt;/h3&gt;

&lt;p&gt;To build &lt;code&gt;pingmq&lt;/code&gt;, you need to have installed &lt;a href=&#34;http://golang.org&#34;&gt;Go 1.3.3 or 1.4&lt;/a&gt;. Then run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# go get github.com/surge/surgemq
# cd surgemq/examples/pingmq
# go build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, you should see the &lt;code&gt;pingmq&lt;/code&gt; command in the pingmq directory.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SurgeMQ: High Performance MQTT Server and Client Libraries in Go</title>
      <link>http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/</link>
      <pubDate>Wed, 24 Dec 2014 19:20:40 -0800</pubDate>
      
      <guid>http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Happy Holidays!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is more of an announcement post as SurgeMQ is now compatibility-tested with some of the popular MQTT clients out there, and it&amp;rsquo;s reaching &lt;em&gt;playable&lt;/em&gt; state.&lt;/p&gt;

&lt;p&gt;For completeness sake, please bear with some of the duplicate content in this post. The &lt;a href=&#34;//blog/surgemq-mqtt-message-queue-750k-mps/&#34;&gt;last post&lt;/a&gt; made front page of &lt;a href=&#34;https://news.ycombinator.com/item?id=8708921&#34;&gt;Hacker News&lt;/a&gt; and generated some great comments and discussions.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;SurgeMQ is a high performance MQTT broker and client library that aims to be fully compliant with MQTT 3.1 and 3.1.1 specs. The primary package that&amp;rsquo;s of interest is package &lt;a href=&#34;http://godoc.org/github.com/surge/surgemq/service&#34;&gt;service&lt;/a&gt;. It provides the MQTT Server and Client services in a library form.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SurgeMQ is currently under active development and should be considered unstable until further notice.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;mqtt:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;MQTT&lt;/h3&gt;

&lt;p&gt;According to the &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT spec&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.&lt;/p&gt;

&lt;p&gt;The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications.&lt;/li&gt;
&lt;li&gt;A messaging transport that is agnostic to the content of the payload.&lt;/li&gt;
&lt;li&gt;Three qualities of service for message delivery:

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;At most once&amp;rdquo;, where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;At least once&amp;rdquo;, where messages are assured to arrive but duplicates can occur.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Exactly once&amp;rdquo;, where message are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A small transport overhead and protocol exchanges minimized to reduce network traffic.&lt;/li&gt;
&lt;li&gt;A mechanism to notify interested parties when an abnormal disconnection occurs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;There&amp;rsquo;s some very large implementation of MQTT such as &lt;a href=&#34;https://www.facebook.com/notes/facebook-engineering/building-facebook-messenger/10150259350998920&#34;&gt;Facebook Messenger&lt;/a&gt;. There&amp;rsquo;s also an active Eclipse project, &lt;a href=&#34;https://eclipse.org/paho/&#34;&gt;Paho&lt;/a&gt;, that provides scalable open-source client implementations for many different languages, including C/C++, Java, Python, JavaScript, C# .Net and Go.&lt;/p&gt;

&lt;h3 id=&#34;features-limitations-and-future:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Features, Limitations, and Future&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Supports QOS 0, 1 and 2 messages&lt;/li&gt;
&lt;li&gt;Supports will messages&lt;/li&gt;
&lt;li&gt;Supports retained messages (add/remove)&lt;/li&gt;
&lt;li&gt;Pretty much everything in the spec except for the list below&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All features supported are in memory only. Once the server restarts everything is cleared.

&lt;ul&gt;
&lt;li&gt;However, all the components are written to be pluggable so one can write plugins based on the Go interfaces defined.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Message redelivery on reconnect is not currently supported.&lt;/li&gt;
&lt;li&gt;Message offline queueing on disconnect is not supported. Though this is also not a specific requirement for MQTT.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Future&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Message re-delivery (DUP)&lt;/li&gt;
&lt;li&gt;$SYS topics&lt;/li&gt;
&lt;li&gt;Server bridge&lt;/li&gt;
&lt;li&gt;Ack timeout/retry&lt;/li&gt;
&lt;li&gt;Session persistence&lt;/li&gt;
&lt;li&gt;Better authentication modules&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;performance:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;Current performance benchmark of SurgeMQ, running all publishers, subscribers and broker on a single 4-core (2.8Ghz i7) MacBook Pro, is able to achieve:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;over &lt;strong&gt;400,000 MPS&lt;/strong&gt; in a 1:1 single publisher and single producer configuration&lt;/li&gt;
&lt;li&gt;over &lt;strong&gt;450,000 MPS&lt;/strong&gt; in a 20:1 fan-in configuration&lt;/li&gt;
&lt;li&gt;over &lt;strong&gt;750,000 MPS&lt;/strong&gt; in a 1:20 fan-out configuration&lt;/li&gt;
&lt;li&gt;over &lt;strong&gt;700,000 MPS&lt;/strong&gt; in a full mesh configuration with 20 clients&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;compatibility:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Compatibility&lt;/h3&gt;

&lt;p&gt;In addition, SurgeMQ has been tested with the following client libraries and it &lt;em&gt;seems&lt;/em&gt; to work:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;libmosquitto 1.3.5 (in C).&lt;/em&gt; Tested with the bundled test programs msgsps_pub and msgsps_sub&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paho MQTT Conformance/Interoperability Testing Suite (in Python).&lt;/em&gt; Tested with all 10 test cases, 3 did not pass. They are

&lt;ol&gt;
&lt;li&gt;&amp;ldquo;offline messages queueing test&amp;rdquo; which is not supported by SurgeMQ&lt;/li&gt;
&lt;li&gt;&amp;ldquo;redelivery on reconnect test&amp;rdquo; which is not yet implemented by SurgeMQ&lt;/li&gt;
&lt;li&gt;&amp;ldquo;run subscribe failure test&amp;rdquo; which is not a valid test&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paho Go Client Library (in Go).&lt;/em&gt; Tested with one of the tests in the library, in fact, that tests is now part of the tests for SurgeMQ.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paho C Client library (in C).&lt;/em&gt; Tested with most of the test cases and failed the same ones as the conformance test because the features are not yet implemented. Actually I think there&amp;rsquo;s a bug in the test suite as it calls the PUBLISH handler function for non-PUBLISH messages.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;documentation:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Documentation&lt;/h3&gt;

&lt;p&gt;Documentation is available at &lt;a href=&#34;http://godoc.org/github.com/surge/surgemq&#34;&gt;godoc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;More information regarding the design of the SurgeMQ is available at &lt;a href=&#34;http://surgemq.com&#34;&gt;zen 3.1&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;license:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;License&lt;/h3&gt;

&lt;p&gt;Copyright &amp;copy; 2014 Dataence, LLC. All rights reserved.&lt;/p&gt;

&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &amp;ldquo;License&amp;rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;ldquo;AS IS&amp;rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.&lt;/p&gt;

&lt;h3 id=&#34;examples:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Examples&lt;/h3&gt;

&lt;h4 id=&#34;server-example:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Server Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;// Create a new server
svr := &amp;amp;service.Server{
    KeepAlive:        300,               // seconds
    ConnectTimeout:   2,                 // seconds
    SessionsProvider: &amp;quot;mem&amp;quot;,             // keeps sessions in memory
    Authenticator:    &amp;quot;mockSuccess&amp;quot;,     // always succeed
    TopicsProvider:   &amp;quot;mem&amp;quot;,             // keeps topic subscriptions in memory
}

// Listen and serve connections at localhost:1883
svr.ListenAndServe(&amp;quot;tcp://:1883&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;client-example:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Client Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;// Instantiates a new Client
c := &amp;amp;Client{}

// Creates a new MQTT CONNECT message and sets the proper parameters
msg := message.NewConnectMessage()
msg.SetWillQos(1)
msg.SetVersion(4)
msg.SetCleanSession(true)
msg.SetClientId([]byte(&amp;quot;surgemq&amp;quot;))
msg.SetKeepAlive(10)
msg.SetWillTopic([]byte(&amp;quot;will&amp;quot;))
msg.SetWillMessage([]byte(&amp;quot;send me home&amp;quot;))
msg.SetUsername([]byte(&amp;quot;surgemq&amp;quot;))
msg.SetPassword([]byte(&amp;quot;verysecret&amp;quot;))

// Connects to the remote server at 127.0.0.1 port 1883
c.Connect(&amp;quot;tcp://127.0.0.1:1883&amp;quot;, msg)

// Creates a new SUBSCRIBE message to subscribe to topic &amp;quot;abc&amp;quot;
submsg := message.NewSubscribeMessage()
submsg.AddTopic([]byte(&amp;quot;abc&amp;quot;), 0)

// Subscribes to the topic by sending the message. The first nil in the function
// call is a OnCompleteFunc that should handle the SUBACK message from the server.
// Nil means we are ignoring the SUBACK messages. The second nil should be a
// OnPublishFunc that handles any messages send to the client because of this
// subscription. Nil means we are ignoring any PUBLISH messages for this topic.
c.Subscribe(submsg, nil, nil)

// Creates a new PUBLISH message with the appropriate contents for publishing
pubmsg := message.NewPublishMessage()
pubmsg.SetPacketId(pktid)
pubmsg.SetTopic([]byte(&amp;quot;abc&amp;quot;))
pubmsg.SetPayload(make([]byte, 1024))
pubmsg.SetQoS(qos)

// Publishes to the server by sending the message
c.Publish(pubmsg, nil)

// Disconnects from the server
c.Disconnect()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>SurgeMQ: MQTT Message Queue @ 750,000 MPS</title>
      <link>http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/</link>
      <pubDate>Thu, 04 Dec 2014 22:44:07 -0800</pubDate>
      
      <guid>http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;Wow, this made front page of &lt;a href=&#34;https://news.ycombinator.com/item?id=8708921&#34;&gt;Hacker News&lt;/a&gt;! First for me!&lt;/li&gt;
&lt;li&gt;jacques_chester on HN has an &lt;a href=&#34;https://news.ycombinator.com/item?id=8709146&#34;&gt;EXCELLENT comment&lt;/a&gt; that&amp;rsquo;s definitely worth reading. &lt;a href=&#34;https://news.ycombinator.com/item?id=8709557&#34;&gt;My response&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;tl-dr:6ef28047216284b846a43eee6e7c23b5&#34;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt; aims to provide a MQTT broker and client library that&amp;rsquo;s fully compliant with &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT spec 3.1.1&lt;/a&gt;. In addition, it tries to be backward compatible with 3.1.&lt;/li&gt;
&lt;li&gt;SurgeMQ is under active development and should be considered unstable. Some of the key MQTT requirements, such as retained messages, still need to be added. The eventual goal is to pass the &lt;a href=&#34;https://eclipse.org/paho/clients/testing/&#34;&gt;MQTT Conformance/Interoperability Testing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Having said that, running all publishers, subscribers and broker on a single 4-core (2.8Ghz i7) MacBook Pro, SurgeMQ is able to achieve

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;over 400,000&lt;/strong&gt; MPS in a 1:1 single publisher and single producer configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;over 450,000&lt;/strong&gt; MPS in a 20:1 fan-in configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;over 750,000&lt;/strong&gt; MPS in a 1:20 fan-out configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;over 700,000&lt;/strong&gt; MPS in a full mesh configuration with 20 clients&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In developing SurgeMQ, I improved the performance 15-20X by keeping it simple and serial (KISS), reducing garbage collector pressure, reducing memory copy, and eliminating anything that could potentially introduce latency.&lt;/li&gt;
&lt;li&gt;There are still many areas that can be improved and I look forward to hearing any suggestions you may have.&lt;/li&gt;
&lt;li&gt;I cannot say this enough: &lt;strong&gt;benchmark, profile, optimize, rinse, repeat&lt;/strong&gt;. Go has made testing, benchmarking, and profiling extremely simple. You owe it to yourself to optimize your code using these tools.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 1: Don&amp;rsquo;t be clever. Keep It Simple and Serial (KISS).&lt;/p&gt;

&lt;p&gt;Lesson 2: Reduce or remove memory copying.&lt;/p&gt;

&lt;p&gt;Lesson 3: Race conditions can happen even if you think you followed all the right steps.&lt;/p&gt;

&lt;p&gt;Lesson 4: Use the race detector!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;go-learn-project-8-message-queue:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Go Learn Project #8 - Message Queue&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s now been over a year since my last post! Family and work have occupied pretty much all of my time so spare time to learn Go was hard to come by.&lt;/p&gt;

&lt;p&gt;However, I was able to squeeze in an implementation of a &lt;a href=&#34;https://github.com/surge/mqtt&#34;&gt;MQTT encoder/decoder&lt;/a&gt; library in July. The implementation is now outdated and is no longer maintained, but it allowed me to learn about the &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT protocol&lt;/a&gt; and got me thinking about potentially implmenting a broker.&lt;/p&gt;

&lt;p&gt;Now months later, I am finally able spend a few weekends and nights developing &lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt;, a (soon to be) full MQTT 3.1.1 compliant message broker.&lt;/p&gt;

&lt;h4 id=&#34;message-queues:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Message Queues&lt;/h4&gt;

&lt;p&gt;According to &lt;a href=&#34;http://en.wikipedia.org/wiki/Message_queue&#34;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Message queues provide an asynchronous communications protocol, meaning that the sender and receiver of the message do not need to interact with the message queue at the same time. Messages placed onto the queue are stored until the recipient retrieves them. Message queues have implicit or explicit limits on the size of data that may be transmitted in a single message and the number of messages that may remain outstanding on the queue.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tyler Treat of &lt;a href=&#34;http://www.bravenewgeek.com&#34;&gt;Brave New Geek&lt;/a&gt; also wrote a &lt;a href=&#34;http://www.bravenewgeek.com/tag/message-queues/&#34;&gt;good series on message queues&lt;/a&gt; that went over several of the key MQ implementations. One specific post, &lt;a href=&#34;http://www.bravenewgeek.com/dissecting-message-queues/&#34;&gt;Dissecting Message Queues&lt;/a&gt;, is especially interesting because it benchmarks some of the major message queue implmentations out there, both brokered and brokerless.&lt;/p&gt;

&lt;p&gt;In that post, Tyler found that borkerless queues had the highest throughput, achieving millions of MPS sent and received. Brokered message queue performances ranged from 12,000 MPS (&lt;a href=&#34;nsq.io&#34;&gt;NSQ&lt;/a&gt;) to 195,000 MPS (&lt;a href=&#34;nats.io&#34;&gt;Gnatsd&lt;/a&gt;). While the post showed that the Gnatsd latency to be around 300+ microseconds, in reality it&amp;rsquo;s probably more like the NSQ in terms of latency due to the sender sleeping whenever Gnatsd is 10+ messages behind. Regardless, hats off to Tyler. Great job!&lt;/p&gt;

&lt;h4 id=&#34;mqtt:6ef28047216284b846a43eee6e7c23b5&#34;&gt;MQTT&lt;/h4&gt;

&lt;p&gt;I got interested in MQTT because &amp;ldquo;&lt;a href=&#34;http://mqtt.org&#34;&gt;MQTT&lt;/a&gt; is a machine-to-machine (M2M)/&amp;ldquo;Internet of Things&amp;rdquo; connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;According to the &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT spec&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.&lt;/p&gt;

&lt;p&gt;The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications.&lt;/li&gt;
&lt;li&gt;A messaging transport that is agnostic to the content of the payload.&lt;/li&gt;
&lt;li&gt;Three qualities of service for message delivery:

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;At most once&amp;rdquo;, where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;At least once&amp;rdquo;, where messages are assured to arrive but duplicates can occur.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Exactly once&amp;rdquo;, where message are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A small transport overhead and protocol exchanges minimized to reduce network traffic.&lt;/li&gt;
&lt;li&gt;A mechanism to notify interested parties when an abnormal disconnection occurs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;There&amp;rsquo;s some very large implementation of MQTT such as &lt;a href=&#34;https://www.facebook.com/notes/facebook-engineering/building-facebook-messenger/10150259350998920&#34;&gt;Facebook Messenger&lt;/a&gt;. There&amp;rsquo;s also an active Eclipse project, &lt;a href=&#34;https://eclipse.org/paho/&#34;&gt;Paho&lt;/a&gt;, that provides scalable open-source client implementations for many different languages, including C/C++, Java, Python, JavaScript, C# .Net and Go.&lt;/p&gt;

&lt;p&gt;Given the popularity, I decided to implement a MQTT broker in order to learn about message queues.&lt;/p&gt;

&lt;h3 id=&#34;architecture:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/surgemq-mqtt-message-queue-750k-mps/smqfailedarch.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The above image showed a couple of the architecture approaches I attempted. In them, R is the receiver, which reads from net.Conn, P is the processor, which processes the messages and determines what to do or where to send them, and S is the sender, which sends any messages out to net.Conn. Each R, P, and S are their own goroutines.&lt;/p&gt;

&lt;p&gt;I started the project wanting to be clever, and wanted to dynamically scale up/down a shared pool of processors as the number of messages increase/decrease. As I thought through it, it just got more and more complicated with the logic and coordination. At the end, before I even wrote much of the code, I scraped the idea.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 1: Don&amp;rsquo;t be clever. Keep It Simple and Serial (KISS).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The second architecture approach I took is much simpler and probably much more idiomatic Go.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each connection has their own complete set of R, P and S, instead of sharing P across multiple connections.&lt;/li&gt;
&lt;li&gt;Each R, P and S are their own goroutines.&lt;/li&gt;
&lt;li&gt;Between R and P, and P and S are channels that carry MQTT messages.&lt;/li&gt;
&lt;li&gt;R was using bufio.Reader to read from net.Conn, and S was using bufio.Writer to write to net.Conn.&lt;/li&gt;
&lt;li&gt;sync.Pool was used to help reduce the amount of memory allocation required, thus reducing GC pressure&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach worked and I was able to write &lt;a href=&#34;https://github.com/surge/mqtt/commit/1eeba02bb5b7f624fc82a0ca975444944c1ec662&#34;&gt;enough code&lt;/a&gt; to test it. However, the performance was hideoous. In a 1:1 (single publisher and single subscriber) configuration, it was doing about 22,000-25,000 MPS.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go test -vv=3 -logtostderr -run=LotsOf -cpu=2 -v
=== RUN TestServiceLotsOfPublish0Messages-2
1000000 messages, 44297366818 ns, 44297.366818 ns/msg, 22574 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After profiling and looking at the &lt;a href=&#34;http://zhen.org/images/surgemq-mqtt-message-queue-750k-mps/2ndfailcpuprof.svg&#34;&gt;CPU profile&lt;/a&gt;, I realized there are a lot of memory copying (io.Copy and io.CopyN), as well as there are still quite a bit GC activities (scanblock). On the memory copying front, there&amp;rsquo;s copying from net.Conn into bufio, then more copying from bufio to the MQTT messages internal buffer, then more copying from MQTT message internal buffers to the outgoing bufio, then to the net.Conn. So lots and lots of memory copying, not a good thing.&lt;/p&gt;

&lt;h4 id=&#34;buffered-network-io:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Buffered Network IO&lt;/h4&gt;

&lt;p&gt;The buffered network IO is a good approach, however, there are two things I wished I had:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;bufio shifts bytes around by copying. For example, whenever it needs to fill the buffer, it copies all the remaining bytes to the front of the buffer, then fill the rest. That&amp;rsquo;s a lot of copying!&lt;/li&gt;
&lt;li&gt;I needed something I can access the bytes directly so I can remove majority of the memory copying.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At this point I decided to try a new technique I learned while doing Go Learn Project #7 - &lt;a href=&#34;http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/&#34;&gt;Ring Buffer&lt;/a&gt;. The basic idea is that instead of using bufio to read and write to net.Conn, I will implement my own version of that.&lt;/p&gt;

&lt;p&gt;The ring buffer will implement the interfaces ReadFrom(), WriteTo(), Read() and Write().&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The receiver will essentially copy data directly from net.Conn into the ring buffer (technially the ring buffer will ReadFrom() net.Conn and put the read bytes into the internal buffer).&lt;/li&gt;
&lt;li&gt;The processor can &amp;ldquo;peek&amp;rdquo; a byte slice (no copying) from the ring buffer, process it, and then commit the bytes once processing is done.&lt;/li&gt;
&lt;li&gt;If the message needs to be send to other subscribers, the bytes will then be copied into the subscriber&amp;rsquo;s outgoing buffer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While this is not &amp;ldquo;zero-copy&amp;rdquo;, it seems good enough.&lt;/p&gt;

&lt;p&gt;I started by implementing a lock-less ring buffer, and it worked quite well. But as mentioned in the ring buffer article, you really shouldn&amp;rsquo;t use it unless there&amp;rsquo;s plenty of CPU cores lying around. And also calling runtime.Gosched() thousands of times is really not healthy for the Go scheduler.&lt;/p&gt;

&lt;p&gt;So keeping Lesson 1 in mind, I modified the ring buffer to use two sync.Cond (reader sync.Cond and writer sync.Cond) to block (cond.Wait()) when there&amp;rsquo;s not enough bytes to read or when there&amp;rsquo;s not enough space to write. And then unblock (cond.Broadcast()) when bytes are either read from it, or written to it.&lt;/p&gt;

&lt;p&gt;This is a single producer/single consumer ring buffer and is not designed for multiples of anything. The original thought was that since each connection has their own set of R, P and S, there shouldn&amp;rsquo;t really be a need for multiple writers or readers. It turns out I was wrong, at least on the writer front. We will explain this a bit later.&lt;/p&gt;

&lt;p&gt;At the end, this turned out to be the winning combination. I was able to achieve 20X performance increase with this approach after some additional tweaking. Specifically, I tested several buffer block size (the amount of data to read from and write to net.Conn) including 1024, 2048, 4096 and 8192 bytes. The highest performing one is 8192 bytes.&lt;/p&gt;

&lt;p&gt;I also experiemented with different buffer sizes, including 256KB, 512KB and 1024KB. 256KB turned out to be sufficient in that it&amp;rsquo;s the smallest buffer size that doesn&amp;rsquo;t reduce performance by alot, nor higher numbers will help inprove performance.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 2: Reduce or remove memory copying.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;final-architecture:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Final Architecture&lt;/h4&gt;

&lt;p&gt;This the final architecture I ended up with and it&amp;rsquo;s working very well. The cost of each client connection are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3 goroutines: R (receiver), P (processor) and S (sender)&lt;/li&gt;
&lt;li&gt;2 ring buffers of 256K each&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There&amp;rsquo;s very few memory copy operations going on, nor is there much memory allocation. So a good outcome overall.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/surgemq-mqtt-message-queue-750k-mps/finalarch.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;race-conditions:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Race Conditions&lt;/h3&gt;

&lt;p&gt;With the ring bufer implementation, I was able to achieve 400,000 MPS with a 1:1 configuration. This worked well until I started doing multiple publishers and subscribers. The first problem I ran into was the Processor hanging. &lt;code&gt;go test -race&lt;/code&gt; also didn&amp;rsquo;t show anything that could help me.&lt;/p&gt;

&lt;p&gt;After running tests over and over again, with more and more glog.Debugf() statements, I tracked the problem to the Processor. It was waiting for space in the ring buffer to write the outgoing messages. I know that&amp;rsquo;s not possible as I am blasting messages out to net.Conn as fast as I can, so there&amp;rsquo;s no way that write space is not available.&lt;/p&gt;

&lt;p&gt;After running even more tests, and with even more glog.Debugf() statements, I finally determined the problem to be the way I was using sync.Cond. (I wish I saved the debug output..sigh) In the following code block, I was waiting for the consumer position (cpos) to pass the point in which there will be enough data for writing (wrap).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		this.pcond.L.Lock()
		for cpos = this.cseq.get(); wrap &amp;gt; cpos; cpos = this.cseq.get() {
			this.pcond.Wait()
		}
		this.pcond.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The steps are really quite simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I lock the producer sync.Conn&lt;/li&gt;
&lt;li&gt;I get the consumer position, compare it to wrap (position that I need cpos to pass to indicate there&amp;rsquo;s enough write space)&lt;/li&gt;
&lt;li&gt;If there&amp;rsquo;s not enough space, I wait, otherwise I move on&lt;/li&gt;
&lt;li&gt;I unlock the producer sync.Conn&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then in the Sender goroutine, I read data from the ring buffer, write to net.Conn, update the consumer position, and call &lt;code&gt;pcond.Broadcast()&lt;/code&gt; to unblock the above &lt;code&gt;pcond.Wait()&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		this.cseq.set(cpos + int64(n))
		this.pcond.Broadcast()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;According to the &lt;a href=&#34;http://golang.org/pkg/sync/#Cond.Broadcast&#34;&gt;Go doc&lt;/a&gt;,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Broadcast wakes all goroutines waiting on c.&lt;/p&gt;

&lt;p&gt;It is allowed but not required for the caller to hold c.L during the call.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So what I have above should work perfectly fine. Except it doesn&amp;rsquo;t. What happens is that I ran into a situation where &lt;code&gt;pcond.Broadcast()&lt;/code&gt; was called after the &lt;code&gt;wrap &amp;gt; cpos&lt;/code&gt; check, but before &lt;code&gt;pcond.Wait()&lt;/code&gt;. In these cases, the &lt;code&gt;wrap &amp;gt; cpos&lt;/code&gt; returned true, which means we need to go wait. But before &lt;code&gt;pcond.Wait()&lt;/code&gt; was called, the Sender goroutine has updated cpos, and called &lt;code&gt;pcond.Broadcast()&lt;/code&gt;. So when &lt;code&gt;pcond.Wait()&lt;/code&gt; is called, there&amp;rsquo;s nothing to wake it up, and thus it hangs forever.&lt;/p&gt;

&lt;p&gt;On the Sender side, because there&amp;rsquo;s no more data to read, it is also just waiting for more data. So both the Sender and Processor are now hung.&lt;/p&gt;

&lt;p&gt;After I finally figured out the root cause, I realized that, unlike what the go doc suggested, the caller should really hold c.L during the call to Broadcast(). So I modified the code to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		this.cseq.set(cpos + int64(n))
		this.pcond.L.Lock()
		this.pcond.Broadcast()
		this.pcond.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What this does is that it ensures I can never call &lt;code&gt;pcond.Broadcast()&lt;/code&gt; after &lt;code&gt;pcond.L.Lock()&lt;/code&gt; (in the Processor goroutine) is called but &lt;code&gt;pcond.Wait()&lt;/code&gt; is not called. When &lt;code&gt;pcond.Wait()&lt;/code&gt; is called, it actually calls &lt;code&gt;pcond.L.Unlock()&lt;/code&gt; internally so it will allow &lt;code&gt;pcond.L.Lock()&lt;/code&gt; in the Sender goroutine to be called.&lt;/p&gt;

&lt;p&gt;In any case, we are finally on our way to working with multiple clients.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 3: Race conditions can happen even if you think you followed all the right steps.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;but-wait-there-s-more-race-conditions:6ef28047216284b846a43eee6e7c23b5&#34;&gt;But Wait, There&amp;rsquo;s More (Race Conditions)&lt;/h4&gt;

&lt;p&gt;As I increase the number of publishers and subscribers, all the sudden I was getting errors about receiving RESERVED messages, and this happens intermittenly, and only when I blast enough messages. Sometimes I have to run the tests many times to catch this from happening.&lt;/p&gt;

&lt;p&gt;It turns out that while I was thinking I only had 1 Publisher per client connection that&amp;rsquo;s writing to the outgoing buffer, I, in fact, had many. This happens when a client is sent a message to a topic that it subscribes to. In this case, the Processor of the publishing client calls the subscriber client&amp;rsquo;s Publish() method, and writes the message to the outgoing ring buffer. At the same time, other publishing clients can be publishing other messages to the subscriber client. When this happens, they could overwrite eachother&amp;rsquo;s message because the ring buffer is NOT designed for multiple writers.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;go test -race&lt;/code&gt; should technically find this race condition (I think). But given that this condition only happens intermittenly and sometimes it only happens when there&amp;rsquo;s a large volume of messages, the race detector was taking too long and I was too impatient.&lt;/p&gt;

&lt;p&gt;Regardless, after identifying the root cause, I added a Mutex to serialize the writes. At some point I may come back and rewrite it without the lock. But for now it&amp;rsquo;s good enough.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 4: Use the race detector!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;performance-benchmarks:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Performance Benchmarks&lt;/h3&gt;

&lt;p&gt;These performance numbers are calculated as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sent messages MPS = total messages sent / total elapsed time between 1st and last message sent for all senders&lt;/li&gt;
&lt;li&gt;received messages MPS = total messages received / total elapsed time between 1st and last message received for all receivers&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;environment:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Environment&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ go version
go version go1.3.3 darwin/amd64

---

Macbook Pro Late 2013
2.8 GHz Intel Core i7
16 GB 1600 MHz DDR3
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;server:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Server&lt;/h4&gt;

&lt;p&gt;To start the server,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd benchmark
$ GOMAXPROCS=2 go test -run=TestServer -vv=2 -logtostderr
server/ListenAndServe: server is ready...
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;1-1:6ef28047216284b846a43eee6e7c23b5&#34;&gt;1:1&lt;/h4&gt;

&lt;p&gt;To run the single publisher and single subscriber test case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 1 -receivers 1
Total Sent 1000000 messages in 2434523153 ns, 2434 ns/msg, 410758 msgs/sec
Total Received 1000000 messages in 2434523153 ns, 2434 ns/msg, 410758 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;fan-in:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Fan-In&lt;/h4&gt;

&lt;p&gt;To run the Fan-In test with 20 senders and 1 receiver:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 20 -receivers 1
Total Sent 1035436 messages in 2212609304 ns, 2136 ns/msg, 467970 msgs/sec
Total Received 1000022 messages in 2212609304 ns, 2212 ns/msg, 451965 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;fan-out:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Fan-Out&lt;/h4&gt;

&lt;p&gt;To run the Fan-Out test with 1 sender and 20 receivers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 1 -receivers 20
Total Sent 1000000 messages in 10715317340 ns, 10715 ns/msg, 93324 msgs/sec
Total Received 8180723 messages in 10715317340 ns, 1309 ns/msg, 763460 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;mesh:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Mesh&lt;/h4&gt;

&lt;p&gt;To run a full mesh test where every client is subscribed to the same topic, thus every message sent w/ the right topic will go to ALL of the other clients:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestMesh -vv=2 -logtostderr -senders 20 -messages 100000
Total Sent 2000000 messages in 51385336097 ns, 25692 ns/msg, 38921 msgs/sec
Total Received 40000000 messages in 51420975243 ns, 1285 ns/msg, 777892 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;next-steps:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Next Steps&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s a lot more to do with SurgeMQ. Given the limited time I have, I expect it will take me a while to get to full compliant with the MQTT spec. But that will be my focus, now that performance is out of the way, as I get time.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>