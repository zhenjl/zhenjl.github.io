<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zen 3.1</title>
    <link>http://zhen.org/</link>
    <description>Recent content on Zen 3.1</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Jian Zhen. All Rights Reserved.</copyright>
    <lastBuildDate>Sun, 22 Feb 2015 19:57:56 -0800</lastBuildDate>
    <atom:link href="http://zhen.org/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Papers I Read: 2015 Week 8</title>
      <link>http://zhen.org/blog/papers-i-read-2015-week-8/</link>
      <pubDate>Sun, 22 Feb 2015 19:57:56 -0800</pubDate>
      
      <guid>http://zhen.org/blog/papers-i-read-2015-week-8/</guid>
      <description>

&lt;h3 id=&#34;random-ramblings:29724b4ec2d333ad12e89fd98257dccd&#34;&gt;Random Ramblings&lt;/h3&gt;

&lt;p&gt;Another week, another report of hacks. This time, &lt;a href=&#34;http://securelist.com/blog/research/68732/the-great-bank-robbery-the-carbanak-apt/&#34;&gt;The Great Bank Robbery&lt;/a&gt;, where up to 100 financial institutions have been hit.Total financial losses could be as a high as $1bn. You can download the &lt;a href=&#34;http://25zbkz3k00wn2tp5092n6di7b5k.wpengine.netdna-cdn.com/files/2015/02/Carbanak_APT_eng.pdf&#34;&gt;full report&lt;/a&gt; and learn all about it.&lt;/p&gt;

&lt;p&gt;Sony spent $15M to clean up and remediate their hack. I wonder how much these banks are going to spend on tracing the footsteps of their intruders and trying to figure out exactly where they have gone, what they have done and what they have taken.&lt;/p&gt;

&lt;p&gt;I didn&amp;rsquo;t make much progress this week on either &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; or &lt;a href=&#34;https://github.com/surgemq/surgemq&#34;&gt;surgemq&lt;/a&gt; because of busy work schedule and my son getting sick AGAIN!! But I did merge the few surgemq &lt;a href=&#34;https://github.com/surgemq/surgemq/pulls?q=is%3Apr+is%3Aclosed&#34;&gt;pull requests&lt;/a&gt; that the community has graciously contributed. One of them actually got it tested on Raspberry! That&amp;rsquo;s pretty cool.&lt;/p&gt;

&lt;p&gt;I also did manage to finish up the &lt;a href=&#34;https://github.com/strace/sequence/commit/713979f70d6025308e434205973249aa3138e58e&#34;&gt;experimental json scanner&lt;/a&gt; that I&amp;rsquo;ve been working on for the past couple of weeks. I will write more about it in the next &lt;a href=&#34;http://strace.io/sequence&#34;&gt;sequence&lt;/a&gt; article.&lt;/p&gt;

&lt;p&gt;Actually I am starting to feel a bit overwhelmed by having both projects. Both of them are very interesting and I can see both move forward in very positive ways. Lots of ideas in my head but not enough time to do them. Now that I am getting feature requests, issues and pull requests, I feel even worse because I haven&amp;rsquo;t spent enough time on them. &amp;lt;sigh&amp;gt;&lt;/p&gt;

&lt;h3 id=&#34;papers-i-read:29724b4ec2d333ad12e89fd98257dccd&#34;&gt;Papers I Read&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pdl.cmu.edu/PDL-FTP/HECStorage/git-cercs-12-08.pdf&#34;&gt;Memory-Efficient GroupBy-Aggregate using Compressed Buffer Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Memory is rapidly becoming a precious resource in many data processing environments. This paper introduces
a new data structure called a Compressed Buffer Tree (CBT). Using a combination of buffering, compression,
and lazy aggregation, CBTs can improve the memoryefficiency of the GroupBy-Aggregate abstraction which
forms the basis of many data processing models like MapReduce and databases. We evaluate CBTs in the
context of MapReduce aggregation, and show that CBTs can provide significant advantages over existing hashbased
aggregation techniques: up to 2× less memory and 1.5× the throughput, at the cost of 2.5× CPU.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.usenix.org/system/files/conference/atc14/atc14-paper-hu.pdf&#34;&gt;ELF: Efficient Lightweight Fast Stream Processing at Scale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Stream processing has become a key means for gaining rapid insights from webserver-captured data. Challenges
include how to scale to numerous, concurrently running streaming jobs, to coordinate across those jobs to share
insights, to make online changes to job functions to adapt to new requirements or data characteristics, and for each job, to efficiently operate over different time windows. The ELF stream processing system addresses these new challenges. Implemented over a set of agents enriching the web tier of datacenter systems, ELF obtains scalability by using a decentralized “many masters” architecture where for each job, live data is extracted directly from webservers, and placed into memory-efficient compressed buffer trees (CBTs) for local parsing and temporary storage, followed by subsequent aggregation using shared reducer trees (SRTs) mapped to sets of worker processes. Job masters at the roots of SRTs can dynamically customize worker actions, obtain aggregated results for end user delivery and/or coordinate with other jobs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html&#34;&gt;Is Parallel Programming Hard, And, If So, What Can You Do About It?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not just a paper, it&amp;rsquo;s a whole book w/ 800+ pages.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The purpose of this book is to help you program shared-memory parallel machines without risking your sanity.1 We hope that this book’s design principles will help you avoid at least some parallel-programming pitfalls. That said, you should think of this book as a foundation on which to build, rather than as a completed cathedral. Your mission, if you choose to accept, is to help make further progress in the exciting field of parallel programming—progress that will in time render this book obsolete. Parallel programming is not as hard as some say, and we hope that this book makes your parallel-programming projects easier and more fun.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Papers I Read: 2015 Week 7</title>
      <link>http://zhen.org/blog/papers-i-read-2015-week-7/</link>
      <pubDate>Sun, 15 Feb 2015 19:57:56 -0800</pubDate>
      
      <guid>http://zhen.org/blog/papers-i-read-2015-week-7/</guid>
      <description>

&lt;h2 id=&#34;random-ramblings:10248ddcf524aa32198bc5cded1aa098&#34;&gt;Random Ramblings&lt;/h2&gt;

&lt;p&gt;Well, another week, &lt;a href=&#34;http://www.nytimes.com/2015/02/05/business/hackers-breached-data-of-millions-insurer-says.html&#34;&gt;another big data breach&lt;/a&gt;. This time is Anthem, one of the nation’s largest health insurers. Ok, maybe it was last week that it happend. But this week they revealed that &lt;a href=&#34;http://consumerist.com/2015/02/13/anthem-says-data-from-as-far-back-as-2004-exposed-during-hack-offering-free-identity-theft-protection/&#34;&gt;hackers had access &amp;hellip; going back as far as 2004&lt;/a&gt;. WSJ blamed Anthem for &lt;a href=&#34;http://www.wsj.com/articles/investigators-eye-china-in-anthem-hack-1423167560&#34;&gt;not encrypting the data&lt;/a&gt;. Though I have to agree with Rich Mogull over at Securosis that &amp;ldquo;&lt;a href=&#34;https://securosis.com/blog/even-if-anthem-encrypted-it-probably-wouldnt-have-mattered&#34;&gt;even if Anthem had encrypted, it probably wouldn’t have helped&lt;/a&gt;&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;I feel bad for saying this but there&amp;rsquo;s one positive side effect from all these data breaches. Security is now officially a boardroom topic. Anthem&amp;rsquo;s CEO, Joseph Swedish, is now &lt;a href=&#34;http://www.latimes.com/business/la-fi-anthem-hack-ceo-20150213-story.html#page=1&#34;&gt;under the gun&lt;/a&gt; because top level executives are no longer immune to major security breaches that affect the company&amp;rsquo;s top line. Just ask &lt;a href=&#34;http://www.forbes.com/sites/ericbasu/2014/06/15/target-ceo-fired-can-you-be-fired-if-your-company-is-hacked/&#34;&gt;Target’s CEO Gregg Steinhafel&lt;/a&gt;, or &lt;a href=&#34;http://abcnews.go.com/Entertainment/wireStory/sony-chief-amy-pascal-acknowledges-fired-28918607&#34;&gt;Sony&amp;rsquo;s Co-Chairwoman Amy Pascal&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Brian Krebs wrote a &lt;a href=&#34;http://krebsonsecurity.com/2015/02/anthem-breach-may-have-started-in-april-2014/&#34;&gt;detailed piece&lt;/a&gt; analyzing the various pieces of information available relating to the Anthem hack. Quite an interesting read.&lt;/p&gt;

&lt;p&gt;One chart in the artile that Brian referred to is the time difference between the “time to compromise” and the “time to discovery&amp;rdquo;, taken from &lt;a href=&#34;http://www.verizonenterprise.com/DBIR/2014/&#34;&gt;Verizon’s 2014 Data Breach Investigations Report&lt;/a&gt;. As Brian summaries, &amp;ldquo;TL;DR: That gap is not improving, but instead is widening.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;What this really says is that, &lt;strong&gt;you will get hacked&lt;/strong&gt;. So how do you shorten the time between getting hacked, and finding out that you are hacked so you can quickly remediate the problem before worse things happen?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://krebsonsecurity.com/wp-content/uploads/2015/02/timetocompromise.png&#34; alt=&#34;The time difference between the “time to compromise” and the “time to discovery.”&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;With all these data breaches as backdrop, this week we also saw &amp;ldquo;President Barack Obama signed an executive order on Friday designed to spur businesses and the Federal Government to share with each other information related to cybersecurity, hacking and data breaches for the purpose of safeguarding U.S. infrastructure, economics and citizens from cyber attacks.&amp;rdquo; (&lt;a href=&#34;https://gigaom.com/2015/02/13/obamas-executive-order-calls-for-sharing-of-security-data/&#34;&gt;Gigaom&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;In general I don&amp;rsquo;t really think government mandates like this will work. The industry has to feel the pain enough that they are willing to participate, otherwise it&amp;rsquo;s just a waste of paper and ink. Facebook seems to be taking a lead in security information sharing and &lt;a href=&#34;https://www.facebook.com/notes/protect-the-graph/threatexchange-sharing-for-a-safer-internet/1566584370248375&#34;&gt;launched their ThreatExchange security framework&lt;/a&gt; this week. along with Pinterest, Tumblr, Twitter, and Yahoo. Good for them! I hope this is not a temporary PR thing, and that they keep funding and supporting the framework.&lt;/p&gt;

&lt;h2 id=&#34;papers-i-read:10248ddcf524aa32198bc5cded1aa098&#34;&gt;Papers I Read&lt;/h2&gt;

&lt;p&gt;Another great resource of computer science papers is Adrian Coyler&amp;rsquo;s &lt;a href=&#34;http://blog.acolyer.org/&#34;&gt;the morning paper&lt;/a&gt;. He selects and summarizes &amp;ldquo;an interesting/influential/important paper from the world of CS every weekday morning&amp;rdquo;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.put.poznan.pl/dweiss/site/publications/download/fsacomp.pdf&#34;&gt;Smaller Representation of Finite State Automata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I read this paper when I was trying to figure out how to make the FSAs smaller for the &lt;a href=&#34;https://github.com/surge/xparse/tree/master/etld&#34;&gt;Effective TLD matcher&lt;/a&gt; I created. The FSM I generated is 212,294 lines long. That&amp;rsquo;s just absolutely crazy. This paper seems to present an interesting way of compressing them.&lt;/p&gt;

&lt;p&gt;I am not exactly sure if &lt;a href=&#34;https://godoc.org/golang.org/x/net/publicsuffix&#34;&gt;PublicSuffix&lt;/a&gt; uses a similar representation but it basically represents a FSA as an array of bytes, and then walk the bytes like a binary search tree. It&amp;rsquo;s interesting for sure.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This paper is a follow-up to Jan Daciuk’s experiments on space-efficient finite state automata representation that can be used directly for traversals in main memory [4]. We investigate several techniques of reducing the memory footprint of minimal automata, mainly exploiting the fact that transition labels and transition pointer offset values are not evenly distributed and so are suitable for compression. We achieve a size gain of around 20–30% compared to the original representation given in [4]. This result is comparable to the state-of-the-art dictionary compression techniques like the LZ-trie [12] method, but remains
memory and CPU efficient during construction.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://arxiv.org/pdf/1409.5942v1.pdf&#34;&gt;IP Tracing and Active Network Response&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;This work presents integrated model for active security response model. The proposed model introduces Active Response Mechanism (ARM) for tracing anonymous attacks in the network back to their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or “spoofed”, source addresses. This paper presents within the proposed model two tracing approaches based on:
• Sleepy Watermark Tracing (SWT) for unauthorized access attacks.
• Probabilistic Packet Marking (PPM) in the network for Denial of Service
(DoS) and Distributed Denial of Service (DDoS) attacks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36356.pdf&#34;&gt;Dapper, a Large-Scale Distributed Systems Tracing Infrastructure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Here we introduce the design of Dapper, Google’s production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design
choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/&#34;&gt;STREAM PROCESSING, EVENT SOURCING, REACTIVE, CEP… AND MAKING SENSE OF IT ALL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not a paper, but a good write up nonetheless.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Some people call it stream processing. Others call it Event Sourcing or CQRS. Some even call it Complex Event Processing. Sometimes, such self-important buzzwords are just smoke and mirrors, invented by companies who want to sell you stuff. But sometimes, they contain a kernel of wisdom which can really help us design better systems. In this talk, we will go in search of the wisdom behind the buzzwords. We will discuss how event streams can help make your application more scalable, more reliable and more maintainable.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Sequence: Optimizing Go For the High Performance Log Scanner</title>
      <link>http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/</link>
      <pubDate>Fri, 13 Feb 2015 01:03:08 -0800</pubDate>
      
      <guid>http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/</guid>
      <description>

&lt;p&gt;Information here maybe outdated. Please visit &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;http://sequence.trustpath.com&lt;/a&gt; for latest.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This is part 3 of the &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;sequence&lt;/a&gt; series.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/&#34;&gt;Part 1&lt;/a&gt; is about the high performance parser that can parse 100,000-200,000 MPs.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/&#34;&gt;Part 2&lt;/a&gt; is about automating the process of reducing 100 of 1000&amp;rsquo;s of log messages down to dozens of unique patterns.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/&#34;&gt;Part 3&lt;/a&gt; is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size and core count) for scanning and tokenizing log messages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I would love to learn more about the state-of-the-art approaches that log vendors are using. These attempts are about scratching my own itch and trying to realize ideas I&amp;rsquo;ve had in my mind. Given some of these ideas are 5 to 10 years old, they may already be outdated. Personally I just haven&amp;rsquo;t heard of any groundbreaking approaches.&lt;/p&gt;

&lt;p&gt;In any case, if you know of some of the more innovative ways people are approaching these problems, please please please comment below as I would love to hear from you.&lt;/p&gt;

&lt;h3 id=&#34;tl-dr:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;sequence&lt;/code&gt; scanner is designed to tokenize free-form log messages.

&lt;ul&gt;
&lt;li&gt;It can scan between 200K to 500K log messages per second depending on message size and core count.&lt;/li&gt;
&lt;li&gt;It recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.&lt;/li&gt;
&lt;li&gt;The design is based mostly on finite-state machines.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The performance was achieved by the following techniques:

&lt;ol&gt;
&lt;li&gt;Go Through the String Once and Only Once&lt;/li&gt;
&lt;li&gt;Avoid Indexing into the String&lt;/li&gt;
&lt;li&gt;Reduce Heap Allocation&lt;/li&gt;
&lt;li&gt;Reduce Data Copying&lt;/li&gt;
&lt;li&gt;Mind the Data Struture&lt;/li&gt;
&lt;li&gt;Avoid Interfaces If Possible&lt;/li&gt;
&lt;li&gt;Find Ways to Short Circuit Checks&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;background:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Background&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;In computer science, lexical analysis is the process of converting a sequence of characters into a sequence of tokens, i.e. meaningful character strings. A program or function that performs lexical analysis is called a lexical analyzer, lexer, tokenizer, or scanner. - &lt;a href=&#34;http://en.wikipedia.org/wiki/Lexical_analysis&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of the most critical functions in the &lt;code&gt;sequence&lt;/code&gt; parser is the message tokenization. At a very high level, message tokenization means taking a single log message and breaking it into a list of tokens.&lt;/p&gt;

&lt;h3 id=&#34;functional-requirements:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Functional Requirements&lt;/h3&gt;

&lt;p&gt;The challenge is knowing where the token break points are. Most log messages are free-form text, which means there&amp;rsquo;s no common structure to them.&lt;/p&gt;

&lt;p&gt;As an example, the following log message can be tokenized into the sequence of tokens below. As you can see, one cannot depend on white spaces to tokenize, as the timestamp would be broken into 3 parts; nor can one use punctuations like &amp;ldquo;;&amp;rdquo; or &amp;ldquo;:&amp;ldquo;, as they would break the log mesage into useless parts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream

  #   0: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 14 10:15:56&amp;quot; }
  #   1: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;testserver&amp;quot; }
  #   2: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sudo&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   4: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;gonner&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;tty&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pts/3&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;pwd&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/home/gonner&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  14: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  15: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;root&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  18: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;command&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/bin/su&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;-&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;ustream&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So a log message &lt;em&gt;scanner&lt;/em&gt; or &lt;em&gt;tokenizer&lt;/em&gt; (we will use these terms interchangeably) must understand common components such as timestamp, URL, hex strings, IP addresses (v4 or v6), and mac addresses, so it can break the messages into &lt;em&gt;meaningful components&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;performance-requirements:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Performance Requirements&lt;/h3&gt;

&lt;p&gt;From a performance requirements perspective, I really didn&amp;rsquo;t start out with any expectations. However, after achieving 100-200K MPS for parsing (not just tokenizing), I have a strong desire to keep the performance at that level. So the more I can optimize the scanner to tokenize faster, the more head room I have for parsing.&lt;/p&gt;

&lt;p&gt;One may ask, who can POSSIBLY use such performance? Many organizations that I know are generating between 50-100M messages per second (MPS), that&amp;rsquo;s only 1,200 MPS. Some larger organizations I know are generating 60GB of Bluecoat logs per day, &lt;strong&gt;8 years ago&lt;/strong&gt;!! That&amp;rsquo;s a good 3,000 MPS assuming an average of 250 bytes per message. Even if log rate grows at 15%, that&amp;rsquo;s still only 10K MPS today.&lt;/p&gt;

&lt;p&gt;To run through an example, &lt;a href=&#34;http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf&#34;&gt;at EMC, 1.4 billion log messages are generated daily on average, at a rate of one terabyte a day&lt;/a&gt;. That&amp;rsquo;s 16,200 messages per second, and about 714 bytes per message. (Btw, what system can possibly generate messages that are 714 bytes long? That&amp;rsquo;s crazy and completely inefficient!) These EMC numbers are from 2013, so they have likely increased by now.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; parser, with a single CPU core, can process about 270,000 MPS for messages averaging 98 bytes. Assuming the performance is linear compare to the message size (which is pretty close to the truth), we can process 37,000 MPS for messages averaging 714 bytes. That&amp;rsquo;s just enough to parse the 16,2000 MPS, with a little head room to do other types of analysis or future growth.&lt;/p&gt;

&lt;p&gt;Obviously one can throw more hardware at solving the scale problem, but then again, why do that if you don&amp;rsquo;t need to. Just because you have the hardware doesn&amp;rsquo;t mean you should waste the money! Besides, there are much more interesting analytics problems your hardware can be used for than just tokenizing a message.&lt;/p&gt;

&lt;p&gt;In any case, I want to squeeze every oz of performance out of the scanner so I can have more time in the back to parse and analyze. So let&amp;rsquo;s set a goal of keeping at least 200,000 MPS for 100 bytes per message (BPM).&lt;/p&gt;

&lt;p&gt;Yes, go ahead and tell me I shouldn&amp;rsquo;t worry about micro-optimization, because this post is all about that. :)&lt;/p&gt;

&lt;h2 id=&#34;sequence-scanner:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Sequence Scanner&lt;/h2&gt;

&lt;p&gt;In the &lt;code&gt;sequence&lt;/code&gt; package, we implemented a general log message scanner, called &lt;a href=&#34;https://github.com/strace/sequence/blob/master/scanner.go&#34;&gt;GeneralScanner&lt;/a&gt;. GeneralScanner is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.&lt;/p&gt;

&lt;p&gt;This implementation was able to achieve both the functional and performance requirements. The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro, although the tests were only using 1 or 2 cores. The first file is a bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message. Last is a mix of ASA, sshd and sudo logs, averaging 136 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272869.35 msgs/sec

  $ ./sequence bench scan -i ../../data/allasa.log
  Scanned 234815 messages in 1.43 secs, ~ 163827.61 msgs/sec

  $ ./sequence bench scan -i ../../data/allasassh.log
  Scanned 447745 messages in 2.27 secs, ~ 197258.42 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/sshd.all -w 2
  Scanned 212897 messages in 0.43 secs, ~ 496961.52 msgs/sec

  $ GOMAXPROCS=2 ./sequenceo bench scan -i ../../data/allasa.log -w 2
  Scanned 234815 messages in 0.80 secs, ~ 292015.98 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/allasassh.log -w 2
  Scanned 447745 messages in 1.20 secs, ~ 373170.45 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;concepts:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Concepts&lt;/h3&gt;

&lt;p&gt;To understand the scanner, you have to understand the following concepts that are part of the package.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;Token&lt;/em&gt; is a piece of information extracted from the original log message. It is a struct that contains fields for &lt;em&gt;TokenType&lt;/em&gt;, &lt;em&gt;FieldType&lt;/em&gt;, and &lt;em&gt;Value&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;TokenType&lt;/em&gt; indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;FieldType&lt;/em&gt; indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Sequence&lt;/em&gt; is a list of Tokens. It is the key data structure consumed and returned by the &lt;em&gt;Scanner&lt;/em&gt;, &lt;em&gt;Analyzer&lt;/em&gt;, and the &lt;em&gt;Parser&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Basically, the scanner takes a log message string, tokenizes it and returns a &lt;em&gt;Sequence&lt;/em&gt; with the recognized &lt;em&gt;TokenType&lt;/em&gt; marked. This &lt;em&gt;Sequence&lt;/em&gt; is then fed into the analyzer or parser, and the analyzer or parser in turn returns another &lt;em&gt;Sequence&lt;/em&gt; that has the recognized &lt;em&gt;FieldType&lt;/em&gt; marked.&lt;/p&gt;

&lt;h3 id=&#34;design:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Design&lt;/h3&gt;

&lt;p&gt;Tokenizers or scanners are usually implemented using finite-state machines. Each FSM (or FSA, finite state automata) understands a specific sequences of characters that make up a type of token.&lt;/p&gt;

&lt;p&gt;In the &lt;code&gt;sequence&lt;/code&gt; scanner, there are three FSMs: Time, HexString and General.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The Time FSM understands a list of &lt;a href=&#34;https://github.com/strace/sequence/blob/master/time.go&#34;&gt;time formats&lt;/a&gt;. This list of time formats are commonly seen in log messages. It is also fairly easy to add to this list if needed.&lt;/li&gt;
&lt;li&gt;The HexString FSM is designed to understand IPv6 addresses (dead:beef:1234:5678:223:32ff:feb1:2e50 or f0f0:f::1), MAC addresses (00:04:c1:8b:d8:82), fingerprints or signatures (de:ad:be:ef:74:a6:bb:45:45:52:71:de:b2:12:34:56).&lt;/li&gt;
&lt;li&gt;The General FSM that recognizes URLs, IPv4 addresses, and any literal or strings.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each character in the log string are run through all three FSMs.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If a time format is matched, that&amp;rsquo;s what it will be returned.&lt;/li&gt;
&lt;li&gt;Next if a hex string is matched, it is also returned.

&lt;ul&gt;
&lt;li&gt;We mark anything with 5 colon characters and no successive colons like &amp;ldquo;::&amp;rdquo; to be a MAC address.&lt;/li&gt;
&lt;li&gt;Anything that has 7 colons and no successive colons are marked as IPv6 address.&lt;/li&gt;
&lt;li&gt;Anything that has less than 7 colons but has only 1 set of successive colons like &amp;ldquo;::&amp;rdquo; are marked as IPv6 address.&lt;/li&gt;
&lt;li&gt;Everything else is just a literal.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Finally if neither of the above matched, we return what the general FSM has matched.

&lt;ul&gt;
&lt;li&gt;The general FSM recognizes these quote characters: &amp;ldquo;, &amp;lsquo; and &amp;lt;. If these characters are encountered, then it will consider anything between the quotes to be a single token.&lt;/li&gt;
&lt;li&gt;Anything that starts with http:// or https:// are considered URLs.&lt;/li&gt;
&lt;li&gt;Anything that matches 4 integer octets are considered IP addresses.&lt;/li&gt;
&lt;li&gt;Anything that matches two integers with a dot in between are considered floats.&lt;/li&gt;
&lt;li&gt;Anything that matches just numbers are considered integers.&lt;/li&gt;
&lt;li&gt;Everything else are literals.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;performance:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;To achieve the performance requirements, the following rules and optimizations are followed. Some of these are Go specific, and some are general recommendations.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Go Through the String Once and Only Once&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is a hard requirement, otherwise we can&amp;rsquo;t call this project a &lt;em&gt;sequential&lt;/em&gt; parser. :)&lt;/p&gt;

&lt;p&gt;This is probably a pretty obvious technique. The more times you loop through loop through a string, the lower the performance. If you used regular expressions to parse logs, you will likely go through parts of the log message multiple times due to back tracking or look forward, etc.&lt;/p&gt;

&lt;p&gt;I took great pain to ensure that I don&amp;rsquo;t need to look forward or look backward in the log string to determine the current token type, and I think the effort paid off.&lt;/p&gt;

&lt;p&gt;In reality though, while I am only looping through the log string once, and only once, I do run each character through three different FSMs. However, it is still much less expensive than looping through three times, each time checking a single FSM. However, the more FSMs I run the characters through, the slower it gets.&lt;/p&gt;

&lt;p&gt;This was apparently when I &lt;a href=&#34;https://github.com/strace/sequence/commit/a5447814f43b4b9b7e804b14dde38e88fd53e6d0&#34;&gt;updated the scanner to support IPv6 and hex strings&lt;/a&gt;. I tried a couple of different approaches. First, I added an IPv6 specific FSM. So in addition to the original time, mac and general FSMs, there are now 4. That dropped performance by like 15%!!! That&amp;rsquo;s just unacceptable.&lt;/p&gt;

&lt;p&gt;The second approach, which is the one I checked in, combines the MAC, IPv6 and general hex strings into a single FSM. That helped somewhat. I was able to regain about 5% of the performance hit. However, because I can no longer short circuit the MAC address check (by string length and colon positions), I was still experiencing a 8-10% hit.&lt;/p&gt;

&lt;p&gt;What this means is that for most tokens, instead of checking just 2 FSMs because I can short circuit the MAC check pretty early, I have to now check all 3 FSMs.&lt;/p&gt;

&lt;p&gt;So the more FSMs, the more comlicated the FSMs, the more performance hits there will be.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Avoid Indexing into the String&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is really a Go-specific recommentation. Each time you index into a slice or string, Go will perform bounds checking for you, which means there&amp;rsquo;s extra operations it&amp;rsquo;s doing, and also means lower performance. As an example, here are results from two benchmark runs. The first is with bounds checking enabled, which is default Go behavior. The second disables bounds checking.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.79 secs, ~ 268673.91 msgs/sec

  $ go run -gcflags=-B ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 277479.58 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The performance difference is approximately 3.5%! However, while it&amp;rsquo;s fun to see the difference, I would never recommend disable bounds checking in production. So the next best thing is to remove as many operations that index into a string or slice as possible. Specifically:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use &amp;ldquo;range&amp;rdquo; in the loops, e.g. &lt;code&gt;for i, r := range str&lt;/code&gt; instead of &lt;code&gt;for i := 0; i &amp;lt; len(str); i++ { if str[i] == ... }&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you are checking a specific character in the string/slice multiple times, assign it to a variable and use the variable instead. This will avoid indexing into the slice/string multiple times.&lt;/li&gt;
&lt;li&gt;If there are multiple conditions in an &lt;code&gt;if&lt;/code&gt; statement, try to move (or add) the non-indexing checks to the front of the statement. This sometimes will help short circuit the checks and avoid the slice-indexing checks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;One might question if this is worth optimizing, but like I said, I am trying to squeeze every oz of performance so 3.5% is still good for me. Unfornately I do know I won&amp;rsquo;t get 3.5% since I can&amp;rsquo;t remove every operation that index into slice/string.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Reduce Heap Allocation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is true for all languages (where you can have some control of stack vs heap allocation), and it&amp;rsquo;s even more true in Go. Mainly in Go, if you allocate a new slice, Go will &amp;ldquo;zero&amp;rdquo; out the allocated memory. This is great from a safety perspective, but it does add to the overhead.&lt;/p&gt;

&lt;p&gt;As an example, in the scanner I originally allocated a new &lt;em&gt;Sequence&lt;/em&gt; (slice of &lt;em&gt;Token&lt;/em&gt;) for every new message. However, when i changed it to re-use the existing slice, the performance increased by over 10%!&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.87 secs, ~ 246027.12 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 275038.83 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The best thing to do is to run Go&amp;rsquo;s builtin CPU profiler, and look at the numbers for Go internal functions such as &lt;code&gt;runtime.makeslice&lt;/code&gt;, &lt;code&gt;runtime.markscan&lt;/code&gt;, and &lt;code&gt;runtime.memclr&lt;/code&gt;. Large percentages and numbers for these internal functions are dead giveaway that you are probably allocating too much stuff on the heap.&lt;/p&gt;

&lt;p&gt;I religiously go through the SVGs generated from the Go profiler to help me identify hot spots where I can optimize.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s also a couple of tips I picked up from the &lt;a href=&#34;https://groups.google.com/forum/#!topic/golang-nuts/baU4PZFyBQQ&#34;&gt;go-nuts mailing list&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Maps are bad&amp;ndash;even if they&amp;rsquo;re storing integers or other non-pointer structs. The implementation appears to have lots of pointers inside which must be evaluated and followed during mark/sweep GC.  Using structures with pointers magnifies the expense.&lt;/li&gt;
&lt;li&gt;Slices are surprisingly bad (including strings and substrings of existing strings). A slice is a pointer to the backing array with a length and capacity. It would appear that the internal pointer that causes the trouble because GC wants to inspect it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. Reduce Data Copying&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Data copying is expensive. It means the run time has to allocate new space and copy the data over. It&amp;rsquo;s even more expensive when you can&amp;rsquo;t have do &lt;code&gt;memcpy&lt;/code&gt; of a slice in Go like you can in C. Again, direct memory copying is not Go&amp;rsquo;s design goal. It is also much safer if you can prevent users from playing with memory directly too much. However, it is still a good idea to avoid any copying of data, whether it&amp;rsquo;s string or slice.&lt;/p&gt;

&lt;p&gt;As much as I can, I try to do in place processing of the data. Every &lt;em&gt;Sequence&lt;/em&gt; is worked on locally and I try not to copy &lt;em&gt;Sequence&lt;/em&gt; or string unless I absolutely have to.&lt;/p&gt;

&lt;p&gt;Unfortunately I don&amp;rsquo;t have any comparison numbers for this one, because I learned from &lt;a href=&#34;http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/&#34;&gt;previous projects&lt;/a&gt; that I should avoid copying as much as possible.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Mind the Data Struture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If there&amp;rsquo;s one thing I learned over the past year, is to use the right data structure for the right job. I&amp;rsquo;ve written about other data structures such as &lt;a href=&#34;http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/&#34;&gt;ring buffer&lt;/a&gt;, &lt;a href=&#34;http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/&#34;&gt;bloom filters&lt;/a&gt;, and &lt;a href=&#34;http://zhen.org/blog/go-skiplist/&#34;&gt;skiplist&lt;/a&gt; before.&lt;/p&gt;

&lt;p&gt;However, &lt;a href=&#34;http://en.wikipedia.org/wiki/Finite-state_machine&#34;&gt;finite-state automata or machine&lt;/a&gt; is my latest love and I&amp;rsquo;ve been using it at various projects such as my &lt;a href=&#34;http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/&#34;&gt;porter2&lt;/a&gt; and &lt;a href=&#34;https://github.com/surge/xparse/tree/master/etld&#34;&gt;effective TLD&lt;/a&gt;. Ok, technical FSM itself is not a data structure and can be implemented in different ways. In the &lt;code&gt;sequence&lt;/code&gt; project, I used both a tree representation as well as a bunch of switch-case statements. For the &lt;a href=&#34;http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/&#34;&gt;porter2&lt;/a&gt; FSMs, I used switch-case to implement them.&lt;/p&gt;

&lt;p&gt;Interestingly, swtich-case doesn&amp;rsquo;t always win. I tested the time FSM using both tree and switch-case implementations, and the tree actually won out. (Below, 1 is tree, 2 is switch-case.) So guess which one is checked in?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;BenchmarkTimeStep1   2000000         696 ns/op
BenchmarkTimeStep2   2000000         772 ns/op
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Writing this actually reminds me that in the parser, I am currently using a tree to parse the sequences. While parsing, there could be multiple paths that the sequence will match. Currently I walk all the matched paths fully, before choosing one that has the highest score. What I should do is to do a weighted walk, and always walk the highest score nodes first. If at the end I get a perfect score, I can just return that path and not have to walk the other paths. (Note to self, more parser optimization to do).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6. Avoid Interfaces If Possible&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is probably not a great advice to give to Go developers. Interface is probably one of the best Go features and everyone should learn to use it. However, if you want high performane, avoid interfaces as it provides additional layers of indirection. I don&amp;rsquo;t have performance numbers for the &lt;code&gt;sequence&lt;/code&gt; project since I tried to avoid interfaces in high performance areas from the start. However, previous in the &lt;a href=&#34;http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/&#34;&gt;ring buffer&lt;/a&gt; project, the version that uses interface is 140% slower than the version that didn&amp;rsquo;t.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t have the direct link but someone on the go-nuts mailing list also said:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If you really want high performance, I would suggest avoiding interfaces and, in general, function calls like the plague, since they are quite expensive in Go (compared to C). We have implemented basically the same for our internal web framework (to be released some day) and we&amp;rsquo;re almost 4x faster than encoding/json without doing too much optimization. I&amp;rsquo;m sure we could make this even faster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;7. Find Ways to Short Circuit Checks&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Find ways to quickly eliminate the need to run a section of the code has been tremendously helpful to improve performance. For example, here are a couple of place where I tried to do that.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&#34;https://github.com/strace/sequence/blob/master/scanner.go#L223&#34;&gt;this first example&lt;/a&gt;, I simply added &lt;code&gt;l == 1&lt;/code&gt; before the actual equality check of the string values. The first output is before the add, the second is after. The difference is about 2% performance increase.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272303.79 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 278433.34 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In &lt;a href=&#34;https://github.com/strace/sequence/blob/master/scanner.go#L282&#34;&gt;the second example&lt;/a&gt;, I added a quick check to make sure the remaining string is at least as long as the shortest time format. If there&amp;rsquo;s not enough characters, then don&amp;rsquo;t run the time FSM. The performance difference is about 2.5%.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272059.04 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 279388.47 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So by simply adding a couple of checks, I&amp;rsquo;ve increased perfromance by close to 5%.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:cb54f18a9944e1962d3fe8e3f09ea809&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At this point I think I have squeezed every bit of performance out of the scanner, to the extend of my knowledge. It&amp;rsquo;s performing relatively well and it&amp;rsquo;s given the parser plenty of head room to do other things. I hope some of these lessons are helpful to whatever you are doing.&lt;/p&gt;

&lt;p&gt;Feel free to take a look at the &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; project and try it out if you. If you have any issues/comments, please don&amp;rsquo;t hestiate to &lt;a href=&#34;https://github.com/strace/sequence/issues&#34;&gt;open a github issue&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns</title>
      <link>http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/</link>
      <pubDate>Tue, 10 Feb 2015 06:40:20 -0800</pubDate>
      
      <guid>http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/</guid>
      <description>

&lt;p&gt;Information here maybe outdated. Please visit &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;http://sequence.trustpath.com&lt;/a&gt; for latest.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This is part 2 of the &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;sequence&lt;/a&gt; series.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/&#34;&gt;Part 1&lt;/a&gt; is about the high performance parser that can parse 100,000-200,000 MPs.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/&#34;&gt;Part 2&lt;/a&gt; is about automating the process of reducing 100 of 1000&amp;rsquo;s of log messages down to dozens of unique patterns.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/&#34;&gt;Part 3&lt;/a&gt; is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size) for scanning and tokenizing log messages&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;background:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;This post really takes me down the memory lane. Back in 2005, while I was at LogLogic, we envisioned an automated approach to tagging, or labeling, log messages. More specifically, we wanted to automatically tag specific components within the log messages with their semantic label, such as a source IP address, or a target user.&lt;/p&gt;

&lt;p&gt;At the time, much like it is still today, the message parsing process is performed manually. This means someone has to manually look at the object and decided that the object should be labeled “user” or “targetUser.” An  analyst has to go through the log data, create a regular expression that extracts the useful strings out, and then finally assigning these to a specific label. This is extremely time consuming and error-prone.&lt;/p&gt;

&lt;p&gt;At that time, the vision was to provide an automated approach to universally parse and analyze ANY log data. The key phrase being “automated approach.” This means the users should only need to provide minimum guidance to the system, if any, for the platforms to be able to analyze the log data. LogLogic never did much with this, unfortunately.&lt;/p&gt;

&lt;p&gt;However, the tagging concept was later on adopted by (and I know how this got into CEE :) the &lt;a href=&#34;http://cee.mitre.org/&#34;&gt;Common Event Expression, or CEE&lt;/a&gt; effort by Mitre. This idea of tags also inspired &lt;a href=&#34;http://www.liblognorm.com/&#34;&gt;liblognorm&lt;/a&gt; to develop their &lt;a href=&#34;http://www.libee.org/&#34;&gt;libee&lt;/a&gt; library and &lt;a href=&#34;http://www.liblognorm.com/news/log-classification-with-liblognorm/&#34;&gt;tagging system&lt;/a&gt;. Rsyslog&amp;rsquo;s &lt;a href=&#34;http://www.rsyslog.com/doc/mmnormalize.html&#34;&gt;mmnormalize&lt;/a&gt; module is based on liblognorm.&lt;/p&gt;

&lt;p&gt;And then there&amp;rsquo;s Fedora&amp;rsquo;s &lt;a href=&#34;https://fedorahosted.org/lumberjack/&#34;&gt;Project Lumberjack&lt;/a&gt;, which &amp;ldquo;is an open-source project to update and enhance the event log architecture&amp;rdquo; and &amp;ldquo;aims to improve the creation and standardize the content of event logs by implementing the concepts and specifications proposed by the ​Common Event Expression (CEE).&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Then finally &lt;a href=&#34;http://logstash.net/&#34;&gt;logstash&lt;/a&gt; has their &lt;a href=&#34;http://logstash.net/docs/1.4.2/filters/grok&#34;&gt;grok filter&lt;/a&gt; that basically does similar extraction of unstructured data into a structured and queryable format. However, it seems like there might be some &lt;a href=&#34;http://ghost.frodux.in/logstash-grok-speeds/&#34;&gt;performance bottlenecks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, none of these efforts attempted to solve the automated tagging/labeling problem. They mostly just try to provide a parser for log messages.&lt;/p&gt;

&lt;p&gt;Also, it looks like many of these efforts have all been abandoned or put in hibernation, and haven&amp;rsquo;t been updated since 2012 or 2013. liblognrom did put out &lt;a href=&#34;http://www.liblognorm.com/news/&#34;&gt;a couple of updates&lt;/a&gt; in the past couple of years. Logstash&amp;rsquo;s grok obviously is being maintained and developed with the &lt;a href=&#34;http://www.elasticsearch.com/&#34;&gt;Elasticsearch&lt;/a&gt; backing.&lt;/p&gt;

&lt;p&gt;It is understandable, unfortunately. Log parsing is &lt;strong&gt;BORING&lt;/strong&gt;. I mean, who wants to sit there and stare at logs all day and try to come up with regular expressions or other types of parsing rules? LogLogic used to have a team of LogLabs analysts that did that, and I have to say I truly appreciated their effort and patience, because I cannot do that.&lt;/p&gt;

&lt;h3 id=&#34;the-end-result:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;The End Result&lt;/h3&gt;

&lt;p&gt;So instead of writing rules all day long, I decided to create an analyzer that can help us get at least 75% of the way there. The end result is the &lt;code&gt;Analyzer&lt;/code&gt;, written in &lt;a href=&#34;http://golang.org&#34;&gt;Go&lt;/a&gt;, in the &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; project I created. Here are some preliminary results. Below, we analyzed 2 files. The first is a file with over 200,000 sshd messages. The second is a file with a mixture of ASA, sshd, sudo and su log messages. It contains almost 450,000 messages.&lt;/p&gt;

&lt;p&gt;By running the analyzer over these logs, the pure sshd log file returned 45 individual patterns, and the second returned 103 unique patterns.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run sequence.go analyze -i ../../data/sshd.all -o sshd.analyze
Analyzed 212897 messages, found 45 unique patterns, 45 are new.

$ go run sequence.go analyze -i ../../data/asasshsudo.log -o asasshsudo.analyze
Analyzed 447745 messages, found 103 unique patterns, 103 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the output file has entries such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%msgtime% %apphost% %appname% [ %sessionid% ] : %status% %method% for %srcuser% from %srcipv4% port %srcport% ssh2
# Jan 15 19:39:26 irc sshd[7778]: Accepted password for jlz from 108.61.8.124 port 57630 ssh2

%msgtime% %appipv4% %appname% : %action% outbound %protocol% connection %sessionid% for %string% : %srcipv4% / %srcport% ( %ipv4% / %integer% ) to %string% : %dstipv4% / %dstport% ( %ipv4% / %integer% )
# 2012-04-05 18:46:18   172.23.0.1  %ASA-6-302013: Built outbound TCP connection 1424575 for outside:10.32.0.100/80 (10.32.0.100/80) to inside:172.23.73.72/2522 (10.32.0.1/54702)

%msgtime% %apphost% %appname% : %string% : tty = %string% ; pwd = %string% ; user = %srcuser% ; command = %command% - %string%
# Jan 15 14:09:11 irc sudo:    jlz : TTY=pts/1 ; PWD=/home/jlz ; USER=root ; COMMAND=/bin/su - irc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, the output is not 100%, but it gets us pretty close. Once the analyst goes through and updates the rules, he/she can re-run the analyzer anytime with any file to determine if there&amp;rsquo;s new patterns. For example, below, we ran the sshd log file with an existing pattern file, and got 4 new log patterns.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go run sequence.go analyze -i ../../data/sshd.all -p ../../patterns/sshd.txt -o sshd.analyze
Analyzed 212897 messages, found 39 unique patterns, 4 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;parser-quick-review:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Parser - Quick Review&lt;/h3&gt;

&lt;p&gt;I wrote about the &lt;a href=&#34;http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/&#34;&gt;sequence parser&lt;/a&gt; a couple of weeks back. It is a &lt;em&gt;high performance sequential log parser&lt;/em&gt;. It &lt;em&gt;sequentially&lt;/em&gt; goes through a log message, &lt;em&gt;parses&lt;/em&gt; out the meaningful parts, without the use regular expressions. It can achieve &lt;em&gt;high performance&lt;/em&gt; parsing of &lt;strong&gt;100,000 - 200,000 messages per second (MPS)&lt;/strong&gt; without the need to separate parsing rules by log source type. Underneath the hood, the &lt;code&gt;sequence&lt;/code&gt; parser basically constructs a tree based on the sequential rules, walks the tree to identify all the possible paths, and returns the path that has the best match (highest weight) for the message.&lt;/p&gt;

&lt;p&gt;While the analyzer is about reducing a large corupus of raw log messages down to a small set of unique patterns, the parser is all about matching log messages to an existing set of patters and determining whether a specific pattern has matched. Based on the pattern, it returns a sequence of tokens that basically extracts out the important pieces of information from the logs. The analysts can then take this sequence and perform other types of analysis.&lt;/p&gt;

&lt;p&gt;The approach taken by the &lt;code&gt;sequence&lt;/code&gt; parser is pretty much the same as liblognorm or other tree-based approaches.&lt;/p&gt;

&lt;h2 id=&#34;sequence-analyzer:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Sequence Analyzer&lt;/h2&gt;

&lt;p&gt;In the following section I will go through additional details of how the &lt;code&gt;sequence&lt;/code&gt; analyzer reduces 100 of 1000&amp;rsquo;s of raw log messages down to just 10&amp;rsquo;s of unique patterns, and then determining how to label the individual tokens.&lt;/p&gt;

&lt;h3 id=&#34;identifying-unique-patterns:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Identifying Unique Patterns&lt;/h3&gt;

&lt;p&gt;Analyzer builds an analysis tree that represents all the Sequences from messages. It can be used to determine all of the unique patterns for a large body of messages.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s based on a single basic concept, that for multiple log messages, if tokens in the same position shares one same parent and one same child, then the tokens in that position is likely variable string, which means it&amp;rsquo;s something we can extract. For example, take a look at the following two messages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jan 12 06:49:42 irc sshd[7034]: Accepted password for root from 218.161.81.238 port 4228 ssh2
Jan 12 14:44:48 jlz sshd[11084]: Accepted publickey for jlz from 76.21.0.16 port 36609 ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first token of each message is a timestamp, and the 3rd token of each message is the literal &amp;ldquo;sshd&amp;rdquo;. For the literals &amp;ldquo;irc&amp;rdquo; and &amp;ldquo;jlz&amp;rdquo;, they both share a common parent, which is a timestamp. They also both share a common child, which is &amp;ldquo;sshd&amp;rdquo;. This means token in between these, the 2nd token in each message, likely represents a variable token in this message type. In this case, &amp;ldquo;irc&amp;rdquo; and &amp;ldquo;jlz&amp;rdquo; happens to
represent the syslog host.&lt;/p&gt;

&lt;p&gt;Looking further down the message, the literals &amp;ldquo;password&amp;rdquo; and &amp;ldquo;publickey&amp;rdquo; also share a common parent, &amp;ldquo;Accepted&amp;rdquo;, and a common child, &amp;ldquo;for&amp;rdquo;. So that means the token in this position is also a variable token (of type TokenString).&lt;/p&gt;

&lt;p&gt;You can find several tokens that share common parent and child in these two messages, which means each of these tokens can be extracted. And finally, we can determine that the single pattern that will match both is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%time% %string% sshd [ %integer% ] : Accepted %string% for %string% from %ipv4% port %integer% ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If later we add another message to this mix:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Jan 12 06:49:42 irc sshd[7034]: Failed password for root from 218.161.81.238 port 4228 ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Analyzer will determine that the literals &amp;ldquo;Accepted&amp;rdquo; in the 1st message, and &amp;ldquo;Failed&amp;rdquo; in the 3rd message share a common parent &amp;ldquo;:&amp;rdquo; and a common child &amp;ldquo;password&amp;rdquo;, so it will determine that the token in this position is also a variable token. After all three messages are analyzed, the final pattern that will match all three
messages is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;%time% %string% sshd [ %integer% ] : %string% %string% for %string% from %ipv4% port %integer% ssh2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By applying this concept, we can effectively identify all the unique patterns in a log file.&lt;/p&gt;

&lt;h3 id=&#34;determining-the-correct-labels:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Determining the Correct Labels&lt;/h3&gt;

&lt;p&gt;Now that we have the unique patterns, we will scan the tokens to determine which labels we should apply to them.&lt;/p&gt;

&lt;p&gt;System and network logs are mostly free form text. There&amp;rsquo;s no specific patterns to any of them. So it&amp;rsquo;s really difficult to determine how to label specific parts of the log message automatically. However, over the years, after looking at so many system and network log messages, some patterns will start to emerge.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s no &amp;ldquo;machine learning&amp;rdquo; here. This section is all about codifying these human learnings. I&amp;rsquo;ve created the following 6 rules to help label tokens in the log messages. By no means are these rules perfect. They are at best just guesses on how to label. But hopefully they can get us 75% of the way there and we human can just take it the rest of the way.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;0. Parsing Email and Hostname Formats&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is technically not a labeling step. Before we actually start the labeling process, we wanted to first parse out a couple more formats like email and host names. The message tokenizer doesn&amp;rsquo;t recognize these because they are difficult to parse and will slow down the tokenizer. These specific formats are also not needed by the parser. So because the analyzer doesn&amp;rsquo;t care about performance as much, we can do this as post-processing step.&lt;/p&gt;

&lt;p&gt;To recognize the hostname, we try to match the &amp;ldquo;effective TLD&amp;rdquo; using the &lt;a href=&#34;https://github.com/surge/xparse/tree/master/etld&#34;&gt;xparse/etld&lt;/a&gt; package. It is an effective TLD matcher that returns the length of the effective domain name for the given string. It uses the data set from &lt;a href=&#34;https://www.publicsuffix.org/list/effective_tld_names.dat&#34;&gt;https://www.publicsuffix.org/list/effective_tld_names.dat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Recognizing Syslog Headers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First we will try to see if we can regonize the syslog headers. We try to recogize both RFC5424 and RFC3164 syslog headers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;	// RFC5424
	// - &amp;quot;1 2003-10-11T22:14:15.003Z mymachine.example.com evntslog - ID47 ...&amp;quot;
	// - &amp;quot;1 2003-08-24T05:14:15.000003-07:00 192.0.2.1 myproc 8710 - ...&amp;quot;
	// - &amp;quot;1 2003-10-11T22:14:15.003Z mymachine.example.com su - ID47 ...&amp;quot;
	// RFC3164
	// - &amp;quot;Oct 11 22:14:15 mymachine su: ...&amp;quot;
	// - &amp;quot;Aug 24 05:34:00 CST 1987 mymachine myproc[10]: ...&amp;quot;
	// - &amp;quot;jan 12 06:49:56 irc last message repeated 6 times&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the sequence pattern matches any of the above sequence, then we assume the first few tokens belong to the syslog header.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Marking Key and Value Pairs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The next step we perform is to mark known &amp;ldquo;keys&amp;rdquo;. There are two types of keys. First, we identify any token before the &amp;ldquo;=&amp;rdquo; as a key. For example, the message &lt;code&gt;fw=TOPSEC priv=6 recorder=kernel type=conn&lt;/code&gt; contains 4 keys: &lt;code&gt;fw&lt;/code&gt;, &lt;code&gt;priv&lt;/code&gt;, &lt;code&gt;recorder&lt;/code&gt; and &lt;code&gt;type&lt;/code&gt;. These keys should be considered string literals, and should not be extracted. However, they can be used to determine how the value part should be labeled.&lt;/p&gt;

&lt;p&gt;The second types of keys are determined by keywords that often appear in front of other tokens, I call these &lt;strong&gt;prekeys&lt;/strong&gt;. For example, we know that the prekey &lt;code&gt;from&lt;/code&gt; usually appears in front of any source host or IP address, and the prekey &lt;code&gt;to&lt;/code&gt; usually appears in front of any destination host or IP address. Below are some examples of these prekeys.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from 		= [ &amp;quot;%srchost%&amp;quot;, &amp;quot;%srcipv4%&amp;quot; ]
port 		= [ &amp;quot;%srcport%&amp;quot;, &amp;quot;%dstport%&amp;quot; ]
proto		= [ &amp;quot;%protocol%&amp;quot; ]
sport		= [ &amp;quot;%srcport%&amp;quot; ]
src 		= [ &amp;quot;%srchost%&amp;quot;, &amp;quot;%srcipv4%&amp;quot; ]
to 			= [ &amp;quot;%dsthost%&amp;quot;, &amp;quot;%dstipv4%&amp;quot;, &amp;quot;%dstuser%&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To help identify these prekeys, I wrote a quick program that goes through many of the logs I have to help identify what keywords appears before IP address, mac addresses, and other non-literal tokens. The result is put into the &lt;a href=&#34;https://github.com/strace/sequence/blob/master/keymaps.go&#34;&gt;keymaps.go&lt;/a&gt; file. It&amp;rsquo;s not comprehensive, but it&amp;rsquo;s also not meant to be. We just need enough hints to help with labeling.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Labeling &amp;ldquo;Values&amp;rdquo; by Their Keys&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once the keys are labeled, we can label the values based on the mapping described above. For key/value pairs, we try to recognize both &lt;code&gt;key=value&lt;/code&gt; or &lt;code&gt;key=&amp;quot;value&amp;quot;&lt;/code&gt; formats (or other quote characters like &amp;lsquo; or &amp;lt;).&lt;/p&gt;

&lt;p&gt;For the prekeys, we try to find the value token within 2 tokens of the key token. That means sequences such as &lt;code&gt;from 192.168.1.1&lt;/code&gt; and &lt;code&gt;from ip 192.168.1.1&lt;/code&gt; will identify &lt;code&gt;192.168.1.1&lt;/code&gt; as the &lt;code&gt;%srcipv4%&lt;/code&gt; based on the above mapping, but we will miss &lt;code&gt;from ip address 192.168.1.1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. Identifying Known Keywords&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Within most log messages, there are certain keywords that would indicate what actions were performed, what the state/status of the action was, and what objects the actions were performed on. CEE had a list that it identified, so I copied the list and added some of my own.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;action = [
	&amp;quot;access&amp;quot;,
	&amp;quot;alert&amp;quot;,
	&amp;quot;allocate&amp;quot;,
	&amp;quot;allow&amp;quot;,
	.
	.
	.
]

status = [
	&amp;quot;accept&amp;quot;,
	&amp;quot;error&amp;quot;,
	&amp;quot;fail&amp;quot;,
	&amp;quot;failure&amp;quot;,
	&amp;quot;success&amp;quot;
]

object = [
	&amp;quot;account&amp;quot;,
	&amp;quot;app&amp;quot;,
	&amp;quot;bios&amp;quot;,
	&amp;quot;driver&amp;quot;,
	.
	.
	.
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In our labeling process, we basically goes through and identify all the string literals that are NOT marked as keys, and perform a &lt;a href=&#34;https://github.com/surge/porter2&#34;&gt;porter2 stemming operation&lt;/a&gt; on the literal, then compare to the above list (which is also porter2 stemmed).&lt;/p&gt;

&lt;p&gt;If a literal matches one of the above lists, then the corresponding label (&lt;code&gt;action&lt;/code&gt;, &lt;code&gt;status&lt;/code&gt;, &lt;code&gt;object&lt;/code&gt;, &lt;code&gt;srcuser&lt;/code&gt;, &lt;code&gt;method&lt;/code&gt;, or &lt;code&gt;protocol&lt;/code&gt;) is applied.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. Determining Positions of Specific Types&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this next step, we are basically looking at the position of where some of the token types appear. Specifically, we are looking for &lt;code&gt;%time%&lt;/code&gt;, &lt;code&gt;%url%&lt;/code&gt;, &lt;code&gt;%mac%&lt;/code&gt;, &lt;code&gt;%ipv4%&lt;/code&gt;, &lt;code&gt;%host%&lt;/code&gt;, and &lt;code&gt;%email%&lt;/code&gt; tokens. Assuming the labels have not already been taken with the previous rules, the rules are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The first %time% token is labeled as %msgtime%&lt;/li&gt;
&lt;li&gt;The first %url% token is labeled as %object%&lt;/li&gt;
&lt;li&gt;The first %mac% token is labeled as %srcmac% and the second is labeld as %dstmac%&lt;/li&gt;
&lt;li&gt;The first %ipv4% token is labeled as %srcipv4% and the second is labeld as %dstipv4%&lt;/li&gt;
&lt;li&gt;The first %host% token is labeled as %srchost% and the second is labeld as %dsthost%&lt;/li&gt;
&lt;li&gt;The first %email% token is labeled as %srcemail% and the second is labeld as %dstemail%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;6. Scanning for ip/port or ip:port Pairs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Finally, after all that, we scan through the sequence again, and identify any numbers that follow an IP address, but separated by either a &amp;ldquo;/&amp;rdquo; or &amp;ldquo;:&amp;ldquo;. Then we label these numbers as either &lt;code&gt;%srcport%&lt;/code&gt; or &lt;code&gt;%dstport%&lt;/code&gt; based on how the previous IP address is labeled.&lt;/p&gt;

&lt;h3 id=&#34;summary:61b1adc09fb9bc31a28d4faabfef3631&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;There are some limitations to the &lt;code&gt;sequence&lt;/code&gt; parser and analyzer. For example, currently &lt;code&gt;sequence&lt;/code&gt; does not handle multi-line logs. Each log message must appear as a single line. So if there&amp;rsquo;s multi-line logs, they must be first be converted into a single line. Also, &lt;code&gt;sequence&lt;/code&gt; has been only tested with a limited set of system (Linux, AIX, sudo, ssh, su, dhcp, etc etc), network (ASA, PIX, Neoteris, CheckPoint, Juniper Firewall) and infrastructure application (apache, bluecoat, etc) logs.&lt;/p&gt;

&lt;p&gt;Documentation is available at godoc: &lt;a href=&#34;http://godoc.org/github.com/strace/sequence&#34;&gt;package&lt;/a&gt;, &lt;a href=&#34;http://godoc.org/github.com/strace/sequence/cmd/sequence&#34;&gt;command&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are some pattern files developed for ASA, Sudo and SSH in the &lt;code&gt;patterns&lt;/code&gt; directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages.&lt;/p&gt;

&lt;p&gt;If you have a set of logs you would like me to test out, please feel free to &lt;a href=&#34;https://github.com/strace/sequence/issues&#34;&gt;open an issue&lt;/a&gt; and we can arrange a way for me to download and test your logs.&lt;/p&gt;

&lt;p&gt;Stay tuned for more log patterns&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Papers I Read: 2015 Week 6</title>
      <link>http://zhen.org/blog/papers-i-read-2015-week-6/</link>
      <pubDate>Sun, 08 Feb 2015 19:57:56 -0800</pubDate>
      
      <guid>http://zhen.org/blog/papers-i-read-2015-week-6/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://paperswelove.org/&#34;&gt;Papers We Love&lt;/a&gt; has been making rounds lately and a lot of people are excited about it. I also think it&amp;rsquo;s kind of cool since I&amp;rsquo;ve been reading a lot of research papers over the past year or so. I have been killing some trees because of that.&lt;/p&gt;

&lt;p&gt;My interests have been mostly around data analytics, but the specific focus areas have changed a few times. I have read papers on data structures (bloom filters, skiplist, bitmap compression, etc), security analytics, consumer behavioral analysis, loyalty analytics, and now back to security analytics. In fact, recently I started reading a few security research papers that I found on &lt;a href=&#34;http://www.covert.io/&#34;&gt;covert.io&lt;/a&gt;, put together by Jason Trost.&lt;/p&gt;

&lt;p&gt;In any case, I thought it might be an interesting idea to share some of the papers I read/scan/skim on weekly basis. This way I can also track what I read over time.&lt;/p&gt;

&lt;h3 id=&#34;random-ramblings:e2137c343047358d9912399632750231&#34;&gt;Random Ramblings&lt;/h3&gt;

&lt;p&gt;This week has been a disaster. I was the last one in the family to catch the cold, but probably lasted the longest. In fact I am still only about 50%. This whole week I have been having headaches, body aches, and haven&amp;rsquo;t been able to concentrate. My body must be trying to catch up on sleep or something. For a couple days I actually slept for almost 12 hours a night!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been meaning to work on &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; and finish updating the analyzer, but really had a hard time concentrating. Any non-working hours are basically spent in bed if I could.&lt;/p&gt;

&lt;p&gt;So this is probably the worst week to start the &amp;ldquo;Papers I Read&amp;rdquo; series since I only technically read 1 paper. But I am going to cheat a little, and list the papers I read over the past couple of weeks, pretty much all in my spare time.&lt;/p&gt;

&lt;p&gt;This week we also saw Sony&amp;rsquo;s accouncement that last year&amp;rsquo;s hack cost them &lt;a href=&#34;http://www.sony.net/SonyInfo/IR/financial/fr/150204_sony.pdf&#34;&gt;$15 million&lt;/a&gt; to investigate and remediate. It&amp;rsquo;s pretty crazy if you think about it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s assume that they hired a bunch of high-priced consultants, say $250/hour, to help comb through the logs and clean the systems. And let&amp;rsquo;s say &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; of the $15m is spent on these consultants. That&amp;rsquo;s &lt;code&gt;$10m / $250 = 40,000 hours&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say these consultants worked full time, non-stop, no weekends, no breaks, for 2 months since the announcement on Nov 24, 2014, that would be a team of 56 people (&lt;code&gt;40,000 hours / 60 days / 12 hours/day = 56&lt;/code&gt;) working 12 hour days!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll tell ya, these security guys are raking it in. They make money upfront by selling products/services to protect the company, then they make money in the back by selling forensic services to clean up after the hack.&lt;/p&gt;

&lt;p&gt;[Disclaimer: any mistake in my calculations/assumptions I blame on my drugged brain cells.]&lt;/p&gt;

&lt;h3 id=&#34;papers-i-read:e2137c343047358d9912399632750231&#34;&gt;Papers I Read&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf&#34;&gt;Beehive: Large-Scale Log Analysis for Detecting Suspicious Activity in Enterprise Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://minds.cs.umn.edu/publications/chapter.pdf&#34;&gt;Data Mining for Cyber Security&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;This chapter provides an overview of the Minnesota Intrusion Detection System (MINDS), which uses a suite of data mining based algorithms to address different aspects of cyber security. The various components of MINDS such as the scan detector, anomaly detector and the profiling module detect different types of attacks
and intrusions on a computer network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/VAST-%20Network%20Visibility%20Across%20Space%20and%20Time.pdf&#34;&gt;VAST: Network Visibility Across Space and Time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Key operational networking tasks, such as troubleshooting and defending against attacks, greatly benefit from attaining views of network activity that are unified across space and time. This means that data from heterogeneous devices and systems is treated in a uniformfashion, and that analyzing past activity and detecting future instances follow the same procedures. Based on previous ideas that formulated principles for comprehensive
network visibility [AKP+08], we present the design and architecture of Visibility Across Space and Time (VAST), an intelligent database that serves as a single vantage point into the network. The system is based on a generic event model to handle network data from disparate sources and provides a query architecture that allows operators or remote applications to extract events matching a given condition. We implemented a proof-of-principle prototype that can archive and index events from a wide range of sources. Moreover, we conducted a preliminary performance evaluation to verify that our implementation works efficient and as expected.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/Finding%20The%20Needle-%20Suppression%20of%20False%20Alarms%20in%20Large%20Intrusion%20Detection%20Data%20Sets.pdf&#34;&gt;Finding The Needle: Suppression of False Alarms in Large Intrusion Detection Data Sets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Managed security service providers (MSSPs) must manage and monitor thousands of intrusion detection sensors.
The sensors often vary by manufacturer and software version, making the problem of creating generalized tools to separate true attacks from false positives particularly difficult. Often times it is useful from an operations perspective to know if a particular sensor is acting out of character. We propose a solution to this problem using anomaly detection techniques over the set of alarms produced by the sensors. Similar to the manner in which an anomaly based sensor detects deviations from normal user or system behavior, we establish the baseline
behavior of a sensor and detect deviations from this baseline. We show that departures from this profile by a sensor have a high probability of being artifacts of genuine attacks. We evaluate a set of time-based Markovian heuristics against a simple compression algorithm and show that we are able to detect the existence of all attacks which were manually identified by security personnel, drastically reduce the number of false positives, and identify attacks which were overlooked during manual evaluation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://user.informatik.uni-goettingen.de/~krieck/docs/2013a-aisec.pdf&#34;&gt;A Close Look on n-Grams in Intrusion Detection: Anomaly Detection vs. Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Detection methods based on n-gram models have been widely studied for the identification of attacks and malicious software. These methods usually build on one of two learning schemes: anomaly detection, where a model of normality is constructed from n-grams, or classification, where a discrimination between benign and malicious n-grams is learned. Although successful in many security domains, previous work falls short of explaining why a particular scheme is used and more importantly what renders one favorable over the other for a given type of data. In this paper we provide a close look on n-gram models for intrusion detection. We specifically study anomaly detection and classification using n-grams and develop criteria for data being used in one or the other
scheme. Furthermore, we apply these criteria in the scope of web intrusion detection and empirically validate their effectiveness with different learning-based detection methods for client-side and service-side attacks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.scalyr.com/2014/05/searching-20-gbsec-systems-engineering-before-algorithms/&#34;&gt;Searching 20 GB/sec: Systems Engineering Before Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ok, this is a blog post, not a research paper, but it&amp;rsquo;s somewhat interesting nonetheless.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This article describes how we met that challenge using an “old school”, brute-force approach, by eliminating layers and avoiding complex data structures. There are lessons here that you can apply to your own engineering challenges.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS</title>
      <link>http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/</link>
      <pubDate>Sun, 01 Feb 2015 10:40:20 -0800</pubDate>
      
      <guid>http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/</guid>
      <description>

&lt;p&gt;Information here maybe outdated. Please visit &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;http://sequence.trustpath.com&lt;/a&gt; for latest.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;This is part 1 of the &lt;a href=&#34;http://sequence.trustpath.com&#34;&gt;sequence&lt;/a&gt; series.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/&#34;&gt;Part 1&lt;/a&gt; is about the high performance parser that can parse 100,000-200,000 MPs.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/&#34;&gt;Part 2&lt;/a&gt; is about automating the process of reducing 100 of 1000&amp;rsquo;s of log messages down to dozens of unique patterns.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/&#34;&gt;Part 3&lt;/a&gt; is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size) for scanning and tokenizing log messages&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;background:d4419a1968098b45153921045f06326c&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;sequence&lt;/code&gt; is a &lt;em&gt;high performance sequential log parser&lt;/em&gt;. It &lt;em&gt;sequentially&lt;/em&gt; goes through a log message, &lt;em&gt;parses&lt;/em&gt; out the meaningful parts, without the use regular expressions. It can achieve &lt;em&gt;high performance&lt;/em&gt; parsing of &lt;strong&gt;100,000 - 200,000 messages per second (MPS)&lt;/strong&gt; without the need to separate parsing rules by log source type.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;sequence&lt;/code&gt; is currently under active development and should be considered unstable until further notice.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you have a set of logs you would like me to test out, please feel free to &lt;a href=&#34;https://github.com/strace/sequence/issues&#34;&gt;open an issue&lt;/a&gt; and we can arrange a way for me to download and test your logs.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;motivation:d4419a1968098b45153921045f06326c&#34;&gt;Motivation&lt;/h3&gt;

&lt;p&gt;Log messages are notoriusly difficult to parse because they all have different formats. Industries (see Splunk, ArcSight, Tibco LogLogic, Sumo Logic, Logentries, Loggly, LogRhythm, etc etc etc) have been built to solve the problems of parsing, understanding and analyzing log messages.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you have a bunch of log files you like to parse. The first problem you will typically run into is you have no way of telling how many DIFFERENT types of messages there are, so you have no idea how much work there will be to develop rules to parse all the messages. Not only that, you have hundreds of thousands, if not  millions of messages, in front of you, and you have no idea what messages are worth parsing, and what&amp;rsquo;s not.&lt;/p&gt;

&lt;p&gt;The typical workflow is develop a set of regular expressions and keeps testing against the logs until some magical moment where all the logs you want parsed are parsed. Ask anyone who does this for a living and they will tell you this process is long, frustrating and error-prone.&lt;/p&gt;

&lt;p&gt;Even after you have developed a set of regular expressions that match the original set of messages, if new messages come in, you will have to determine which of the new messages need to be parsed. And if you develop a new set of regular expressions to parse those new messages, you still have no idea if the regular expressions will conflict with the ones you wrote before. If you write your regex parsers too liberally, it can easily parse the wrong messages.&lt;/p&gt;

&lt;p&gt;After all that, you will end up finding out the regex parsers are quite slow. It can typically parse several thousands messages per second. Given enough CPU resources on a large enough machine, regex parsers can probably parse tens of thousands of messages per second. Even to achieve this type of performance, you will likely need to limit the number of regular expressions the parser has. The more regex rules, the slower the parser will go.&lt;/p&gt;

&lt;p&gt;To work around this performance issue, companies have tried to separate the regex rules for different log message types into different parsers. For example, they will have a parser for Cisco ASA logs, a parser for sshd logs, a parser for Apache logs, etc etc. And then they will require the users to tell them which parser to use (usually by indicating the log source type of the originating IP address or host.)&lt;/p&gt;

&lt;p&gt;Sequence is developed to make analyzing and parsing log messages a lot easier and faster.&lt;/p&gt;

&lt;h3 id=&#34;performance:d4419a1968098b45153921045f06326c&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro. The first file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 1.69 secs, ~ 126319.27 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 2.89 secs, ~ 81323.41 msgs/sec

  $ ./sequence bench -d ../patterns -i ../data/asasshsudo.log
  Parsed 447745 messages in 4.47 secs, ~ 100159.65 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.00 secs, ~ 212711.83 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 1.56 secs, ~ 150769.68 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -d ../patterns -i ../data/asasshsudo.log -w 2
  Parsed 447745 messages in 2.52 secs, ~ 177875.94 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;documentation:d4419a1968098b45153921045f06326c&#34;&gt;Documentation&lt;/h3&gt;

&lt;p&gt;Documentation is available at godoc: &lt;a href=&#34;http://godoc.org/github.com/strace/sequence&#34;&gt;package&lt;/a&gt;, &lt;a href=&#34;http://godoc.org/github.com/strace/sequence/sequence&#34;&gt;command&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;license:d4419a1968098b45153921045f06326c&#34;&gt;License&lt;/h3&gt;

&lt;p&gt;Copyright &amp;copy; 2014 Dataence, LLC. All rights reserved.&lt;/p&gt;

&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &amp;ldquo;License&amp;rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;ldquo;AS IS&amp;rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.&lt;/p&gt;

&lt;h3 id=&#34;roadmap-futures:d4419a1968098b45153921045f06326c&#34;&gt;Roadmap / Futures&lt;/h3&gt;

&lt;p&gt;There are some pattern files developed for ASA, Sudo and SSH in the &lt;code&gt;patterns&lt;/code&gt; directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages. So currently there&amp;rsquo;s not a set roadmap.&lt;/p&gt;

&lt;h2 id=&#34;concepts:d4419a1968098b45153921045f06326c&#34;&gt;Concepts&lt;/h2&gt;

&lt;p&gt;The following concepts are part of the package:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;Token&lt;/em&gt; is a piece of information extracted from the original log message. It is a struct that contains fields for &lt;em&gt;TokenType&lt;/em&gt;, &lt;em&gt;FieldType&lt;/em&gt;, &lt;em&gt;Value&lt;/em&gt;, and indicators of whether it&amp;rsquo;s a key or value in the key=value pair.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;TokenType&lt;/em&gt; indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;FieldType&lt;/em&gt; indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Sequence&lt;/em&gt; is a list of Tokens. It is returned by the &lt;em&gt;Tokenizer&lt;/em&gt;, and the &lt;em&gt;Parser&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Scanner&lt;/em&gt; is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, IPv4 addresses, URLs, MAC addresses,
integers and floating point numbers. It also recgonizes key=value or key=&amp;ldquo;value&amp;rdquo; or key=&amp;lsquo;value&amp;rsquo; or key=&lt;value&gt; pairs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Parser&lt;/em&gt; is a tree-based parsing engine for log messages. It builds a parsing tree based on pattern sequence supplied, and for each message sequence, returns the matching pattern sequence. Each of the message tokens will be marked with the semantic field types.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sequence-command:d4419a1968098b45153921045f06326c&#34;&gt;Sequence Command&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; command is developed to demonstrate the use of this package. You can find it in the &lt;code&gt;sequence&lt;/code&gt; directory. The &lt;code&gt;sequence&lt;/code&gt; command implements the &lt;em&gt;sequential semantic log parser&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   Usage:
     sequence [command]

   Available Commands:
     scan                      scan will tokenize a log file or message and output a list of tokens
     parse                     parse will parse a log file and output a list of parsed tokens for each of the log messages
     bench                     benchmark the parsing of a log file, no output is provided
     help [command]            Help about any command
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;scan:d4419a1968098b45153921045f06326c&#34;&gt;Scan&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence scan [flags]

   Available Flags:
    -h, --help=false: help for scan
    -m, --msg=&amp;quot;&amp;quot;: message to tokenize
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence scan -m &amp;quot;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream&amp;quot;
  #   0: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 14 10:15:56&amp;quot; }
  #   1: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;testserver&amp;quot; }
  #   2: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sudo&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   4: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;gonner&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;tty&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pts/3&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;pwd&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/home/gonner&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  14: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  15: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;root&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  18: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;command&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/bin/su&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;-&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;ustream&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;parse:d4419a1968098b45153921045f06326c&#34;&gt;Parse&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence parse [flags]

   Available Flags:
    -h, --help=false: help for parse
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -o, --outfile=&amp;quot;&amp;quot;: output file, if empty, to stdout
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&amp;quot;&amp;quot;: initial pattern file, required
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command parses a file based on existing rules. Note that the
performance number (9570.20 msgs/sec) is mostly due to reading/writing to disk.
To get a more realistic performance number, see the benchmark section below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence parse -d ../../patterns -i ../../data/sshd.all  -o parsed.sshd
  Parsed 212897 messages in 22.25 secs, ~ 9570.20 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is an entry from the output file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
  #   0: { Field=&amp;quot;%createtime%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 15 19:39:26&amp;quot; }
  #   1: { Field=&amp;quot;%apphost%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;jlz&amp;quot; }
  #   2: { Field=&amp;quot;%appname%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;sshd&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;[&amp;quot; }
  #   4: { Field=&amp;quot;%sessionid%&amp;quot;, Type=&amp;quot;%integer%&amp;quot;, Value=&amp;quot;7778&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;]&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pam_unix&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;(&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sshd&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;session&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;)&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #  14: { Field=&amp;quot;%object%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;session&amp;quot; }
  #  15: { Field=&amp;quot;%action%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;opened&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;for&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  18: { Field=&amp;quot;%dstuser%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;jlz&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;by&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;(&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;uid&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  23: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%integer%&amp;quot;, Value=&amp;quot;0&amp;quot; }
  #  24: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;)&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;benchmark:d4419a1968098b45153921045f06326c&#34;&gt;Benchmark&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence bench [flags]

   Available Flags:
    -c, --cpuprofile=&amp;quot;&amp;quot;: CPU profile filename
    -h, --help=false: help for bench
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&amp;quot;&amp;quot;: pattern file, required
    -w, --workers=1: number of parsing workers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will benchmark the parsing of two files. First file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA
log file, averaging 180 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 1.69 secs, ~ 126319.27 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 2.89 secs, ~ 81323.41 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.00 secs, ~ 212711.83 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 1.56 secs, ~ 150769.68 msgs/sec
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Generating Porter2 FSM For Fun and Performance in Go</title>
      <link>http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/</link>
      <pubDate>Wed, 21 Jan 2015 20:48:44 -0800</pubDate>
      
      <guid>http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://godoc.org/github.com/surgebase/porter2&#34;&gt;&lt;img src=&#34;http://godoc.org/github.com/surgebase/porter2?status.svg&#34; alt=&#34;GoDoc&#34; /&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;tl-dr:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;tl;dr&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;This post describes the &lt;a href=&#34;https://github.com/surgebase/porter2&#34;&gt;Porter2&lt;/a&gt; package I implemented. It is written in Go (#golang).&lt;/li&gt;
&lt;li&gt;By using a &lt;a href=&#34;http://en.wikipedia.org/wiki/Finite-state_machine&#34;&gt;finite-state-machine&lt;/a&gt; approach to &lt;a href=&#34;http://snowball.tartarus.org/algorithms/english/stemmer.html&#34;&gt;Porter2&lt;/a&gt; &lt;a href=&#34;http://en.wikipedia.org/wiki/Stemming&#34;&gt;stemming&lt;/a&gt;, I was able to achieve 660% better performance compare to other Go implementations.&lt;/li&gt;
&lt;li&gt;FSM-based approach is great for known/fixed data set, but obviously not workable if the data set changes at runtime.&lt;/li&gt;
&lt;li&gt;Hand-coding FSM is a PITA!!! &lt;a href=&#34;https://github.com/surgebase/porter2/tree/master/cmd/suffixfsm&#34;&gt;Automate&lt;/a&gt; if possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;introduction:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In a personal project I am working on, I had the need to perform word stemming in two scenarios. First, I need to perform stemming for all the string literals in a LARGE corpus and then determine if the words are in a fixed set of literals. Second, I need to perform stemming for a subset of words in real-time, as messages stream in.&lt;/p&gt;

&lt;p&gt;In the first case, performance is important but not critical; in the second case, performance is a huge factor.&lt;/p&gt;

&lt;h3 id=&#34;stemming:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Stemming&lt;/h3&gt;

&lt;p&gt;To start, according to &lt;a href=&#34;http://en.wikipedia.org/wiki/Stemming&#34;&gt;wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Stemming is the term used in linguistic morphology and information retrieval to describe the process for reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As a quick example, the words &lt;em&gt;fail&lt;/em&gt;, &lt;em&gt;failed&lt;/em&gt;, and &lt;em&gt;failing&lt;/em&gt; all mean something has &lt;em&gt;failed&lt;/em&gt;. By stemming these three words, I will get a single form which is &lt;em&gt;fail&lt;/em&gt;. I can then just use &lt;em&gt;fail&lt;/em&gt; going forward instead of having to compare all three forms all the time.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://tartarus.org/martin/PorterStemmer/def.txt&#34;&gt;Porter&lt;/a&gt; stemming algorithm is by far the most commonly used stemmer and also considered to be one of the most gentle stemmers. The Porter stemming algorithm (or ‘Porter stemmer’) works by removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems. (&lt;a href=&#34;http://tartarus.org/martin/PorterStemmer/&#34;&gt;ref&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://snowball.tartarus.org/algorithms/english/stemmer.html&#34;&gt;Porter2&lt;/a&gt; is universally considered to be an enhancement over the original Porter algorithm. Porter2 has an improved set of rules and it&amp;rsquo;s widely used as well.&lt;/p&gt;

&lt;h2 id=&#34;implementation:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;This package, &lt;a href=&#34;https://github.com/surgebase/porter2&#34;&gt;Porter2&lt;/a&gt;, implements the Porter2 stemmer. It is written completely using finite state machines to perform suffix comparison, rather than the usual string-based or tree-based approaches. As a result, it is 660% faster compare to string comparison-based approach written in the same (Go) language.&lt;/p&gt;

&lt;p&gt;This implementation has been successfully validated with the dataset from &lt;a href=&#34;http://snowball.tartarus.org/algorithms/english/&#34;&gt;http://snowball.tartarus.org/algorithms/english/&lt;/a&gt;, so it should be in a usable state. If you encounter any issues, please feel free to &lt;a href=&#34;https://github.com/surgebase/porter2/issues&#34;&gt;open an issue&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Usage is fairly simple:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import &amp;quot;github.com/surgebase/porter2&amp;quot;

fmt.Println(porter2.Stem(&amp;quot;seaweed&amp;quot;)) // should get seawe
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;performance:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;This implementation by far has the highest performance of the various Go-based implementations, AFAICT. I tested a few of the implementations and the results are below.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Implementation&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;th&gt;Algorithm&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/surgebase/porter2&#34;&gt;surgebase&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;319.009358ms&lt;/td&gt;
&lt;td&gt;Porter2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/dchest/stemmer&#34;&gt;dchest&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;2.106912401s&lt;/td&gt;
&lt;td&gt;Porter2&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/kljensen/snowball&#34;&gt;kljensen&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;5.725917198s&lt;/td&gt;
&lt;td&gt;Porter2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To run the test again, you can run &lt;a href=&#34;https://github.com/surgebase/porter2/tree/master/cmd/compare&#34;&gt;compare.go&lt;/a&gt; (&lt;code&gt;go run compare.go&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&#34;state-machines:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;State Machines&lt;/h3&gt;

&lt;p&gt;Most of the implementations, like the ones in the table above, rely completely on suffix string comparison. Basically there&amp;rsquo;s a list of suffixes, and the code will loop through the list to see if there&amp;rsquo;s a match. Given most of the time you are looking for the longest match, so you order the list so the longest is the first one. So if you are luckly, the match will be early on the list. But regardless that&amp;rsquo;s a huge performance hit.&lt;/p&gt;

&lt;p&gt;This implementation is based completely on finite state machines to perform suffix comparison. You compare each chacter of the string starting at the last character going backwards. The state machines will determine what the longest suffix is.&lt;/p&gt;

&lt;p&gt;As an example, let&amp;rsquo;s look at the 3 suffixes from step0 of the porte2 algorithm. The goal, and it&amp;rsquo;s the same for all the other steps, it&amp;rsquo;s to find the longest matching suffix.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;
&#39;s
&#39;s&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you were to build a non-space-optimized &lt;a href=&#34;http://en.wikipedia.org/wiki/Suffix_tree&#34;&gt;suffix tree&lt;/a&gt;, you would get this, where R is the root of the tree, and any node with * is designated as a final state:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        R
       / \
      &#39;*  s
     /     \
    s       &#39;*
   /
  &#39;*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a fairly easy tree to build, and we actually did that in the FSM generator we will talk about later. However, to build a working suffix tree in Go, we would need to use a &lt;code&gt;map[rune]*node&lt;/code&gt; structure at each of the nodes. And then search the map for each rune we encounter.&lt;/p&gt;

&lt;p&gt;To test the performance of using a switch statement vs using a map, I wrote a &lt;a href=&#34;https://github.com/surgebase/porter2/tree/master/cmd/switchvsmap&#34;&gt;quick test&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;switch: 4.956523ms
   map: 10.016601ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The test basically runs a switch statement and a map each for 1,000,000 times. So it seems like using a switch statement is faster than a map. Though I think the compiler basically builds a map for all the switch case statements.  (Maybe we should call this post &lt;em&gt;Microbenchmarking for fun and performance&lt;/em&gt;?)&lt;/p&gt;

&lt;p&gt;In any case, let&amp;rsquo;s go with the switch approach. We basically need to unroll the suffix tree into a finite state machine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        R0
       / \
      &#39;1* s2
     /     \
    s3      &#39;4*
   /
  &#39;5*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To do that, we need to assign a state number to each of the nodes in the suffix tree, and output each of the states and the transitions based on the rune encountered. The tree above is the same as the one before, but now has a state number assigned to each node.&lt;/p&gt;

&lt;h3 id=&#34;generator:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Generator&lt;/h3&gt;

&lt;p&gt;I actually started building all the porter2 FSMs manually with a completely different approach than what I am describing here. I won&amp;rsquo;t go into details here but needless to say, it was disastrous. Not only was hand coding state machines extremely error-prone, the approach I was taking also had a lot of potential for bugs. It took me MANY HOURS to hand build those FSMs but at the end, I was happy to abandon all of them for the approach I am taking now.&lt;/p&gt;

&lt;p&gt;To reduce errors and make updating the FSM easier, I wrote a quick tool called &lt;a href=&#34;https://github.com/surgebase/porter2/tree/master/cmd/suffixfsm&#34;&gt;suffixfsm&lt;/a&gt; to generate the FSMs. The tool basically takes a list of suffixes, creates a suffix tree as described above, and unrolls the tree into a set of states using the &lt;code&gt;switch&lt;/code&gt; statement.&lt;/p&gt;

&lt;p&gt;It took me just a couple hours to write and debug the tool, and I was well on my way to fixing other bugs!&lt;/p&gt;

&lt;p&gt;For example, running the command &lt;code&gt;go run suffixfsm.go step0.txt&lt;/code&gt; generated the following code. This is a complete function for step0 of the porter2 algorithm. The only thing missing is what to do with each of the final states, which are in the last &lt;code&gt;switch&lt;/code&gt; statement.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var (
		l int = len(rs) // string length
		m int			// suffix length
		s int			// state
		f int			// end state of longgest suffix
		r rune			// current rune
	)

loop:
	for i := 0; i &amp;lt; l; i++ {
		r = rs[l-i-1]

		switch s {
		case 0:
			switch r {
			case &#39;\&#39;&#39;:
				s = 1
				m = 1
				f = 1
				// &#39; - final
			case &#39;s&#39;:
				s = 2
			default:
				break loop
			}
		case 1:
			switch r {
			case &#39;s&#39;:
				s = 4
			default:
				break loop
			}
		case 2:
			switch r {
			case &#39;\&#39;&#39;:
				s = 3
				m = 2
				f = 3
				// &#39;s - final
			default:
				break loop
			}
		case 4:
			switch r {
			case &#39;\&#39;&#39;:
				s = 5
				m = 3
				f = 5
				// &#39;s&#39; - final
			default:
				break loop
			}
		default:
			break loop
		}
	}

	switch f {
	case 1:
		// &#39; - final

	case 3:
		// &#39;s - final

	case 5:
		// &#39;s&#39; - final

	}

	return rs
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;finally:cb3ebc7d4c620be4b494c5a79db4f68b&#34;&gt;Finally&lt;/h2&gt;

&lt;p&gt;This is a technique that can probably be applied to any fixed data set. The performance may vary based on the size of the state machine so test it with both maps and FSM to see what works best.&lt;/p&gt;

&lt;p&gt;Happy Go&amp;rsquo;ing!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go: From a Non-Programmer&#39;s Perspective</title>
      <link>http://zhen.org/blog/golang-from-a-non-programmers-perspective/</link>
      <pubDate>Tue, 13 Jan 2015 13:30:00 -0800</pubDate>
      
      <guid>http://zhen.org/blog/golang-from-a-non-programmers-perspective/</guid>
      <description>

&lt;p&gt;&lt;em&gt;Warning: Long Post. Over 3900 words according to &lt;code&gt;wc&lt;/code&gt;. So read at your own risk. :)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://golang.org/&#34;&gt;Go&lt;/a&gt; is a fairly recent programming language &lt;a href=&#34;http://golang.org/doc/faq#history&#34;&gt;created&lt;/a&gt; by Robert Griesemer, Rob Pike and Ken Thompson of Google. It has risen in popularity over the the past few years, especially since Go 1.0 was released.&lt;/p&gt;

&lt;p&gt;There are a ton of posts out there that talks about the pros and cons of Go, and why one would use it or not. In addition, there&amp;rsquo;s a bunch of posts out there written by different developers coming from different perspectives, such as Python, Ruby, Node, Rust, etc, etc. Recently I even read a couple of Chinese blog posts on why Go is popular in China, and why some of the Chinese developers have abandoned Go, which are quite interesting as well.&lt;/p&gt;

&lt;p&gt;This post is my perspective of Go, how I picked it up, and what I think of it after using it for a while. It is not a post about why Go is better or worse than other languages.&lt;/p&gt;

&lt;p&gt;In short, I like Go. It&amp;rsquo;s the first programming language I&amp;rsquo;ve used in recent years that I can actually build some interesting projects, e.g., &lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt; (&lt;a href=&#34;http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/&#34;&gt;detailed post&lt;/a&gt;), in my limited spare time.&lt;/p&gt;

&lt;h2 id=&#34;my-background:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;My Background&lt;/h2&gt;

&lt;p&gt;I am not a programmer/developer. Not full-time, not part-time, not moonlight. I tell my colleagues and teams that &amp;ldquo;I am not technical.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;But I do have a technical background. I have a MSCS degree from way back when, and have spent the first 6-7 years of my career performing security audits and penetration tests, and building one of the world&amp;rsquo;s largest managed security services (at least at the time).&lt;/p&gt;

&lt;p&gt;My programming langauge progression, when I was technical, has been BASIC (high school), Pascal and C (college), Perl, PHP, Java, and Javascript (during my technical career). I can&amp;rsquo;t claim to be an &amp;ldquo;expert&amp;rdquo; in any of these languages, but I consider myself quite proficient in each at the time I was using them.&lt;/p&gt;

&lt;p&gt;I was also reasonably network and system savvy, in the sense that I can get myself in and around the Solaris and Linux (UN*X) systems pretty well, and understand the networking stack sufficiently. I consider myself fairly proficient with the various system commands and tools.&lt;/p&gt;

&lt;p&gt;For the past 12 years, however, I have not been a developer, nor a systems guy, nor a networking guy. Instead, I have been running product management for various startups and large companies in the security and infrastructure space.&lt;/p&gt;

&lt;p&gt;Since the career change, I&amp;rsquo;ve not done any meaningful code development. I&amp;rsquo;ve written a script here and there, but nothing that I would consider to be &amp;ldquo;software.&amp;rdquo; However, I&amp;rsquo;ve managed engineering teams as part of my resonsibility, in addition to product management, to produce large scale software.&lt;/p&gt;

&lt;p&gt;In the past 12 years, my most used IDE is called Microsoft Office. So, in short, I am probably semi-technical, and know just enough to be dangerous.&lt;/p&gt;

&lt;h3 id=&#34;my-history-with-go:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;My History with Go&lt;/h3&gt;

&lt;p&gt;In &lt;sup&gt;2011&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2012&lt;/sub&gt;, I had the responsibility of building a brand new engineering team (I was already running product management) at VMware to embark on a new strategic initiative. The nature of the product/service is not important now. However, at the time, because the team is brand new, we had some leeway in choosing a language for the project. VMware at the time was heavily Java, and specifically Spring given the &lt;a href=&#34;http://www.vmware.com/company/news/releases/springsource&#34;&gt;2009 acquisition of SpringSource&lt;/a&gt;. While the new team had mostly Java experience, there was desire to choose something less bloated, and something that had good support for the emerging patterns of distributed software.&lt;/p&gt;

&lt;h4 id=&#34;first-touch:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;First Touch&lt;/h4&gt;

&lt;p&gt;Some of the team members had experience with Scala, so that became an obvious option. I did some research on the web, and found some discussions of Go. At the time, Go hasn&amp;rsquo;t reached 1.0 yet, but there was already a buzz around it. I looked on Amazon, and found &lt;a href=&#34;http://www.amazon.com/gp/product/B0083RVAJW/ref=docs-os-doi_0&#34;&gt;The Way to Go&lt;/a&gt;, which was probably the only Go book around at the time. For $3 on the Kindle, it was well worth it. However, due to the nascent nature of Go (pre 1.0), it was not a comfortable choice so I didn&amp;rsquo;t put that as an option. But this was my &lt;strong&gt;first touch of Go and it felt relatively painless&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;At the end, the team chose Scala because of existing experience, and that in theory, people with Java experience should move fairly easily to Scala. We were the first team in VMware to use Scala and we were pretty excited about it.&lt;/p&gt;

&lt;p&gt;However, to this date, I am still not sure we made the right decision to move to Scala (not that it&amp;rsquo;s wrong either.) The learning curve I believe was higher than we originally anticipated. Many of the developers wrote Java code w/ Scala syntax. And hiring also became an issue. Basically every new developer that came onboard must be sent to Typesafe for training. It was simply not easy for most developers who came from a non-functional mindset to jump into a totally functional mindset. Lastly, the knowledge differences of new Scala developers and experienced ones made it more difficult for them to collaborate.&lt;/p&gt;

&lt;p&gt;I also tried to read up Scala and at least understand the concept. I even tried to take the online course on Coursera offered by Martin Odersky. However, I just could not get my non-functional mind to wrap around the functional Scala. And since I really didn&amp;rsquo;t need to code (nor the developers want me to), I gave up on learning Scala.&lt;/p&gt;

&lt;h4 id=&#34;second-touch:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Second Touch&lt;/h4&gt;

&lt;p&gt;In any case, fast forward 2 years to Q3 of 2013. I had since left VMware and joined my current company, &lt;a href=&#34;http://jolata.com&#34;&gt;Jolata&lt;/a&gt;, to build a big data network analytics solutions for mobile carriers and high-frequency trading financial services firms. We are a small startup that&amp;rsquo;s trying to do a ton of things. So even though I run products, I have to get my hands dirty often.&lt;/p&gt;

&lt;p&gt;One of the things we had to do as a company is to build a repeatable demo environment. The goal is to have a prebuilt vagrant VM that we can run on our Macs, and we can demonstrate our product without connecting to the network. The requirement was that we had an interesting set of data in the database so we can walk through different scenarios.&lt;/p&gt;

&lt;p&gt;The data set we needed was network flow data. And to make the UI look realistic, interesting and non-blocky, we wanted to generate noisy data so the UI looks like it&amp;rsquo;s monitoring a real network. Because all of the developers are focused on feature development, I took on the task of building out the data set.&lt;/p&gt;

&lt;p&gt;By now, Go has released v1.1 and on its way to 1.2. It was then I started seriously considering Go as a candidate for this project. To build a tool that can generate the data set, we needed two libraries. The first is a &lt;a href=&#34;http://en.wikipedia.org/wiki/Perlin_noise&#34;&gt;Perlin Noise&lt;/a&gt; generator, and the second is &lt;a href=&#34;https://code.google.com/p/cityhash/&#34;&gt;Google&amp;rsquo;s Cityhash&lt;/a&gt;. Neither of these were available in Go (or not that I remember). I thought this would be a great opportunity to test out Go. The end results were my Go Learn Projects &lt;a href=&#34;https://github.com/dataence/perlinnoise&#34;&gt;#0 Perlin&lt;/a&gt;, and &lt;a href=&#34;https://github.com/dataence/cityhash&#34;&gt;#1 Cityhash&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Both of these projects were relatively simple since I didn&amp;rsquo;t have to spend a lot of time figuring out HOW to write them. Perlin Noise has well-established C libraries and algorithms, and Cityhash was written in C so it was easy to translate to Go. However, these projects gave me a good feel of how Go works.&lt;/p&gt;

&lt;p&gt;In the end, I wrote the data generator in Go (private repo) and got the first taste of goroutines. Again, &lt;strong&gt;this second touch with Go was also relatively painless&lt;/strong&gt;. The only confusion I had at the time was the Go source tree structure. Trying to understand $GOROOT, $GOPATH and other Go environment variables were all new to me. This was also the first time in 10 years that I really spent time writing a piece of software, so I just considered the confusion as my inexperience.&lt;/p&gt;

&lt;h4 id=&#34;third-touch-and-beyond:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Third Touch and Beyond&lt;/h4&gt;

&lt;p&gt;Today, I no longer code at work as we have more developers now. Also, the Jolata product is mostly C/C++, Java and Node, so Go is also no longer in the mix. However, After getting a taste of Go in the first couple of Go projects, I&amp;rsquo;ve since spent a tremendous amount of my limited personal spare time working with it.&lt;/p&gt;

&lt;p&gt;I have since written various libraries for &lt;a href=&#34;https://github.com/dataence/bitmap&#34;&gt;bitmap compression&lt;/a&gt;, &lt;a href=&#34;https://github.com/dataence/encoding&#34;&gt;integer compression&lt;/a&gt;, &lt;a href=&#34;https://github.com/dataence/bloom&#34;&gt;bloom filters&lt;/a&gt;, &lt;a href=&#34;https://github.com/dataence/skiplist&#34;&gt;skiplist&lt;/a&gt;, and &lt;a href=&#34;https://github.com/dataence&#34;&gt;many others&lt;/a&gt;. And I have &lt;a href=&#34;http://zhen.org/blog/&#34;&gt;blogged my journey&lt;/a&gt; along the way as I learn. With these projects, I&amp;rsquo;ve learned how to use the Go toolchain, how to write idiomatic Go, how to write tests with Go, and more importantly, how to optimize Go.&lt;/p&gt;

&lt;p&gt;Interestingly, one of my most popular posts is &lt;a href=&#34;http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/&#34;&gt;Go vs Java: Decoding Billions of Integers Per Second&lt;/a&gt;. This tells me that a lot of Java developers are potentially looking to adopt Go.&lt;/p&gt;

&lt;p&gt;All these have allowed me to learn Go enough to build a real project, &lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt;. It is by far my most popular project and one that I expect to continue developing.&lt;/p&gt;

&lt;h2 id=&#34;my-views-on-go:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;My Views on Go&lt;/h2&gt;

&lt;p&gt;Go is not just a langauge, it also has a very active community around it. The views are based on my observation over the past 1.5 years of using Go. My Go environment is primary Sublime Text 3 with GoSublime plugin.&lt;/p&gt;

&lt;h3 id=&#34;as-a-language:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;As a Language&amp;hellip;&lt;/h3&gt;

&lt;p&gt;I am not a language theorist, nor do I claim to be a language expert. In fact, prior to actually using Go, I&amp;rsquo;ve barely heard of generics, communicating sequential processes, and other &amp;ldquo;cool&amp;rdquo; and &amp;ldquo;advanced&amp;rdquo; concepts. I&amp;rsquo;ve heard of all the new cool programming languages such as Clojure and Rust, but have never looked at any of the code. So my view of Go is basically one of a developer n00b.&lt;/p&gt;

&lt;p&gt;In a way, I consider that to be an advantage coming in to a new programming language, in that I have no preconceived notion of how things &amp;ldquo;SHOULD&amp;rdquo; be. I can learn the language and use the constructs as they were intended, and not have to question WHY it was designed that way because it&amp;rsquo;s different than what I know.&lt;/p&gt;

&lt;p&gt;Others may consider this to be a huge disadvantage, since I don&amp;rsquo;t know any better. There maybe constructs in other languages that would make my work a lot easier, or make my code a lot simpler.&lt;/p&gt;

&lt;p&gt;However, as long as the language doesn&amp;rsquo;t slow me down, then I feel it&amp;rsquo;s serving my needs.&lt;/p&gt;

&lt;h4 id=&#34;go-is-simple:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Go is Simple&lt;/h4&gt;

&lt;p&gt;As a language for a new deverloper, Go was very easy to pick up. Go&amp;rsquo;s design is fairly simple and minimalistic. You can sit down and read through the &lt;a href=&#34;https://golang.org/ref/spec&#34;&gt;Language Specification&lt;/a&gt; fairly quickly in an idle afternoon. I actually didn&amp;rsquo;t find the language reference until later. My first touch of Go was by scanning through the book &lt;em&gt;The Way To Go&lt;/em&gt;. Regardless, there&amp;rsquo;s not a lot to the language so it&amp;rsquo;s relatively easy for someone like myself to pick up the basics. (Btw, I&amp;rsquo;ve also never gone through the &lt;a href=&#34;https://tour.golang.org/&#34;&gt;Go Tour&lt;/a&gt;. I know it&amp;rsquo;s highly recommended to all new Go developers. I just never did it.)&lt;/p&gt;

&lt;p&gt;There are more advanced concepts in Go, such as interface, channel, and goroutine. Channel in general is a fairly straightforward concept. Most new programmers should be able to understand that quickly. You write stuff in, you read stuff out. It&amp;rsquo;s that simple. From there, you can slowly expand on the concept as you &lt;em&gt;go&lt;/em&gt; along by adding buffered channels, or ranging over channels, or checking if the read is ok, or using quit channels.&lt;/p&gt;

&lt;p&gt;For anyone coming from a language with threads, goroutine is not a difficult concept to understand. It&amp;rsquo;s basically a light-weight thread that can be executed concurrently. You can run any function as a goroutine.&lt;/p&gt;

&lt;p&gt;The more difficult concept is interface. That&amp;rsquo;s because it&amp;rsquo;s a &lt;strike&gt;fairly new concept that doesn&amp;rsquo;t really exist in&lt;/strike&gt; concept that&amp;rsquo;s fairly different than other languages. Once you understand what interfaces are, it&amp;rsquo;s fairly easy to start using them. However, designing your own interfaces is a different story.&lt;/p&gt;

&lt;p&gt;The one thing I&amp;rsquo;ve seen most developers complain about Go is the lack of generics. Egon made a nice &lt;a href=&#34;https://docs.google.com/document/d/1vrAy9gMpMoS3uaVphB32uVXX4pi-HnNjkMEgyAHX4N4&#34;&gt;Summary of Go Generics Discussions&lt;/a&gt; that you can read through. For me personally, I don&amp;rsquo;t know any better. I have never used generics and I haven&amp;rsquo;t found a situation where I strongly require it.&lt;/p&gt;

&lt;p&gt;As a language a team, the simplicity of Go is &lt;em&gt;HUGE&lt;/em&gt;. It allows develoeprs to quickly come up to speed and be productive in the shortest period of time. And in this case, time is literally money.&lt;/p&gt;

&lt;h4 id=&#34;go-is-opinionated:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Go is Opinionated&lt;/h4&gt;

&lt;p&gt;Go is opinionated in many ways. For example, probably one of the most frustrating thing about Go is how to structure the code directory. Unlike other languages where you can just create a directory and get started, Go wants you to put things in $GOPATH. It took a few readings of &lt;a href=&#34;https://golang.org/doc/code.html&#34;&gt;How to Write Go Code&lt;/a&gt; for me to grasp what&amp;rsquo;s going on, and it took even longer for me to really get the hang of code organization, and how Go imports packages (e.g., &lt;code&gt;go get&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;If I go back and look at my first internal project, I would probably cry because it&amp;rsquo;s all organized in a non-idiomatic way. However, once I got the hang of how Go expects things to be organized, it no longer was a obstacle for me. &lt;strong&gt;Instead of fighting the way things should be organized in Go, I learned to &lt;em&gt;go&lt;/em&gt; with the flow.&lt;/strong&gt; At the end of the day, the $GOPATH organizational structure actually helps me track the different packages I import.&lt;/p&gt;

&lt;p&gt;Another way Go is opinionated is code formatting. Go, and Go developers, expect that all Go programs are formatted with &lt;a href=&#34;http://blog.golang.org/go-fmt-your-code&#34;&gt;&lt;code&gt;go fmt&lt;/code&gt;&lt;/a&gt;. A lot of developers hate it and some even listed it as a top reason for leaving Go. However, this is one of those things that you just have to learn to &lt;em&gt;go&lt;/em&gt; with the flow. Personally I love it.&lt;/p&gt;

&lt;p&gt;And as a team language it will save a ton of argument time. Again, time is money for a new team. When my new VMware team got started, we probably spent a good 30 person-hours debating code formatting. That&amp;rsquo;s $2700 at a $180K fully-burdened rate. And that&amp;rsquo;s not counting all the issues we will run into later trying to reformat code that&amp;rsquo;s not properly formatted.&lt;/p&gt;

&lt;p&gt;Go is also very opininated in terms of variable use and package import. If a variable is declared but not used, the Go compiler will complain. If a package is imported but not used, the Go compiler will complain. Personally, I like the compiler complaining about the unused variables. It keeps the code clean, and reduce the chance of unexpected bugs. I am less concerned about unused packages but have also learned to live with the compiler complains. I use &lt;a href=&#34;https://github.com/bradfitz/goimports&#34;&gt;goimports&lt;/a&gt; in Sublime Text 3 to effectively and quickly take care of the import statements. In fact, in 99% of the cases I don&amp;rsquo;t even need to type in the import statements myself.&lt;/p&gt;

&lt;h4 id=&#34;go-is-safe:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Go is Safe&lt;/h4&gt;

&lt;p&gt;Go is safe for a couple of reasons. For a new developer, Go does not make it easy for you to be lazy. For example, Go is a statically typed language, which means every variable must explicitly have a type associated with it. The Go compiler does infer types under certain situations, but regardless, there&amp;rsquo;s a type for every variable. This may feel uncomfortable for developers coming from dynamic languages, but the benefit definitely outweighs the cost. I&amp;rsquo;ve experience first hand, as a product person waiting for bugs to be fixed, how long it takes to troubleshoot problems in Node. Having static types gives you a feeling of &amp;ldquo;correctness&amp;rdquo; after you have written the code.&lt;/p&gt;

&lt;p&gt;Another example of Go not allowing you to be lazy is that Go&amp;rsquo;s error handling is through the return of &lt;code&gt;error&lt;/code&gt; from functions. There has been a ton of discussions and debates on the merit of &lt;code&gt;error&lt;/code&gt; vs exception handling so I won&amp;rsquo;t go through it here. However, for a new programmer, it really requires your explicit attention to handle the errors. And I consider that to be a good thing as you know what to expect at each step of the program.&lt;/p&gt;

&lt;p&gt;Making things explicit and making it harder for developers to be lazy are a couple of the reasons that make Go safe.&lt;/p&gt;

&lt;p&gt;Another reason is that &lt;a href=&#34;http://golang.org/doc/faq#garbage_collection&#34;&gt;Go has a garbage collector&lt;/a&gt;. This makes it different from C/C++ as it no longer require developers to perform memory management. The difficulty in memory management is the single biggest source of memory leaks in C/C++ programs. Having a GC removes that burden from developers and makes the overall program much safer. Having said that, there&amp;rsquo;s much improvement to be made to the GC given its nascent state. And, as I learned over the past 1.5 years, to write high performance programs in Go today, developers need to make serious efforts to reduce GC pressure.&lt;/p&gt;

&lt;p&gt;Again, as a team langauge, the safety aspect is very important. The team will likely end up spending much less time dealing with memory bugs and focus more on feature development.&lt;/p&gt;

&lt;h4 id=&#34;go-is-powerful:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Go is Powerful&lt;/h4&gt;

&lt;p&gt;What makes Go powerful are its simplicity, its high performance, and advanced concepts such as channels, goroutines, interfaces, type composition, etc. We have discussed all of these in previous sections.&lt;/p&gt;

&lt;p&gt;In addition to all that, one of the killer feature of Go is that all Go programs are statically compiled into a single binary. There&amp;rsquo;s no shared libraries to worry about. There&amp;rsquo;s no jar files to worry about. There&amp;rsquo;s no packages to bundle. It&amp;rsquo;s just a single binary. And that&amp;rsquo;s an extremely powerful feature from the deployment and maintenance perspectives. To deploy a Go program, you just need to copy a single Go binary over. To update it, copy a single Go binary over.&lt;/p&gt;

&lt;p&gt;In contrast, to deploy a Node.js application, you may end up downloading hundreds of little packages at deployment time. And you have to worry about whether all these packages are compatible. The Node community has obviously developed a lot of good tools to manage dependencies and version control. But still, every time I see a Node app get deployed on a new machine, and have to download literally hundreds of little packages, I die a little inside.&lt;/p&gt;

&lt;p&gt;Also, if you deploy C/C++ programs and depend on shared libraries, now you have to worry about OS and shared library version compatibility issues.&lt;/p&gt;

&lt;p&gt;Another powerful feature of Go is that you can mix C and assembly code with Go code in a single program. I haven&amp;rsquo;t used this extensively, but in my attempt to &lt;a href=&#34;http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/&#34;&gt;optimize&lt;/a&gt; the &lt;a href=&#34;https://github.com/dataence/encoding&#34;&gt;integer compression&lt;/a&gt; library, I added different C and assembly snippets to try to squeeze the last ounce of performance out of Go. It was fairly easy and straightforward to do.&lt;/p&gt;

&lt;p&gt;One last thing, Go has a very large and complete standard library. It enables developers to do most, if not all, of their work quickly and efficiently. As the language matures and the community grows, there will be more and more 3rd party open source libraries one can leverage.&lt;/p&gt;

&lt;h3 id=&#34;as-a-community:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;As a Community&lt;/h3&gt;

&lt;p&gt;Today, Go has a very active community behind it. Specifically, the information sources I&amp;rsquo;ve followed and gotten help from include &lt;a href=&#34;https://botbot.me/freenode/go-nuts/&#34;&gt;#go-nuts IRC&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/search?q=golang&amp;amp;sort=new&amp;amp;restrict_sr=on&amp;amp;t=all&#34;&gt;golang subreddit&lt;/a&gt;, and obviously the &lt;a href=&#34;https://groups.google.com/forum/#!forum/golang-nuts&#34;&gt;golang-nuts mailing list&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I spent quite a bit of time in IRC when I first started. I&amp;rsquo;ve gotten help from quite a few people such as dsal, tv42, and others, and I am grateful for that. I am spending less time there now because of the limited time I have (remember, my day job is not development. :)&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s been some sentiments in the developer community that Go developers (gophers) are Google worshippers, don&amp;rsquo;t accept any feedbacks on language changes, harsh to new comers who come from different languages,  difficult to ask questions because the sample code is not on play.golang.org, etc etc.&lt;/p&gt;

&lt;p&gt;To be clear, I&amp;rsquo;ve never really spent much time with the different language communites, even when I was technical. So I have nothing else to compare to. So I can only speak from a human interaction level.&lt;/p&gt;

&lt;p&gt;I can see it from both perspectives. For example, developers coming from different language backgrounds sometimes have experience with a different way of doing things. When they want to perfrom the same tasks in Go, they ask the question by saying here&amp;rsquo;s how I solved this problem in language X, how do I translate that to Go?&lt;/p&gt;

&lt;p&gt;In some cases I&amp;rsquo;ve definitely seen people responding by saying that&amp;rsquo;s not how Go works and you are doing it wrong. That type of response can quickly create negative sentiment and kill the conversation.&lt;/p&gt;

&lt;p&gt;Another type of response I&amp;rsquo;ve seen is some developers telling the original poster (OP) that they are not asking questions the right way, and then promptly sending the OP a link to some web page on how to properly ask questions. Again, I can see how the OP can have a negative view on the matter.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve expereinced some of this myself. When I implemented a &lt;a href=&#34;https://github.com/dataence/bloom&#34;&gt;Bloom Filter&lt;/a&gt; package last year, I did a bunch of performance tests and wrote a &lt;a href=&#34;http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/&#34;&gt;blog post&lt;/a&gt; about the it. As a newbie learning Go, I felt like I accomplished something and I was pretty happy with it. I posted the link to reddit, and the first comment I got was&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Downvoted because I dislike this pattern of learning a new language and then immediately publishing performance data about it, before you know how to write idiomatic or performant code in it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Ouch!!&lt;/strong&gt; As a new Go developer, this is not the response I expected. In the end though, the commenter also pointed out something that helped me improve the performance of the implementation. I was grateful for that. It was also then I realized how important it is to reduce the number of allocation in order to reduce the Go GC pressure.&lt;/p&gt;

&lt;p&gt;In hindsight, the comment has a very valid point. I can understand why some developers would feel annoyed about benchmarks from people who have no idea on what they are doing. Regardless, being nice is not a bad thing. Saying things like &amp;ldquo;WTF is wrong with you&amp;rdquo; (not related to the bloom filter post) will only push new developers away.&lt;/p&gt;

&lt;p&gt;I quickly got over the sting because I am just too old to care about what others think I should or should not do. I continued my learning process by writing and optimizing Go packages, and posting the results in my blog. In fact, the &lt;a href=&#34;http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/&#34;&gt;Go vs Java: Decoding Billions of Integers Per Second&lt;/a&gt; post has many of the optimization techniques I tried to increase the performance of Go programs.&lt;/p&gt;

&lt;p&gt;Overall though, I felt I&amp;rsquo;ve learned a ton from the Go community. People have generally been helpful and are willing to offer solutions to problems. I have nothing to compare to, but I feel that the positives of the Go community far outweighs any negatives.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:b5701f21733ea64a37f8a0bc7a6884f3&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In conclusion, it has been a tremendous 1.5 years of working with Go, and seeing Go grow both as a language and as a community has been very rewarding.&lt;/p&gt;

&lt;p&gt;My focus now, in my limited spare personal time, is to continue the development of &lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt;, which is a high performance MQTT broker and client library that aims to be fully compliant with MQTT 3.1 and 3.1.1 specs.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PingMQ: A SurgeMQ-based ICMP Monitoring Tool</title>
      <link>http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/</link>
      <pubDate>Thu, 25 Dec 2014 00:00:33 -0800</pubDate>
      
      <guid>http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/surge/surgemq/tree/master/cmd/pingmq&#34;&gt;pingmq&lt;/a&gt; is developed to demonstrate the different use cases one can use &lt;a href=&#34;//surgemq.com&#34;&gt;SurgeMQ&lt;/a&gt;, a high performance MQTT server and client library. In this simplified use case, a network administrator can setup server uptime monitoring system by periodically sending ICMP ECHO_REQUEST to all the IPs in their network, and send the results to SurgeMQ.&lt;/p&gt;

&lt;p&gt;Then multiple clients can subscribe to results based on their different needs. For example, a client maybe only interested in any failed ping attempts, as that would indicate a host might be down. After a certain number of failures the client may then raise some type of flag to indicate host down.&lt;/p&gt;

&lt;p&gt;There are three benefits of using SurgeMQ for this use case.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First, with all the different monitoring tools out there that wants to know if hosts are up or down, they can all now subscribe to a single source of information. They no longer need to write their own uptime tools.&lt;/li&gt;
&lt;li&gt;Second, assuming there are 5 monitoring tools on the network that wants to ping each and every host, the small packets are going to congest the network. The company can save 80% on their uptime monitoring bandwidth by having a single tool that pings the hosts, and have the rest subscribe to the results.&lt;/li&gt;
&lt;li&gt;Third/last, the company can enhance their security posture by placing tighter restrictions on their firewalls if there&amp;rsquo;s only a single host that can send ICMP ECHO_REQUESTS to all other hosts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following commands will run pingmq as a server, pinging the 8.8.8.0/28 CIDR block, and publishing the results to /ping/success/{ip} and /ping/failure/{ip} topics every 30 seconds. &lt;code&gt;sudo&lt;/code&gt; is needed because we are using RAW sockets and that requires root privilege.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build
$ sudo ./pingmq server -p 8.8.8.0/28 -i 30
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./pingmq client -t /ping/failure/+
8.8.8.6: Request timed out for seq 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./pingmq client -t /ping/success/+
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One can also subscribe to a specific IP by using the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./pingmq client -t /ping/+/8.8.8.8
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;commands:2953f7aa0fe0aa3757fe6bf025e7fa57&#34;&gt;Commands&lt;/h3&gt;

&lt;p&gt;There are two builtin commands for &lt;code&gt;pingmq&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;pingmq server&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage:
  pingmq server [flags]

 Available Flags:
  -h, --help=false: help for server
  -i, --interval=60: ping interval in seconds
  -p, --ping=[]: Comma separated list of IPv4 addresses to ping
  -q, --quiet=false: print out ping results
  -u, --uri=&amp;quot;tcp://:5836&amp;quot;: URI to run the server on
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;pingmq client&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage:
  pingmq client [flags]

 Available Flags:
  -h, --help=false: help for client
  -s, --server=&amp;quot;tcp://127.0.0.1:5836&amp;quot;: PingMQ server to connect to
  -t, --topic=[]: Comma separated list of topics to subscribe to
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ip-addresses:2953f7aa0fe0aa3757fe6bf025e7fa57&#34;&gt;IP Addresses&lt;/h3&gt;

&lt;p&gt;To list IPs you like to use with &lt;code&gt;pingmq&lt;/code&gt;, you can use the following formats:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;10.1.1.1      -&amp;gt; 10.1.1.1
10.1.1.1,2    -&amp;gt; 10.1.1.1, 10.1.1.2
10.1.1,2.1    -&amp;gt; 10.1.1.1, 10.1.2.1
10.1.1,2.1,2  -&amp;gt; 10.1.1.1, 10.1.1.2 10.1.2.1, 10.1.2.2
10.1.1.1-2    -&amp;gt; 10.1.1.1, 10.1.1.2
10.1.1.-2     -&amp;gt; 10.1.1.0, 10.1.1.1, 10.1.1.2
10.1.1.1-10   -&amp;gt; 10.1.1.1, 10.1.1.2 ... 10.1.1.10
10.1.1.1-     -&amp;gt; 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-3.1    -&amp;gt; 10.1.1.1, 10.1.2.1, 10.1.3.1
10.1-3.1-3.1  -&amp;gt; 10.1.1.1, 10.1.2.1, 10.1.3.1, 10.2.1.1, 10.2.2.1, 10.2.3.1, 10.3.1.1, 10.3.2.1, 10.3.3.1
10.1.1        -&amp;gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-2      -&amp;gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1-2        -&amp;gt; 10.1.0.0, 10.1.0,1 ... 10.2.255.254, 10..2.255.255
10            -&amp;gt; 10.0.0.0 ... 10.255.255.255
10.1.1.2,3,4  -&amp;gt; 10.1.1.1, 10.1.1.2, 10.1.1.3, 10.1.1.4
10.1.1,2      -&amp;gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1.1/28     -&amp;gt; 10.1.1.0 ... 10.1.1.255
10.1.1.0/28   -&amp;gt; 10.1.1.0 ... 10.1.1.15
10.1.1.0/30   -&amp;gt; 10.1.1.0, 10.1.1.1, 10.1.1.2, 10.1.1.3
10.1.1.128/25 -&amp;gt; 10.1.1.128 ... 10.1.1.255
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;topic-format:2953f7aa0fe0aa3757fe6bf025e7fa57&#34;&gt;Topic Format&lt;/h3&gt;

&lt;p&gt;TO subscribe to the &lt;code&gt;pingmq&lt;/code&gt; results, you can use the following formats:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/ping/#&lt;/code&gt; will subscribe to both success and failed pings for all IP addresses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping/success/+&lt;/code&gt; will subscribe to success pings for all IP addresses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping/failure/+&lt;/code&gt; will subscribe to failed pings for all IP addresses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping/+/8.8.8.8&lt;/code&gt; will subscribe to both success and failed pings for all IP 8.8.8.8&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;building:2953f7aa0fe0aa3757fe6bf025e7fa57&#34;&gt;Building&lt;/h3&gt;

&lt;p&gt;To build &lt;code&gt;pingmq&lt;/code&gt;, you need to have installed &lt;a href=&#34;http://golang.org&#34;&gt;Go 1.3.3 or 1.4&lt;/a&gt;. Then run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# go get github.com/surge/surgemq
# cd surgemq/examples/pingmq
# go build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, you should see the &lt;code&gt;pingmq&lt;/code&gt; command in the pingmq directory.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SurgeMQ: High Performance MQTT Server and Client Libraries in Go</title>
      <link>http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/</link>
      <pubDate>Wed, 24 Dec 2014 19:20:40 -0800</pubDate>
      
      <guid>http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Happy Holidays!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is more of an announcement post as SurgeMQ is now compatibility-tested with some of the popular MQTT clients out there, and it&amp;rsquo;s reaching &lt;em&gt;playable&lt;/em&gt; state.&lt;/p&gt;

&lt;p&gt;For completeness sake, please bear with some of the duplicate content in this post. The &lt;a href=&#34;//blog/surgemq-mqtt-message-queue-750k-mps/&#34;&gt;last post&lt;/a&gt; made front page of &lt;a href=&#34;https://news.ycombinator.com/item?id=8708921&#34;&gt;Hacker News&lt;/a&gt; and generated some great comments and discussions.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;SurgeMQ is a high performance MQTT broker and client library that aims to be fully compliant with MQTT 3.1 and 3.1.1 specs. The primary package that&amp;rsquo;s of interest is package &lt;a href=&#34;http://godoc.org/github.com/surge/surgemq/service&#34;&gt;service&lt;/a&gt;. It provides the MQTT Server and Client services in a library form.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SurgeMQ is currently under active development and should be considered unstable until further notice.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;mqtt:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;MQTT&lt;/h3&gt;

&lt;p&gt;According to the &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT spec&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.&lt;/p&gt;

&lt;p&gt;The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications.&lt;/li&gt;
&lt;li&gt;A messaging transport that is agnostic to the content of the payload.&lt;/li&gt;
&lt;li&gt;Three qualities of service for message delivery:

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;At most once&amp;rdquo;, where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;At least once&amp;rdquo;, where messages are assured to arrive but duplicates can occur.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Exactly once&amp;rdquo;, where message are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A small transport overhead and protocol exchanges minimized to reduce network traffic.&lt;/li&gt;
&lt;li&gt;A mechanism to notify interested parties when an abnormal disconnection occurs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;There&amp;rsquo;s some very large implementation of MQTT such as &lt;a href=&#34;https://www.facebook.com/notes/facebook-engineering/building-facebook-messenger/10150259350998920&#34;&gt;Facebook Messenger&lt;/a&gt;. There&amp;rsquo;s also an active Eclipse project, &lt;a href=&#34;https://eclipse.org/paho/&#34;&gt;Paho&lt;/a&gt;, that provides scalable open-source client implementations for many different languages, including C/C++, Java, Python, JavaScript, C# .Net and Go.&lt;/p&gt;

&lt;h3 id=&#34;features-limitations-and-future:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Features, Limitations, and Future&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Supports QOS 0, 1 and 2 messages&lt;/li&gt;
&lt;li&gt;Supports will messages&lt;/li&gt;
&lt;li&gt;Supports retained messages (add/remove)&lt;/li&gt;
&lt;li&gt;Pretty much everything in the spec except for the list below&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All features supported are in memory only. Once the server restarts everything is cleared.

&lt;ul&gt;
&lt;li&gt;However, all the components are written to be pluggable so one can write plugins based on the Go interfaces defined.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Message redelivery on reconnect is not currently supported.&lt;/li&gt;
&lt;li&gt;Message offline queueing on disconnect is not supported. Though this is also not a specific requirement for MQTT.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Future&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Message re-delivery (DUP)&lt;/li&gt;
&lt;li&gt;$SYS topics&lt;/li&gt;
&lt;li&gt;Server bridge&lt;/li&gt;
&lt;li&gt;Ack timeout/retry&lt;/li&gt;
&lt;li&gt;Session persistence&lt;/li&gt;
&lt;li&gt;Better authentication modules&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;performance:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;Current performance benchmark of SurgeMQ, running all publishers, subscribers and broker on a single 4-core (2.8Ghz i7) MacBook Pro, is able to achieve:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;over &lt;strong&gt;400,000 MPS&lt;/strong&gt; in a 1:1 single publisher and single producer configuration&lt;/li&gt;
&lt;li&gt;over &lt;strong&gt;450,000 MPS&lt;/strong&gt; in a 20:1 fan-in configuration&lt;/li&gt;
&lt;li&gt;over &lt;strong&gt;750,000 MPS&lt;/strong&gt; in a 1:20 fan-out configuration&lt;/li&gt;
&lt;li&gt;over &lt;strong&gt;700,000 MPS&lt;/strong&gt; in a full mesh configuration with 20 clients&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;compatibility:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Compatibility&lt;/h3&gt;

&lt;p&gt;In addition, SurgeMQ has been tested with the following client libraries and it &lt;em&gt;seems&lt;/em&gt; to work:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;libmosquitto 1.3.5 (in C).&lt;/em&gt; Tested with the bundled test programs msgsps_pub and msgsps_sub&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paho MQTT Conformance/Interoperability Testing Suite (in Python).&lt;/em&gt; Tested with all 10 test cases, 3 did not pass. They are

&lt;ol&gt;
&lt;li&gt;&amp;ldquo;offline messages queueing test&amp;rdquo; which is not supported by SurgeMQ&lt;/li&gt;
&lt;li&gt;&amp;ldquo;redelivery on reconnect test&amp;rdquo; which is not yet implemented by SurgeMQ&lt;/li&gt;
&lt;li&gt;&amp;ldquo;run subscribe failure test&amp;rdquo; which is not a valid test&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paho Go Client Library (in Go).&lt;/em&gt; Tested with one of the tests in the library, in fact, that tests is now part of the tests for SurgeMQ.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paho C Client library (in C).&lt;/em&gt; Tested with most of the test cases and failed the same ones as the conformance test because the features are not yet implemented. Actually I think there&amp;rsquo;s a bug in the test suite as it calls the PUBLISH handler function for non-PUBLISH messages.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;documentation:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Documentation&lt;/h3&gt;

&lt;p&gt;Documentation is available at &lt;a href=&#34;http://godoc.org/github.com/surge/surgemq&#34;&gt;godoc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;More information regarding the design of the SurgeMQ is available at &lt;a href=&#34;http://surgemq.com&#34;&gt;zen 3.1&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;license:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;License&lt;/h3&gt;

&lt;p&gt;Copyright &amp;copy; 2014 Dataence, LLC. All rights reserved.&lt;/p&gt;

&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &amp;ldquo;License&amp;rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;ldquo;AS IS&amp;rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.&lt;/p&gt;

&lt;h3 id=&#34;examples:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Examples&lt;/h3&gt;

&lt;h4 id=&#34;server-example:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Server Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;// Create a new server
svr := &amp;amp;service.Server{
    KeepAlive:        300,               // seconds
    ConnectTimeout:   2,                 // seconds
    SessionsProvider: &amp;quot;mem&amp;quot;,             // keeps sessions in memory
    Authenticator:    &amp;quot;mockSuccess&amp;quot;,     // always succeed
    TopicsProvider:   &amp;quot;mem&amp;quot;,             // keeps topic subscriptions in memory
}

// Listen and serve connections at localhost:1883
svr.ListenAndServe(&amp;quot;tcp://:1883&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;client-example:924dac401fc2f0061be94c47a8c89e4c&#34;&gt;Client Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;// Instantiates a new Client
c := &amp;amp;Client{}

// Creates a new MQTT CONNECT message and sets the proper parameters
msg := message.NewConnectMessage()
msg.SetWillQos(1)
msg.SetVersion(4)
msg.SetCleanSession(true)
msg.SetClientId([]byte(&amp;quot;surgemq&amp;quot;))
msg.SetKeepAlive(10)
msg.SetWillTopic([]byte(&amp;quot;will&amp;quot;))
msg.SetWillMessage([]byte(&amp;quot;send me home&amp;quot;))
msg.SetUsername([]byte(&amp;quot;surgemq&amp;quot;))
msg.SetPassword([]byte(&amp;quot;verysecret&amp;quot;))

// Connects to the remote server at 127.0.0.1 port 1883
c.Connect(&amp;quot;tcp://127.0.0.1:1883&amp;quot;, msg)

// Creates a new SUBSCRIBE message to subscribe to topic &amp;quot;abc&amp;quot;
submsg := message.NewSubscribeMessage()
submsg.AddTopic([]byte(&amp;quot;abc&amp;quot;), 0)

// Subscribes to the topic by sending the message. The first nil in the function
// call is a OnCompleteFunc that should handle the SUBACK message from the server.
// Nil means we are ignoring the SUBACK messages. The second nil should be a
// OnPublishFunc that handles any messages send to the client because of this
// subscription. Nil means we are ignoring any PUBLISH messages for this topic.
c.Subscribe(submsg, nil, nil)

// Creates a new PUBLISH message with the appropriate contents for publishing
pubmsg := message.NewPublishMessage()
pubmsg.SetPacketId(pktid)
pubmsg.SetTopic([]byte(&amp;quot;abc&amp;quot;))
pubmsg.SetPayload(make([]byte, 1024))
pubmsg.SetQoS(qos)

// Publishes to the server by sending the message
c.Publish(pubmsg, nil)

// Disconnects from the server
c.Disconnect()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>SurgeMQ: MQTT Message Queue @ 750,000 MPS</title>
      <link>http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/</link>
      <pubDate>Thu, 04 Dec 2014 22:44:07 -0800</pubDate>
      
      <guid>http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;Wow, this made front page of &lt;a href=&#34;https://news.ycombinator.com/item?id=8708921&#34;&gt;Hacker News&lt;/a&gt;! First for me!&lt;/li&gt;
&lt;li&gt;jacques_chester on HN has an &lt;a href=&#34;https://news.ycombinator.com/item?id=8709146&#34;&gt;EXCELLENT comment&lt;/a&gt; that&amp;rsquo;s definitely worth reading. &lt;a href=&#34;https://news.ycombinator.com/item?id=8709557&#34;&gt;My response&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;tl-dr:6ef28047216284b846a43eee6e7c23b5&#34;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt; aims to provide a MQTT broker and client library that&amp;rsquo;s fully compliant with &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT spec 3.1.1&lt;/a&gt;. In addition, it tries to be backward compatible with 3.1.&lt;/li&gt;
&lt;li&gt;SurgeMQ is under active development and should be considered unstable. Some of the key MQTT requirements, such as retained messages, still need to be added. The eventual goal is to pass the &lt;a href=&#34;https://eclipse.org/paho/clients/testing/&#34;&gt;MQTT Conformance/Interoperability Testing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Having said that, running all publishers, subscribers and broker on a single 4-core (2.8Ghz i7) MacBook Pro, SurgeMQ is able to achieve

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;over 400,000&lt;/strong&gt; MPS in a 1:1 single publisher and single producer configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;over 450,000&lt;/strong&gt; MPS in a 20:1 fan-in configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;over 750,000&lt;/strong&gt; MPS in a 1:20 fan-out configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;over 700,000&lt;/strong&gt; MPS in a full mesh configuration with 20 clients&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In developing SurgeMQ, I improved the performance 15-20X by keeping it simple and serial (KISS), reducing garbage collector pressure, reducing memory copy, and eliminating anything that could potentially introduce latency.&lt;/li&gt;
&lt;li&gt;There are still many areas that can be improved and I look forward to hearing any suggestions you may have.&lt;/li&gt;
&lt;li&gt;I cannot say this enough: &lt;strong&gt;benchmark, profile, optimize, rinse, repeat&lt;/strong&gt;. Go has made testing, benchmarking, and profiling extremely simple. You owe it to yourself to optimize your code using these tools.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 1: Don&amp;rsquo;t be clever. Keep It Simple and Serial (KISS).&lt;/p&gt;

&lt;p&gt;Lesson 2: Reduce or remove memory copying.&lt;/p&gt;

&lt;p&gt;Lesson 3: Race conditions can happen even if you think you followed all the right steps.&lt;/p&gt;

&lt;p&gt;Lesson 4: Use the race detector!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;go-learn-project-8-message-queue:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Go Learn Project #8 - Message Queue&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s now been over a year since my last post! Family and work have occupied pretty much all of my time so spare time to learn Go was hard to come by.&lt;/p&gt;

&lt;p&gt;However, I was able to squeeze in an implementation of a &lt;a href=&#34;https://github.com/surge/mqtt&#34;&gt;MQTT encoder/decoder&lt;/a&gt; library in July. The implementation is now outdated and is no longer maintained, but it allowed me to learn about the &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT protocol&lt;/a&gt; and got me thinking about potentially implmenting a broker.&lt;/p&gt;

&lt;p&gt;Now months later, I am finally able spend a few weekends and nights developing &lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt;, a (soon to be) full MQTT 3.1.1 compliant message broker.&lt;/p&gt;

&lt;h4 id=&#34;message-queues:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Message Queues&lt;/h4&gt;

&lt;p&gt;According to &lt;a href=&#34;http://en.wikipedia.org/wiki/Message_queue&#34;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Message queues provide an asynchronous communications protocol, meaning that the sender and receiver of the message do not need to interact with the message queue at the same time. Messages placed onto the queue are stored until the recipient retrieves them. Message queues have implicit or explicit limits on the size of data that may be transmitted in a single message and the number of messages that may remain outstanding on the queue.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tyler Treat of &lt;a href=&#34;http://www.bravenewgeek.com&#34;&gt;Brave New Geek&lt;/a&gt; also wrote a &lt;a href=&#34;http://www.bravenewgeek.com/tag/message-queues/&#34;&gt;good series on message queues&lt;/a&gt; that went over several of the key MQ implementations. One specific post, &lt;a href=&#34;http://www.bravenewgeek.com/dissecting-message-queues/&#34;&gt;Dissecting Message Queues&lt;/a&gt;, is especially interesting because it benchmarks some of the major message queue implmentations out there, both brokered and brokerless.&lt;/p&gt;

&lt;p&gt;In that post, Tyler found that borkerless queues had the highest throughput, achieving millions of MPS sent and received. Brokered message queue performances ranged from 12,000 MPS (&lt;a href=&#34;nsq.io&#34;&gt;NSQ&lt;/a&gt;) to 195,000 MPS (&lt;a href=&#34;nats.io&#34;&gt;Gnatsd&lt;/a&gt;). While the post showed that the Gnatsd latency to be around 300+ microseconds, in reality it&amp;rsquo;s probably more like the NSQ in terms of latency due to the sender sleeping whenever Gnatsd is 10+ messages behind. Regardless, hats off to Tyler. Great job!&lt;/p&gt;

&lt;h4 id=&#34;mqtt:6ef28047216284b846a43eee6e7c23b5&#34;&gt;MQTT&lt;/h4&gt;

&lt;p&gt;I got interested in MQTT because &amp;ldquo;&lt;a href=&#34;http://mqtt.org&#34;&gt;MQTT&lt;/a&gt; is a machine-to-machine (M2M)/&amp;ldquo;Internet of Things&amp;rdquo; connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;According to the &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT spec&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.&lt;/p&gt;

&lt;p&gt;The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications.&lt;/li&gt;
&lt;li&gt;A messaging transport that is agnostic to the content of the payload.&lt;/li&gt;
&lt;li&gt;Three qualities of service for message delivery:

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;At most once&amp;rdquo;, where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;At least once&amp;rdquo;, where messages are assured to arrive but duplicates can occur.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Exactly once&amp;rdquo;, where message are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A small transport overhead and protocol exchanges minimized to reduce network traffic.&lt;/li&gt;
&lt;li&gt;A mechanism to notify interested parties when an abnormal disconnection occurs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;There&amp;rsquo;s some very large implementation of MQTT such as &lt;a href=&#34;https://www.facebook.com/notes/facebook-engineering/building-facebook-messenger/10150259350998920&#34;&gt;Facebook Messenger&lt;/a&gt;. There&amp;rsquo;s also an active Eclipse project, &lt;a href=&#34;https://eclipse.org/paho/&#34;&gt;Paho&lt;/a&gt;, that provides scalable open-source client implementations for many different languages, including C/C++, Java, Python, JavaScript, C# .Net and Go.&lt;/p&gt;

&lt;p&gt;Given the popularity, I decided to implement a MQTT broker in order to learn about message queues.&lt;/p&gt;

&lt;h3 id=&#34;architecture:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/surgemq-mqtt-message-queue-750k-mps/smqfailedarch.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The above image showed a couple of the architecture approaches I attempted. In them, R is the receiver, which reads from net.Conn, P is the processor, which processes the messages and determines what to do or where to send them, and S is the sender, which sends any messages out to net.Conn. Each R, P, and S are their own goroutines.&lt;/p&gt;

&lt;p&gt;I started the project wanting to be clever, and wanted to dynamically scale up/down a shared pool of processors as the number of messages increase/decrease. As I thought through it, it just got more and more complicated with the logic and coordination. At the end, before I even wrote much of the code, I scraped the idea.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 1: Don&amp;rsquo;t be clever. Keep It Simple and Serial (KISS).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The second architecture approach I took is much simpler and probably much more idiomatic Go.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each connection has their own complete set of R, P and S, instead of sharing P across multiple connections.&lt;/li&gt;
&lt;li&gt;Each R, P and S are their own goroutines.&lt;/li&gt;
&lt;li&gt;Between R and P, and P and S are channels that carry MQTT messages.&lt;/li&gt;
&lt;li&gt;R was using bufio.Reader to read from net.Conn, and S was using bufio.Writer to write to net.Conn.&lt;/li&gt;
&lt;li&gt;sync.Pool was used to help reduce the amount of memory allocation required, thus reducing GC pressure&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach worked and I was able to write &lt;a href=&#34;https://github.com/surge/mqtt/commit/1eeba02bb5b7f624fc82a0ca975444944c1ec662&#34;&gt;enough code&lt;/a&gt; to test it. However, the performance was hideoous. In a 1:1 (single publisher and single subscriber) configuration, it was doing about 22,000-25,000 MPS.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go test -vv=3 -logtostderr -run=LotsOf -cpu=2 -v
=== RUN TestServiceLotsOfPublish0Messages-2
1000000 messages, 44297366818 ns, 44297.366818 ns/msg, 22574 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After profiling and looking at the &lt;a href=&#34;http://zhen.org/images/surgemq-mqtt-message-queue-750k-mps/2ndfailcpuprof.svg&#34;&gt;CPU profile&lt;/a&gt;, I realized there are a lot of memory copying (io.Copy and io.CopyN), as well as there are still quite a bit GC activities (scanblock). On the memory copying front, there&amp;rsquo;s copying from net.Conn into bufio, then more copying from bufio to the MQTT messages internal buffer, then more copying from MQTT message internal buffers to the outgoing bufio, then to the net.Conn. So lots and lots of memory copying, not a good thing.&lt;/p&gt;

&lt;h4 id=&#34;buffered-network-io:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Buffered Network IO&lt;/h4&gt;

&lt;p&gt;The buffered network IO is a good approach, however, there are two things I wished I had:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;bufio shifts bytes around by copying. For example, whenever it needs to fill the buffer, it copies all the remaining bytes to the front of the buffer, then fill the rest. That&amp;rsquo;s a lot of copying!&lt;/li&gt;
&lt;li&gt;I needed something I can access the bytes directly so I can remove majority of the memory copying.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At this point I decided to try a new technique I learned while doing Go Learn Project #7 - &lt;a href=&#34;http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/&#34;&gt;Ring Buffer&lt;/a&gt;. The basic idea is that instead of using bufio to read and write to net.Conn, I will implement my own version of that.&lt;/p&gt;

&lt;p&gt;The ring buffer will implement the interfaces ReadFrom(), WriteTo(), Read() and Write().&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The receiver will essentially copy data directly from net.Conn into the ring buffer (technially the ring buffer will ReadFrom() net.Conn and put the read bytes into the internal buffer).&lt;/li&gt;
&lt;li&gt;The processor can &amp;ldquo;peek&amp;rdquo; a byte slice (no copying) from the ring buffer, process it, and then commit the bytes once processing is done.&lt;/li&gt;
&lt;li&gt;If the message needs to be send to other subscribers, the bytes will then be copied into the subscriber&amp;rsquo;s outgoing buffer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While this is not &amp;ldquo;zero-copy&amp;rdquo;, it seems good enough.&lt;/p&gt;

&lt;p&gt;I started by implementing a lock-less ring buffer, and it worked quite well. But as mentioned in the ring buffer article, you really shouldn&amp;rsquo;t use it unless there&amp;rsquo;s plenty of CPU cores lying around. And also calling runtime.Gosched() thousands of times is really not healthy for the Go scheduler.&lt;/p&gt;

&lt;p&gt;So keeping Lesson 1 in mind, I modified the ring buffer to use two sync.Cond (reader sync.Cond and writer sync.Cond) to block (cond.Wait()) when there&amp;rsquo;s not enough bytes to read or when there&amp;rsquo;s not enough space to write. And then unblock (cond.Broadcast()) when bytes are either read from it, or written to it.&lt;/p&gt;

&lt;p&gt;This is a single producer/single consumer ring buffer and is not designed for multiples of anything. The original thought was that since each connection has their own set of R, P and S, there shouldn&amp;rsquo;t really be a need for multiple writers or readers. It turns out I was wrong, at least on the writer front. We will explain this a bit later.&lt;/p&gt;

&lt;p&gt;At the end, this turned out to be the winning combination. I was able to achieve 20X performance increase with this approach after some additional tweaking. Specifically, I tested several buffer block size (the amount of data to read from and write to net.Conn) including 1024, 2048, 4096 and 8192 bytes. The highest performing one is 8192 bytes.&lt;/p&gt;

&lt;p&gt;I also experiemented with different buffer sizes, including 256KB, 512KB and 1024KB. 256KB turned out to be sufficient in that it&amp;rsquo;s the smallest buffer size that doesn&amp;rsquo;t reduce performance by alot, nor higher numbers will help inprove performance.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 2: Reduce or remove memory copying.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;final-architecture:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Final Architecture&lt;/h4&gt;

&lt;p&gt;This the final architecture I ended up with and it&amp;rsquo;s working very well. The cost of each client connection are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3 goroutines: R (receiver), P (processor) and S (sender)&lt;/li&gt;
&lt;li&gt;2 ring buffers of 256K each&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There&amp;rsquo;s very few memory copy operations going on, nor is there much memory allocation. So a good outcome overall.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/surgemq-mqtt-message-queue-750k-mps/finalarch.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;race-conditions:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Race Conditions&lt;/h3&gt;

&lt;p&gt;With the ring bufer implementation, I was able to achieve 400,000 MPS with a 1:1 configuration. This worked well until I started doing multiple publishers and subscribers. The first problem I ran into was the Processor hanging. &lt;code&gt;go test -race&lt;/code&gt; also didn&amp;rsquo;t show anything that could help me.&lt;/p&gt;

&lt;p&gt;After running tests over and over again, with more and more glog.Debugf() statements, I tracked the problem to the Processor. It was waiting for space in the ring buffer to write the outgoing messages. I know that&amp;rsquo;s not possible as I am blasting messages out to net.Conn as fast as I can, so there&amp;rsquo;s no way that write space is not available.&lt;/p&gt;

&lt;p&gt;After running even more tests, and with even more glog.Debugf() statements, I finally determined the problem to be the way I was using sync.Cond. (I wish I saved the debug output..sigh) In the following code block, I was waiting for the consumer position (cpos) to pass the point in which there will be enough data for writing (wrap).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		this.pcond.L.Lock()
		for cpos = this.cseq.get(); wrap &amp;gt; cpos; cpos = this.cseq.get() {
			this.pcond.Wait()
		}
		this.pcond.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The steps are really quite simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I lock the producer sync.Conn&lt;/li&gt;
&lt;li&gt;I get the consumer position, compare it to wrap (position that I need cpos to pass to indicate there&amp;rsquo;s enough write space)&lt;/li&gt;
&lt;li&gt;If there&amp;rsquo;s not enough space, I wait, otherwise I move on&lt;/li&gt;
&lt;li&gt;I unlock the producer sync.Conn&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then in the Sender goroutine, I read data from the ring buffer, write to net.Conn, update the consumer position, and call &lt;code&gt;pcond.Broadcast()&lt;/code&gt; to unblock the above &lt;code&gt;pcond.Wait()&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		this.cseq.set(cpos + int64(n))
		this.pcond.Broadcast()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;According to the &lt;a href=&#34;http://golang.org/pkg/sync/#Cond.Broadcast&#34;&gt;Go doc&lt;/a&gt;,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Broadcast wakes all goroutines waiting on c.&lt;/p&gt;

&lt;p&gt;It is allowed but not required for the caller to hold c.L during the call.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So what I have above should work perfectly fine. Except it doesn&amp;rsquo;t. What happens is that I ran into a situation where &lt;code&gt;pcond.Broadcast()&lt;/code&gt; was called after the &lt;code&gt;wrap &amp;gt; cpos&lt;/code&gt; check, but before &lt;code&gt;pcond.Wait()&lt;/code&gt;. In these cases, the &lt;code&gt;wrap &amp;gt; cpos&lt;/code&gt; returned true, which means we need to go wait. But before &lt;code&gt;pcond.Wait()&lt;/code&gt; was called, the Sender goroutine has updated cpos, and called &lt;code&gt;pcond.Broadcast()&lt;/code&gt;. So when &lt;code&gt;pcond.Wait()&lt;/code&gt; is called, there&amp;rsquo;s nothing to wake it up, and thus it hangs forever.&lt;/p&gt;

&lt;p&gt;On the Sender side, because there&amp;rsquo;s no more data to read, it is also just waiting for more data. So both the Sender and Processor are now hung.&lt;/p&gt;

&lt;p&gt;After I finally figured out the root cause, I realized that, unlike what the go doc suggested, the caller should really hold c.L during the call to Broadcast(). So I modified the code to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		this.cseq.set(cpos + int64(n))
		this.pcond.L.Lock()
		this.pcond.Broadcast()
		this.pcond.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What this does is that it ensures I can never call &lt;code&gt;pcond.Broadcast()&lt;/code&gt; after &lt;code&gt;pcond.L.Lock()&lt;/code&gt; (in the Processor goroutine) is called but &lt;code&gt;pcond.Wait()&lt;/code&gt; is not called. When &lt;code&gt;pcond.Wait()&lt;/code&gt; is called, it actually calls &lt;code&gt;pcond.L.Unlock()&lt;/code&gt; internally so it will allow &lt;code&gt;pcond.L.Lock()&lt;/code&gt; in the Sender goroutine to be called.&lt;/p&gt;

&lt;p&gt;In any case, we are finally on our way to working with multiple clients.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 3: Race conditions can happen even if you think you followed all the right steps.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;but-wait-there-s-more-race-conditions:6ef28047216284b846a43eee6e7c23b5&#34;&gt;But Wait, There&amp;rsquo;s More (Race Conditions)&lt;/h4&gt;

&lt;p&gt;As I increase the number of publishers and subscribers, all the sudden I was getting errors about receiving RESERVED messages, and this happens intermittenly, and only when I blast enough messages. Sometimes I have to run the tests many times to catch this from happening.&lt;/p&gt;

&lt;p&gt;It turns out that while I was thinking I only had 1 Publisher per client connection that&amp;rsquo;s writing to the outgoing buffer, I, in fact, had many. This happens when a client is sent a message to a topic that it subscribes to. In this case, the Processor of the publishing client calls the subscriber client&amp;rsquo;s Publish() method, and writes the message to the outgoing ring buffer. At the same time, other publishing clients can be publishing other messages to the subscriber client. When this happens, they could overwrite eachother&amp;rsquo;s message because the ring buffer is NOT designed for multiple writers.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;go test -race&lt;/code&gt; should technically find this race condition (I think). But given that this condition only happens intermittenly and sometimes it only happens when there&amp;rsquo;s a large volume of messages, the race detector was taking too long and I was too impatient.&lt;/p&gt;

&lt;p&gt;Regardless, after identifying the root cause, I added a Mutex to serialize the writes. At some point I may come back and rewrite it without the lock. But for now it&amp;rsquo;s good enough.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 4: Use the race detector!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;performance-benchmarks:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Performance Benchmarks&lt;/h3&gt;

&lt;p&gt;These performance numbers are calculated as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sent messages MPS = total messages sent / total elapsed time between 1st and last message sent for all senders&lt;/li&gt;
&lt;li&gt;received messages MPS = total messages received / total elapsed time between 1st and last message received for all receivers&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;environment:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Environment&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ go version
go version go1.3.3 darwin/amd64

---

Macbook Pro Late 2013
2.8 GHz Intel Core i7
16 GB 1600 MHz DDR3
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;server:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Server&lt;/h4&gt;

&lt;p&gt;To start the server,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd benchmark
$ GOMAXPROCS=2 go test -run=TestServer -vv=2 -logtostderr
server/ListenAndServe: server is ready...
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;1-1:6ef28047216284b846a43eee6e7c23b5&#34;&gt;1:1&lt;/h4&gt;

&lt;p&gt;To run the single publisher and single subscriber test case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 1 -receivers 1
Total Sent 1000000 messages in 2434523153 ns, 2434 ns/msg, 410758 msgs/sec
Total Received 1000000 messages in 2434523153 ns, 2434 ns/msg, 410758 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;fan-in:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Fan-In&lt;/h4&gt;

&lt;p&gt;To run the Fan-In test with 20 senders and 1 receiver:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 20 -receivers 1
Total Sent 1035436 messages in 2212609304 ns, 2136 ns/msg, 467970 msgs/sec
Total Received 1000022 messages in 2212609304 ns, 2212 ns/msg, 451965 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;fan-out:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Fan-Out&lt;/h4&gt;

&lt;p&gt;To run the Fan-Out test with 1 sender and 20 receivers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 1 -receivers 20
Total Sent 1000000 messages in 10715317340 ns, 10715 ns/msg, 93324 msgs/sec
Total Received 8180723 messages in 10715317340 ns, 1309 ns/msg, 763460 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;mesh:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Mesh&lt;/h4&gt;

&lt;p&gt;To run a full mesh test where every client is subscribed to the same topic, thus every message sent w/ the right topic will go to ALL of the other clients:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestMesh -vv=2 -logtostderr -senders 20 -messages 100000
Total Sent 2000000 messages in 51385336097 ns, 25692 ns/msg, 38921 msgs/sec
Total Received 40000000 messages in 51420975243 ns, 1285 ns/msg, 777892 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;next-steps:6ef28047216284b846a43eee6e7c23b5&#34;&gt;Next Steps&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s a lot more to do with SurgeMQ. Given the limited time I have, I expect it will take me a while to get to full compliant with the MQTT spec. But that will be my focus, now that performance is out of the way, as I get time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graceful Shutdown of Go net.Listeners</title>
      <link>http://zhen.org/blog/graceful-shutdown-of-go-net-dot-listeners/</link>
      <pubDate>Thu, 12 Dec 2013 23:33:22 -0800</pubDate>
      
      <guid>http://zhen.org/blog/graceful-shutdown-of-go-net-dot-listeners/</guid>
      <description>&lt;p&gt;Comments/Feedbacks at &lt;a href=&#34;https://news.ycombinator.com/item?id=6899568&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1ss929/graceful_shutdown_of_go_netlisteners/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The past few evenings I&amp;rsquo;ve been working on a Go program that listens on a TCP socket, accepts new connections, processes some data and then return some data to the client. This is actually fairly simple in Go. The &lt;a href=&#34;http://golang.org/pkg/net&#34;&gt;top&lt;/a&gt; of the &lt;em&gt;net&lt;/em&gt; package has some fairly simple examples of how to do that. Another example can be found about &lt;a href=&#34;http://golang.org/pkg/net/#example_Listener&#34;&gt;mid-section&lt;/a&gt; of the page. You can also find a TON of examples online that shows you how to build a simple TCP listener.&lt;/p&gt;

&lt;p&gt;However, one thing I noticed in all these examples is that none of them shows you how to gracefully shutdown the TCP listener. Most of the examples expect to program to exit so there&amp;rsquo;s no need to clean up anything. However, if you have a program that&amp;rsquo;s a long-running server, and need to, for whatever reason, need to shutdown the TCP listener, you will need to clean up after yourself. Otherwise you may leave a bunch of goroutines behind unintentionally.&lt;/p&gt;

&lt;p&gt;Another reason I wanted a way to shutdown the TCP listener is I want to be able to start a listener in my tests, then start up a bunch of clients, test some stuff, then afterwards shutdown the server. Then I can start another listener in another test for some other tests.&lt;/p&gt;

&lt;p&gt;After some help from jhoto and foobaz on the #go-nuts IRC channel, I wrote the following example to demonstrate the graceful shutdown approach.&lt;/p&gt;

&lt;p&gt;The basic idea is to leverage a quit channel to tell the Accept() goroutine that it&amp;rsquo;s time to quit. Using quit channel is a fairly common practice in Go. However, in this case, the Accept() call is blocking waiting for new connections, so closing the quit channel won&amp;rsquo;t have any effect unless the goroutine actually checks it. So to force Accept() to return from blocking, we can close the net.Listener.&lt;/p&gt;

&lt;p&gt;The order of the operation matters somewhat. We will want to first close the quit channel, then close the net.Listener. If we reverse the order, you will likely see a few more errors from the Accept() call.&lt;/p&gt;

&lt;p&gt;The netgrace_test.go file below shows an example of how to use the quit channel to help gracefully shutdown net.Listeners.&lt;/p&gt;

&lt;p&gt;Hopefully you will find this tip useful. You can find it as a &lt;a href=&#34;https://gist.github.com/zhenjl/7940977&#34;&gt;gist&lt;/a&gt; as well.&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/zhenjl/7940977.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Comments/Feedbacks at &lt;a href=&#34;https://news.ycombinator.com/item?id=6899568&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1ss929/graceful_shutdown_of_go_netlisteners/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ring Buffer - Variable-Length, Low-Latency, Lock-Free, Disruptor-Style</title>
      <link>http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/</link>
      <pubDate>Sat, 30 Nov 2013 23:20:22 -0800</pubDate>
      
      <guid>http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/</guid>
      <description>

&lt;p&gt;Comments/feedbacks on &lt;a href=&#34;https://news.ycombinator.com/item?id=6831293&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1rvvb6/ring_buffer_variablelength_lowlatency_lockfree/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2013-12-04 Update #1: Read a &lt;a href=&#34;https://groups.google.com/forum/#!topic/golang-nuts/7tUShPuPfNM&#34;&gt;very interesting thread on golang-nuts list&lt;/a&gt; on the performance of interface. Seems like using interfaces really affects performance. In Joshua&amp;rsquo;s test he saw a 3-4x performance difference. I decided to try this on the ring buffer implementation since I am currently using interface. A quick test laster, looks like NOT using interface increased performance 2.4x. For now the code is in the &amp;ldquo;&lt;a href=&#34;https://github.com/reducedb/ringbuffer/tree/nointerface&#34;&gt;nointerface&lt;/a&gt;&amp;rdquo; branch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;Benchmark1ProducerAnd1Consumer-3         5000000               353 ns/op
Benchmark1ProducerAnd1ConsumerInBytes-3 10000000               147 ns/op
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;tl-dr:bdc146ca9cda0984f90718c5048177bd&#34;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/reducedb/ringbuffer&#34;&gt;This project&lt;/a&gt; implements a low-latency lock-free ring buffer for variable length byte slices. It is modeled after the &lt;a href=&#34;https://github.com/LMAX-Exchange/disruptor/&#34;&gt;LMAX Disruptor&lt;/a&gt;, but not a direct port.&lt;/li&gt;
&lt;li&gt;If you have only a single core, don&amp;rsquo;t use this. Use Go channels instead! In fact, unless you can spare as many cores as the number of producers and consumers, don&amp;rsquo;t use this ring buffer.&lt;/li&gt;
&lt;li&gt;In fact, for MOST use cases, Go channel is a better approach. This ring buffer is really a specialized solution for very specific use cases.&lt;/li&gt;
&lt;li&gt;Primary pattern of this ring buffer is single producer and multiple consumer, where the single producer put bytes into the buffer, and each consumer will process ALL of the items in the buffer. (Other patterns can be implemented later but this is what&amp;rsquo;s here now.)&lt;/li&gt;
&lt;li&gt;This ring buffer is designed to deal with situations where we don&amp;rsquo;t know the length of the byte slice before hand. It will write the byte slice to the buffer across multiple slots in the ring if necessary.&lt;/li&gt;
&lt;li&gt;The ring buffer currently employs a lock-free busy-wait strategy, where the producer and consumers will continue to loop until data is available. As such, it performs very well in a multi-core environment (almost twice as fast as Go channels) if you can spare 1 core per produer/consumer, but extremely poorly in a single-core environment (600 times worse compare to Go channels).&lt;/li&gt;
&lt;li&gt;You can find a lot of information on the LMAX Disruptor. The resources I used include &lt;a href=&#34;http://mechanitis.blogspot.com/search/label/disruptor&#34;&gt;Trisha&amp;rsquo;s Disruptor blog series&lt;/a&gt;, &lt;a href=&#34;http://lmax-exchange.github.io/disruptor/&#34;&gt;LMAX Disruptor main page&lt;/a&gt;, &lt;a href=&#34;http://lmax-exchange.github.com/disruptor/files/Disruptor-1.0.pdf&#34;&gt;Disruptor technical whitepaper&lt;/a&gt;, and Martin Fowler&amp;rsquo;s &lt;a href=&#34;http://martinfowler.com/articles/lmax.html&#34;&gt;LMAX Architecture article&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You can also read more about &lt;a href=&#34;http://en.wikipedia.org/wiki/Circular_buffer&#34;&gt;circular buffer&lt;/a&gt;, &lt;a href=&#34;http://en.wikipedia.org/wiki/Producer-consumer_problem&#34;&gt;producer–consumer problem&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;go-learn-project-7-ring-buffer:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Go Learn Project #7 - Ring Buffer&lt;/h3&gt;

&lt;p&gt;For the past several projects (&lt;a href=&#34;http://zhen.org/blog/benchmarking-integer-compression-in-go/&#34;&gt;#6&lt;/a&gt;, &lt;a href=&#34;http://zhen.org/blog/bitmap-compression-using-ewah-in-go/&#34;&gt;#5&lt;/a&gt;, &lt;a href=&#34;http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/&#34;&gt;#4&lt;/a&gt;), I&amp;rsquo;ve mostly been hacking bits, and optimizing them as much as possible for a single core.&lt;/p&gt;

&lt;p&gt;For project #7, I decided to do something slightly different. This time we will create a ring buffer that can support variable length byte slices, and leverage multi-cores using multiple goroutines.&lt;/p&gt;

&lt;p&gt;Primary target use case is for a producer to read bytes from sockets, ZMQ, files, etc that require process, like JSON, csv, and tsv strings, at a very high speed, such as millions of lines per second, and put these lines into a buffer so other consumers can process these.&lt;/p&gt;

&lt;p&gt;A good example is when we import files with millions of lines of JSON object. These JSON objects are read from files, inserted into the buffer, and then they are unmarshalled into other structures.&lt;/p&gt;

&lt;p&gt;The goal is to process millions of data items per second.&lt;/p&gt;

&lt;h4 id=&#34;ring-buffer:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Ring Buffer&lt;/h4&gt;

&lt;p&gt;There are several ways to tackle this. A queue or ring buffer is usually a good data structure for this. You can think of a ring buffer is just a special type of queue that&amp;rsquo;s just contiguous by wrapping itself. As Wikipedia said,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A circular buffer, cyclic buffer or ring buffer is a data structure that uses a single, fixed-size buffer as if it were connected end-to-end. This structure lends itself easily to buffering data streams.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, implementation-wise, we can tackle this problem using a standard ring buffer. The most common way of implementing a standard ring buffer is to keep track of a head and a tail pointer, and keep putting items into the buffer but ensuring that head and tail don&amp;rsquo;t cross each other.&lt;/p&gt;

&lt;p&gt;In most implementations, the pointers are mod&amp;rsquo;ed with the ring size to determine the next slot size. Also, mutex is used to ensure only one thread is modifying these pointers at the same time.&lt;/p&gt;

&lt;h4 id=&#34;go-channels:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Go Channels&lt;/h4&gt;

&lt;p&gt;In Go, an idiomatic and fairly common way is to use &lt;a href=&#34;http://golang.org/doc/effective_go.html#channels&#34;&gt;buffered channels&lt;/a&gt; to solve this problem. It is effectively a queue where a producer puts data items into one end of the channel, and the consumer reads the data items on the other end of the channel. There are certainly pros and cons to to this approach.&lt;/p&gt;

&lt;p&gt;First, it is Go idiomatic! Need I say more&amp;hellip; :)&lt;/p&gt;

&lt;p&gt;This is probably the easiest approach since Go Channel is already a battle-tested data structure and it&amp;rsquo;s readily available. An example is provided below in the examples section. Performance-wise it is actually not too bad. In a multi-core environment, it&amp;rsquo;s about 60% of the performance compare to the ring buffer. However, in a single-core environment, it is MUCH faster. In fact, 1300 times faster than my ring buffer!&lt;/p&gt;

&lt;p&gt;There is one major difference between using channels vs this ring buffer. When the channel has multiple consumers, the data is multiplexed to the consumers. So each consumer will get only part of the data rather than going through all the data. This is illustrated by &lt;a href=&#34;http://play.golang.org/p/ACC5LIohIe&#34;&gt;this play&lt;/a&gt;. The current design of the ring buffer allows multiple consumers to go through every item in the queue. A obvious workaround is to send the data items to multiple channels.&lt;/p&gt;

&lt;p&gt;One down side with my channel approach is that I end up creating a lot of garbage over time and will need to be GC&amp;rsquo;ed. Specifically, I am creating a new byte slice for each new data item. Again, there is workaround for this. One can implement a &lt;a href=&#34;http://golang.org/doc/effective_go.html#leaky_buffer&#34;&gt;leaky buffer&lt;/a&gt;. However, because we don&amp;rsquo;t know how big the data items are before hand, it&amp;rsquo;s more difficult to preallocate the buffers up front.&lt;/p&gt;

&lt;p&gt;There might actually be a way to implement the leaky buffer with a big preallocated slice. I may just do that as the next project. The goal is to see if we can avoid having to allocate individual byte slices and leverage CPU caching for the big buffer.&lt;/p&gt;

&lt;h4 id=&#34;lock-free-ring-buffer:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Lock-Free Ring Buffer&lt;/h4&gt;

&lt;p&gt;The way that I&amp;rsquo;ve decided to tackle this problem is to model the ring buffer after the LMAX Disruptor. If you haven&amp;rsquo;t read Martin Fowler&amp;rsquo;s &lt;a href=&#34;http://martinfowler.com/articles/lmax.html&#34;&gt;article on LMAX Architecture&lt;/a&gt;, at this time I would recommend that you stop and go read it first. After that, you should go read &lt;a href=&#34;http://mechanitis.blogspot.com/search/label/disruptor&#34;&gt;Trisha&amp;rsquo;s Disruptor blog series&lt;/a&gt; that explains in even more details how the Disruptor works.&lt;/p&gt;

&lt;p&gt;One thing to keep in mind is that the Disruptor-style ring buffer has significant resource requirement, i.e., it requires N cores, where N is the number of producers and consumers, to be performant. And it will keep cores busy by busy waiting (looping). So huge downside. If you don&amp;rsquo;t need this type of low latency architecture, it&amp;rsquo;s much better to stay with channels.&lt;/p&gt;

&lt;h3 id=&#34;performance-comparison:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Performance Comparison&lt;/h3&gt;

&lt;p&gt;These benchmarks are performed on a MacBook Pro with 2.8GHz Intel Core i7 procesor (Haswell), and 16 GB 1600MHz DDR3 memory. Go version 1.2rc5.&lt;/p&gt;

&lt;p&gt;The 2 channel consumers benchmark is a single producer sending to 2 channels, each channel consumed by a separate goroutine. So it is apples-to-apples compare to the ring buffer 2 consumers benchmark.&lt;/p&gt;

&lt;p&gt;You can see clearly the requirement of 1 core per consumer/producer in the ring buffer implementation (first 6 lines). Without that, performance suffer greately!&lt;/p&gt;

&lt;p&gt;Also notice that the channel benchmark (last 6 lines) is faster for a single core than multi-cores. This is probably due to your friendly cache at play.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go test -bench=. -run=xxx -cpu=1,2,3
PASS
Benchmark1ProducerAnd1Consumer     10000            339807 ns/op
Benchmark1ProducerAnd1Consumer-2        10000000               258 ns/op
Benchmark1ProducerAnd1Consumer-3        10000000               260 ns/op
Benchmark1ProducerAnd2Consumers    10000            341859 ns/op
Benchmark1ProducerAnd2Consumers-2         200000             14967 ns/op
Benchmark1ProducerAnd2Consumers-3        5000000               340 ns/op
BenchmarkChannels1Consumer      10000000               241 ns/op
BenchmarkChannels1Consumer-2     5000000               436 ns/op
BenchmarkChannels1Consumer-3     5000000               446 ns/op
BenchmarkChannels2Consumers      5000000               319 ns/op
BenchmarkChannels2Consumers-2    5000000               647 ns/op
BenchmarkChannels2Consumers-3    5000000               570 ns/op
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;examples:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Examples&lt;/h3&gt;

&lt;h4 id=&#34;go-channel-1-producer-and-1-consumer:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Go Channel: 1 Producer and 1 Consumer&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;func BenchmarkChannels(b *testing.B) {
	dataSize := 256
	data := make([]byte, dataSize)
	for i := 0; i &amp;lt; dataSize; i++ {
		data[i] = byte(i % 256)
	}

	ch := make(chan []byte, 128)
	go func() {
		for i := 0; i &amp;lt; b.N; i++ {
			// To be fair, we want to make a copy of the data, otherwise we are just
			// sending the same slice header over and over. In the real-world, the
			// original data slice may get over-written by the next set of bytes.
			tmp := make([]byte, dataSize)
			copy(tmp, data)
			ch &amp;lt;- tmp
		}
	}()

	for i := 0; i &amp;lt; b.N; i++ {
		out := &amp;lt;-ch
		if !bytes.Equal(out, data) {
			b.Fatalf(&amp;quot;bytes not the same&amp;quot;)
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ring-buffer-1-producer-and-1-consumer:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Ring Buffer: 1 Producer and 1 Consumer&lt;/h4&gt;

&lt;p&gt;This test function creates a 256-slot ring buffer, with each slot being 128 bytes long. It also creates 1 producer and 1 consumer, where the producer will put the same byte slice into the buffer 10,000 times, and the consumer will read from the buffer and then make sure we read the correct byte slice.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Test1ProducerAnd1ConsumerAgain(t *testing.T) {
	// Creates a new ring buffer that&#39;s 256 slots and each slot 128 bytes long.
	r, err := New(128, 256)
	if err != nil {
		t.Fatal(err)
	}

	// Gets a single producer from the the ring buffer. If NewProducer() is called
	// the second time, an error will be returned.
	p, err := r.NewProducer()
	if err != nil {
		t.Fatal(err)
	}

	// Gets a singel consumer from the ring buffer. You can call NewConsumer() multiple
	// times and get back a new consumer each time. The consumers are independent and will
	// go through the ring buffers separately. In other words, each consumer will have 
	// their own independent sequence tracker.
	c, err := r.NewConsumer()
	if err != nil {
		t.Fatal(err)
	}

	// We are going to write 10,000 items into the buffer.
	var count int64 = 10000

	// Let&#39;s prepare the data to write. It&#39;s just a basic byte slice that&#39;s 256 bytes long.
	dataSize := 256
	data := make([]byte, dataSize)
	for i := 0; i &amp;lt; dataSize; i++ {
		data[i] = byte(i % 256)
	}

	// Producer goroutine
	go func() {
		// Producer will put the same data slice into the buffer _count_ times
		for i := int64(0); i &amp;lt; count; i++ {
			if _, err := p.Put(data); err != nil {
				// Unfortuantely we have an issue here. If the producer gets an error 
				// and exits, the consumer will continue to wait and not exit. In the
				// real-world, we need to notify all the consumers that there&#39;s been
				// an error and ensure they exit as well.
				t.Fatal(err)
			}
		}
	}()

	var total int64

	// Consumer goroutine
	
	// The consumer will also read from the buffer _count_ times
	for i := int64(0); i &amp;lt; count; i++ {
		if out, err := c.Get(); err != nil {
			t.Fatal(err)
		} else {
			// Check to see if the byte slice we got is the same as the original data
			if !bytes.Equal(out.([]byte), data) {
				t.Fatalf(&amp;quot;bytes not the same&amp;quot;)
			}

			total++
		}
	}

	// Check to make sure the count matches
	if total != count {
		t.Fatalf(&amp;quot;Expected to have read %d items, got %d\n&amp;quot;, count, total)
	}
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ring-buffer-1-producer-and-2-consumers:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Ring Buffer: 1 Producer and 2 Consumers&lt;/h4&gt;

&lt;p&gt;As mentioned before, the ring buffer supports multiple consumers. &lt;a href=&#34;https://github.com/reducedb/ringbuffer/blob/master/bytebuffer/ringbuffer_test.go#L304&#34;&gt;This example&lt;/a&gt; shows how you would create two consumers.&lt;/p&gt;

&lt;h3 id=&#34;conclusion:bdc146ca9cda0984f90718c5048177bd&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;See tl;dr on top.&lt;/p&gt;

&lt;p&gt;Comments/feedbacks on &lt;a href=&#34;https://news.ycombinator.com/item?id=6831293&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1rvvb6/ring_buffer_variablelength_lowlatency_lockfree/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go vs Java: Decoding Billions of Integers Per Second</title>
      <link>http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/</link>
      <pubDate>Thu, 14 Nov 2013 19:40:22 -0800</pubDate>
      
      <guid>http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/</guid>
      <description>

&lt;p&gt;Comments/feedbacks on &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qquqz/go_vs_java_decoding_billions_of_integers_per/&#34;&gt;reddit&lt;/a&gt;, &lt;a href=&#34;https://news.ycombinator.com/item?id=6743821&#34;&gt;hacker news&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2013-11-17 Update #2: Tried another new trick. This time I updated the leading bit position function, when using the gccgo compiler, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/bitlen_gccgo.go&#34;&gt;to use libgcc&amp;rsquo;s &lt;code&gt;__clzdi2&lt;/code&gt; routine&lt;/a&gt;. This had the same effect as the update #1 except it&amp;rsquo;s for when gccgo is used. Performance increase ranged from 0% to 20% for encoding only. Thanks dgryski on reddit and minux on the golang-nuts mailing list.&lt;/p&gt;

&lt;p&gt;2013-11-16 Update #1: Tried a new trick, which is to use an &lt;a href=&#34;https://github.com/reducedb/encoding/commit/ea080c479fb4994e400ebba021d13f10c4f3fecc&#34;&gt;assembly version of bitlen&lt;/a&gt; to calculate the leading bit position. See the section below on &amp;ldquo;Use Some Assembly&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, before you crucify me for benchmarking Go vs Java, let me just say that I am not trying to discredit Go. I like Go and will use it for more projects. I am simply trying to show the results as objectively as I can, and hope that the community can help me improve my skills as well as the performance of the libraries.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t consider myself a Go expert. I&amp;rsquo;ve been using Go for a few months and have been documenting the projects as I go in this blog. I&amp;rsquo;ve learned a ton and have applied many of the things I learned in optimizing this project. However, I cannot claim that I have done everything possible, so the performance numbers &amp;ldquo;could&amp;rdquo; still be better.&lt;/p&gt;

&lt;p&gt;I would like to ask for your help, if you can spare the time, to share some of your optimization tips and secrets. I would love to make this library even faster if possible.&lt;/p&gt;

&lt;h3 id=&#34;tl-dr:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;tl;dr&lt;/h3&gt;

&lt;p&gt;The following chart shows how much (%) faster Java is compare to Go in decoding integers that are encoded using different codecs. It shows results from processing two different files. See below for more details.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_faster.png&#34;&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Daniel Lemire&amp;rsquo;s &lt;a href=&#34;https://github.com/lemire/JavaFastPFOR&#34;&gt;Java version&lt;/a&gt; is anywhere from 12% to 180% faster than my &lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;Go version&lt;/a&gt; for decoding integers. I didn&amp;rsquo;t compare the &lt;a href=&#34;https://github.com/lemire/fastpfor&#34;&gt;C++ version&lt;/a&gt; but given that the C++ version has access to SIMD operations, it can be much faster.&lt;/li&gt;
&lt;li&gt;I tried many different ways to optimize my Go code for this projects, including using range for looping through slices, inlining simple functions, unrolling simple loops, unrolling even more loops, disabling bound checking (not generally recommended), using &lt;em&gt;gccgo&lt;/em&gt; to compile, and used some assembly.&lt;/li&gt;
&lt;li&gt;Using &lt;em&gt;gccgo -O3&lt;/em&gt; resulted in the highest performance. I tested using standard gc compiler, gc -B, gccgo, and gccgo -O3. The comparison above uses the &lt;em&gt;gccgo -O3&lt;/em&gt; numbers.&lt;/li&gt;
&lt;li&gt;Using a range loop instead of unrolling a loop in one of the often used functions, AND compiling using &lt;em&gt;gccgo -O3&lt;/em&gt;, I was able to get within 6% of Java version for Delta BP32 decoding. However, all of the other Go binaries suffered greatly.&lt;/li&gt;
&lt;li&gt;This benchmark is purely a CPU benchmark. The test environment has enough memory to keep all the arrays in memory without causing swap, and there&amp;rsquo;s no disk IO involved in the actual encoding/decoding functions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;project-review:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Project Review&lt;/h3&gt;

&lt;p&gt;A month ago I wrote about my &amp;ldquo;Go Learn&amp;rdquo; project #6: &lt;a href=&#34;http://zhen.org/blog/benchmarking-integer-compression-in-go/&#34;&gt;Benchmarking Integer Compression in Go&lt;/a&gt; &lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;(github)&lt;/a&gt;. In that project I ported 6 different codecs for encoding and decoding 32-bit integers. Since then, I have ported a couple more codecs, cleaned up the directories, and performed a ton of profiling and optimization to increase performance.&lt;/p&gt;

&lt;p&gt;There are now a total of 8 codecs available:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Binary Packing (BP32), FastPFOR, Variable Byte (varint) (top level directories)

&lt;ul&gt;
&lt;li&gt;Standard codec that encodes/decodes the integers as they are&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Delta BP32, Delta FastPFOR (&lt;strong&gt;new&lt;/strong&gt;), Delta Variable Byte (under &lt;em&gt;delta/&lt;/em&gt;)

&lt;ul&gt;
&lt;li&gt;Encodes/decodes the deltas of the integers&lt;/li&gt;
&lt;li&gt;These codecs generally produce much more compact representations if the integers are sorted&lt;/li&gt;
&lt;li&gt;These codecs generally perform much faster, but there are some exceptions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ZigZag BP32, ZigZag FastPFOR (&lt;strong&gt;new&lt;/strong&gt;) (under &lt;em&gt;zigzag/&lt;/em&gt;)

&lt;ul&gt;
&lt;li&gt;Encode/decodes the deltas of the integers, where the deltas themselves are encoded using Google&amp;rsquo;s zigzag encoding&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, the &lt;em&gt;benchmark&lt;/em&gt; program under &lt;em&gt;benchmark/&lt;/em&gt; is provided to let users easily test different integer lists and codecs.&lt;/p&gt;

&lt;h3 id=&#34;techniques-tried:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Techniques Tried&lt;/h3&gt;

&lt;p&gt;I cannot say this enough: &lt;strong&gt;benchmark, profile, optimize, rinse, repeat&lt;/strong&gt;. Go has made testing, benchmarking, and profiling extremely simple. You owe it to yourself to optimize your code using these tools. Previously I have written about how I was able to &lt;a href=&#34;http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/&#34;&gt;improve the cityhash Go implementation performance by 3-16X by Go profiling&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To optimize the integer encoding library, I followed the same techniques to profile each codec, and try as much as I can to optimzie the hot spots.&lt;/p&gt;

&lt;p&gt;Below are some of the optimizations I&amp;rsquo;ve tried. Some helped, some didn&amp;rsquo;t.&lt;/p&gt;

&lt;h4 id=&#34;for-range-through-slices:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;For-Range Through Slices&lt;/h4&gt;

&lt;p&gt;I learned this when Ian Taylor from Google (and others) helped me optimize one of the functions using range to loop through the slices instead of &lt;code&gt;for i := 0; i &amp;lt; b; i++ {}&lt;/code&gt; loops. The for-range method can be 4-7 times faster than the other way. You can see the difference between BenchmarkOffset and BenchmarkRange &lt;a href=&#34;https://gist.github.com/zhenjl/7495442&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also found that &lt;em&gt;gccgo -O3&lt;/em&gt; can do some really good optimizations with simple range loops. You can see the difference with this &lt;a href=&#34;https://gist.github.com/zhenjl/7495442&#34;&gt;gist&lt;/a&gt;. When using &lt;em&gt;gc&lt;/em&gt; the standard Go compiler, BenchmarkRange (31.3 ns/op) is 56% slower than BenchmarkUnrolled (13.9 ns/op). However, then reverse is true when using &lt;em&gt;gccgo -O3&lt;/em&gt;. BenchmarkUnrolled (8.92 ns/op) is 100% slower than BenchmarkRange (4.46 ns/op).&lt;/p&gt;

&lt;p&gt;Side note: this set of benchmarks are courtesy of DisposaBoy and Athiwat in the #go-nuts IRC channel. Thanks for your help guys.&lt;/p&gt;

&lt;h4 id=&#34;unroll-simple-loops:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Unroll Simple Loops&lt;/h4&gt;

&lt;p&gt;For some simple, known-size, loops, such as initializing a slice with the same initial non-zero value, unrolling the loop makes a big difference. The caveat is that &lt;em&gt;gccgo -O3&lt;/em&gt; does an amazing job of optimizing these simple range loops, so in that case unrolling the loop is actually slower.&lt;/p&gt;

&lt;p&gt;As an example, the &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/bitpacking/delta_bitpacking.go#L242&#34;&gt;following function&lt;/a&gt; is unrolled as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func deltaunpack0(initoffset int32, in []int32, inpos int, out []int32, outpos int) {
    out[outpos+0] = initoffset
    out[outpos+1] = initoffset
    out[outpos+2] = initoffset
    .
    .
    .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It was originally written as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp := out[outpos:outpos+32]
for i, _ := range tmp {
    tmp[i] = initoffset
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When unrolled, AND using the standard &lt;em&gt;gc&lt;/em&gt; compiler, we saw performance increase by almost 45% if this function is called often. However, as we mentioned above, when using &lt;em&gt;gccgo -O3&lt;/em&gt;, the for-range loop is 33% faster than the unrolled method.&lt;/p&gt;

&lt;p&gt;For now, I am keeping the unrolled version of the function.&lt;/p&gt;

&lt;h4 id=&#34;unroll-even-more-loops:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Unroll Even More Loops&lt;/h4&gt;

&lt;p&gt;Given the success of unrolling the above simple loop, I thought I try unrolling &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L150&#34;&gt;even&lt;/a&gt; . &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L281&#34;&gt;more&lt;/a&gt; . &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L412&#34;&gt;loops&lt;/a&gt; to see if it helps.&lt;/p&gt;

&lt;p&gt;It turns out performance actually suffered in some cases. I speculated that the reason may have to do with the bound checking when accessing slices. It turns out I might be right. After I disabled bound checking, performance increased when unrolling these loops. See below regarding disable bound checking.&lt;/p&gt;

&lt;h4 id=&#34;inline-simple-functions:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Inline Simple Functions&lt;/h4&gt;

&lt;p&gt;There are &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L53&#34;&gt;several&lt;/a&gt; . &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L60&#34;&gt;small&lt;/a&gt; functions that gets called quite often in this encoding library. During profiling I see these functions being on top all the time.&lt;/p&gt;

&lt;p&gt;I decided to try and inline these functions to see if the reduced function call overhead will help. In general I didn&amp;rsquo;t see much performance improvements using this technique. The most I saw was a 1% increase in performance. I contribute that to noise.&lt;/p&gt;

&lt;h4 id=&#34;disable-bound-checking-generally-not-recommended:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Disable Bound Checking (Generally NOT Recommended)&lt;/h4&gt;

&lt;p&gt;This integer encoding library operates on large slices of data. There&amp;rsquo;s a TON of slice access using index. Knowing the every slice access using index requires bound checking, I decided to try disabling bound checking using &lt;code&gt;-gcflags -B&lt;/code&gt; to compile the code.&lt;/p&gt;

&lt;p&gt;Disabling the bound checking for the Go compiler didn&amp;rsquo;t help as much as I hoped. For &lt;em&gt;ts.txt&lt;/em&gt;, disabling bound checking increased performance by 10% for Delta BP32 encoding only.&lt;/p&gt;

&lt;p&gt;However, if I disabled bound checking AND unrolled even more loops, we saw decoding performance increase anywhere from 10-40%. Encoding performance didn&amp;rsquo;t see much change. The following chart shows the performance increase (%) from the for-range loops to unrolled some additional loops when I disabled bound checking. This is comparing to the standard &lt;em&gt;gc&lt;/em&gt; compiler.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/no_bound_checking.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The question is, is it worth the removal of bound checking? For the raw results, I kept the numbers from the for-range loops.&lt;/p&gt;

&lt;h4 id=&#34;using-gccgo:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Using gccgo&lt;/h4&gt;

&lt;p&gt;When I was looking around for Go optimization techniques, I saw several posts on StackOverflow as well as the Go mailing list suggesting people try &lt;em&gt;gccgo&lt;/em&gt; as the compiler if they wanted more performance. So I thought I give it a shot as well. After downloading gcc 4.8.2 and letting it compile overnight, I was finally able to test it out.&lt;/p&gt;

&lt;p&gt;Compiling with &lt;em&gt;gccgo&lt;/em&gt; without any optimization flags actually saw performance drop by 50-60%. The best performance result was achieved when I compiled using &lt;em&gt;gccgo -O3&lt;/em&gt;. The comparison to Java uses the numbers from that binary.&lt;/p&gt;

&lt;p&gt;As mentioned above, &lt;em&gt;gccgo -O3&lt;/em&gt; seems to do a pretty amazing job of optimizing simple range loops. I was able to achieve 33% performance increase using for-range with &lt;em&gt;gccgo -O3&lt;/em&gt; instead of unrolling the simple loop. The final result was within 6% of the Java version for Delta BP32 decoding.&lt;/p&gt;

&lt;h4 id=&#34;use-some-assembly:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Use Some Assembly&lt;/h4&gt;

&lt;p&gt;The final trick I tried is to convert one of the often used functions to assembly language. This was suggested to me almost 6 weeks ago by Dave Andersen on the golang-nuts Google group. He suggested that I steal the bitLen function in the math/big/arith files. That&amp;rsquo;s &lt;a href=&#34;https://github.com/reducedb/encoding/commit/ea080c479fb4994e400ebba021d13f10c4f3fecc&#34;&gt;exactly what I did&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The bitlen function returns the position of the most significant bit that&amp;rsquo;s set to 1. It is most often called by encoding methods to determine how many bits are required to store that integers. So natually one would expect only the encoding functions will be improved.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s exactly what happened. Using the new bitlen assembly function, I was able to improve encoding performance by anywhere from 3% to 40%, depending on the codec. The most significant improvement was saw when Delta FastPFOR encoding was applied on &lt;em&gt;latency.txt&lt;/em&gt;. It consistently saw ~40% performance increase.&lt;/p&gt;

&lt;p&gt;As such, the &lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;code&lt;/a&gt; has been updated to use the assembly version of the bitlen.&lt;/p&gt;

&lt;h3 id=&#34;benchmark-environment:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Benchmark Environment&lt;/h3&gt;

&lt;p&gt;The system I used to run the benchmarks was graciously provided by Dr. Daniel Lemire. Here are the CPU and memory information at the time I ran the benchmarks. As you can see, we have plenty of memory to load the large integer arrays and should cause no swap. (I had this issue running these tests on my little MBA with 4GB of memory. :)&lt;/p&gt;

&lt;p&gt;No disk IO is involved in this benchmark.&lt;/p&gt;

&lt;h4 id=&#34;os:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;OS&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;NAME=&amp;quot;Ubuntu&amp;quot;
VERSION=&amp;quot;12.10, Quantal Quetzal&amp;quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&amp;quot;Ubuntu quantal (12.10)&amp;quot;
VERSION_ID=&amp;quot;12.10&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;cpu:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;CPU&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;model name      : Intel(R) Xeon(R) CPU E5-1620 0 @ 3.60GHz
cpu MHz         : 3591.566
cache size      : 10240 KB
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;memory:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Memory&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;             total       used       free     shared    buffers     cached
Mem:      32872068   13548960   19323108          0     209416   11517476
-/+ buffers/cache:    1822068   31050000
Swap:     33476604          0   33476604
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;java:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;java&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;java version &amp;quot;1.7.0_25&amp;quot;
OpenJDK Runtime Environment (IcedTea 2.3.10) (7u25-2.3.10-1ubuntu0.12.10.2)
OpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;go-and-gccgo:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;go and gccgo&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;go version go1.2rc4 linux/amd64
gcc version 4.8.2 (GCC)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;files:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Files&lt;/h4&gt;

&lt;p&gt;There are two files used in this benchmark:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;File 1: &lt;em&gt;&lt;a href=&#34;https://github.com/reducedb/encoding/tree/master/benchmark/data&#34;&gt;ts.txt&lt;/a&gt;&lt;/em&gt; contains 144285498 sorted integers. They are timestamps at 1 second precision. There are a lot of repeats as multiple events are recorded for that second.&lt;/li&gt;
&lt;li&gt;File 2: &lt;em&gt;latency.txt&lt;/em&gt; contains 144285498 &lt;strong&gt;unsorted&lt;/strong&gt; integers. An example, lat.txt.gz, can be seen &lt;a href=&#34;https://github.com/reducedb/encoding/tree/master/benchmark/data&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;codecs:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Codecs&lt;/h4&gt;

&lt;p&gt;For this benchmark, I used 4 different codecs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Delta Binary Packing (BP32) (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/IntegratedBinaryPacking.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/delta/bp32/bp32.go&#34;&gt;Go&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Delta FastPFOR (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/IntegratedFastPFOR.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/delta/fastpfor/fastpfor.go&#34;&gt;Go&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Binary Packing (BP32) (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/BinaryPacking.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/bp32/bp32.go&#34;&gt;Go&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;FastPFOR (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/FastPFOR.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/fastpfor/fastpfor.go&#34;&gt;Go&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because both BP32 and FastPFOR work on 128 integer blocks, the 58 remaining integers from the test files are encoded using Delta VariableByte and Variable Byte codecs, respectively. This is achieved using a &lt;strong&gt;Composition&lt;/strong&gt; (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/IntegratedComposition.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/composition/composition.go&#34;&gt;Go&lt;/a&gt;) codec.&lt;/p&gt;

&lt;p&gt;The Java and Go versions of the codecs are almost identical, logic-wise, aside from language differences. These codecs operate on arrays (or slices in Go) of integers. There are a lot of bitwise and shift operations, and lots of loops.&lt;/p&gt;

&lt;h3 id=&#34;benchmark-results:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Benchmark Results&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdDRyNEhPWUlqMHZzMG5FWFQzX1ZoZ1E&amp;amp;output=html&#34;&gt;raw results&lt;/a&gt; from running different Go compilers (and flags) and codes are in the Google spreadsheet at the bottom of the post.&lt;/p&gt;

&lt;p&gt;To be consistent, all the percentage numbers presented below are based on dividing the difference between the larger number (A) and the smaller number (B) by the smaller number (B).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(A - B) / B&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So you can say that A is faster than B by X%.&lt;/p&gt;

&lt;h4 id=&#34;go-fastest:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Go Fastest&lt;/h4&gt;

&lt;p&gt;I compiled 4 different versions of Go binary for this benchmark:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;benchmark.gc&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;This is built using the standard Go compiler, &lt;em&gt;gc&lt;/em&gt;. The command is &lt;code&gt;go build benchmark.o&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;benchmark.gc-B&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;This is built using the standard Go compiler, &lt;em&gt;gc&lt;/em&gt;, and I turned off bound checking for this version since the codecs deals with slices a lot. The command is &lt;code&gt;go build -gcflags -B benchmark.o&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;benchmark.gccgo&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;This is built using the &lt;em&gt;gccgo&lt;/em&gt; compiler with no additional flags. The command is &lt;code&gt;go build -compiler gccgo benchmark.o&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;benchmark.gccgo-O3&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;This is built using the &lt;em&gt;gccgo&lt;/em&gt; compiler with the &lt;em&gt;-O3&lt;/em&gt; flag. The command is &lt;code&gt;go build -compiler gccgo -gccgoflags &#39;-O3 -static&#39; benchmark.o&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Surprisingly, disabling the bound checking for the Go compiler didn&amp;rsquo;t help as much as I hoped. For &lt;em&gt;ts.txt&lt;/em&gt;, disabling bound checking increased performance by 10-40% for encoding only. For &lt;em&gt;ts.txt&lt;/em&gt; decoding and &lt;em&gt;latency.txt&lt;/em&gt;, it didn&amp;rsquo;t help at all.&lt;/p&gt;

&lt;p&gt;Using the &lt;em&gt;gccgo&lt;/em&gt; compiler with no flags had the worst performance. In general we saw that &lt;em&gt;benchmark.gc&lt;/em&gt; (standard Go version) is about 110%-130% faster than the &lt;em&gt;gccgo&lt;/em&gt; version.&lt;/p&gt;

&lt;p&gt;Lastly, the &lt;em&gt;gccgo-O3&lt;/em&gt; version is the fastest. This is the version that&amp;rsquo;s compiled using &lt;code&gt;-O3&lt;/code&gt; flag for gccgo. We saw that the &lt;em&gt;gccgo-O3&lt;/em&gt; version is anywhere from 10% to 60% faster than the &lt;em&gt;gc&lt;/em&gt; version.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/comparing_go_binaries.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;Above is a chart that shows the decoding performance of the &lt;em&gt;ts.txt&lt;/em&gt; file for the different binaries and codecs.&lt;/p&gt;

&lt;p&gt;For the comparison with Java, I am using the &lt;em&gt;gccgo-O3&lt;/em&gt; numbers.&lt;/p&gt;

&lt;h4 id=&#34;bits-per-integer-lower-is-better:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Bits Per Integer (Lower is Better)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/bits_per_integer.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;As you can tell, when the integers are sorted (ts.txt), the compression ratio is VERY good. Delta BP32 achieved 0.25 bits per 32-bit integer, and Delta FastPFOR is even better at 0.13 bits per integer. This is also because there are a lot of repeats in ts.txt.&lt;/p&gt;

&lt;p&gt;Because the timestamps are rather large numbers, e.g., 1375228800, when the non-delta codecs are used, they did not achieve very good compression ratio. We achieved ~31 bits per 32-bit integer using standard FastPFOR and BP32 codecs.&lt;/p&gt;

&lt;p&gt;When the integers are NOT sorted, then we run into trouble. When delta codecs are used, a lot of deltas are negative numbers, which means the MSB for most of the deltas is 1. In this case, it&amp;rsquo;s actually better to use the standard codecs instead of the delta codecs. The standard codecs achieved ~24 bits per 32-bit integer, and the delta codecs were ~32 bits per 32-bit integer.&lt;/p&gt;

&lt;p&gt;I also tested the latency file against the &lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/encoding#varints&#34;&gt;zigzag&lt;/a&gt; delta codecs and achieved ~25 bits per 32-bit integer. So it&amp;rsquo;s not much better than the standard codecs. However, zigzag delta comes in extremely handy when the negative numbers are smaller.&lt;/p&gt;

&lt;h4 id=&#34;java-vs-go:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Java vs. Go&lt;/h4&gt;

&lt;p&gt;For this section, we are comparing the decoding speed between the fastest Go version and Java. As you saw at the beginning of this post. Daniel Lemire&amp;rsquo;s &lt;a href=&#34;https://github.com/lemire/JavaFastPFOR&#34;&gt;Java version&lt;/a&gt; is anywhere from 12% to 180% faster than my &lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;Go version&lt;/a&gt; for decoding integers.&lt;/p&gt;

&lt;p&gt;The following chart shows how much (%) faster Java is compare to Go in decoding integers that are encoded using different codecs. It shows results from processing two different files. See below for more details.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_faster.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The following chart shows the decoding performance while processing &lt;em&gt;ts.txt&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_tstxt.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The following chart shows the decoding performance while processing &lt;em&gt;latency.txt&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://zhen.org/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_latency.png&#34;&gt;&lt;/p&gt;

&lt;h4 id=&#34;raw-results:2c8d0496131f181c56ff2ef05dc70d5e&#34;&gt;Raw Results&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdDRyNEhPWUlqMHZzMG5FWFQzX1ZoZ1E&amp;amp;output=html&#34;&gt;raw results&lt;/a&gt; are in the following Google spreadsheet.&lt;/p&gt;

&lt;iframe width=&#39;800&#39; height=&#39;500&#39; frameborder=&#39;0&#39; src=&#39;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdDRyNEhPWUlqMHZzMG5FWFQzX1ZoZ1E&amp;output=html&amp;widget=true&#39;&gt;&lt;/iframe&gt;

&lt;p&gt;Comments/feedbacks on &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qquqz/go_vs_java_decoding_billions_of_integers_per/&#34;&gt;reddit&lt;/a&gt;, &lt;a href=&#34;https://news.ycombinator.com/item?id=6743821&#34;&gt;hacker news&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving Cityhash Performance by Go Profiling</title>
      <link>http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/</link>
      <pubDate>Sun, 10 Nov 2013 15:04:22 -0800</pubDate>
      
      <guid>http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/</guid>
      <description>

&lt;p&gt;Comments/Feedback on &lt;a href=&#34;https://news.ycombinator.com/item?id=6710115&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qcygc/improving_cityhash_performance_by_go_profiling/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2013-11-19 Update #1: After more profiling (see &lt;em&gt;top&lt;/em&gt; output in the &amp;ldquo;After modification&amp;rdquo; section), I&amp;rsquo;ve found that these 3 functions, unalignedLoad64, fetch64, and uint64InExpectedOrder,  add up to quite a bit of execution time. I looked at the &lt;a href=&#34;https://code.google.com/p/cityhash/source/browse/trunk/src/city.cc&#34;&gt;original cityhash implementation&lt;/a&gt; and realized that the combination of these functions is basically geting a LittleEndian uint64, which we can read by doing LittleEndian.Uint64(). So I updated fetch64 to do just that. Performance increased by ~20% just because of that. Also, the bloom filter test showed that cityhash is now faster than both FNV64 and CRC64.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another month has gone by since the last post. This month has been extremely busy at work in an extraordinary good way. We had a huge POC that went quite succesfully at a large customer site. I also got a chance to visit Lisbon, Portugal as part of this POC. So things overall went pretty well.&lt;/p&gt;

&lt;p&gt;However, since family and work pretty much occupied most of my waking hours over the past few weeks, I haven&amp;rsquo;t made much progress on the &amp;ldquo;Go Learn&amp;rdquo; projects. To keep myself going, I picked a smaller task over the weekend and decided to go back to &amp;ldquo;Go Learn Project #1&amp;rdquo;, my &lt;a href=&#34;https://github.com/reducedb/cityhash&#34;&gt;cityhash&lt;/a&gt; Go implementation.&lt;/p&gt;

&lt;h3 id=&#34;go-learn-project-1:c82123e7d16007176a816d99942a250f&#34;&gt;Go Learn Project #1&lt;/h3&gt;

&lt;p&gt;When I first decided to learn Go, I struggled quite a bit to find a project that I can sink my teeth into. I am not sure if others are the same way, for me to learn a new language, I have to have something meaningful to work on. I can&amp;rsquo;t just write hello world programs or follow tutorials. I can read books and articles, but I will also procrastinate for weeks if I can&amp;rsquo;t find a relevant project.&lt;/p&gt;

&lt;p&gt;Luckily, I was thinking about creating a data generator at work and wanted to write that in Go. But in order to write the data generator in Go, I first have to have a cityhash implementation in Go because our backend (C/C++) is using cityhash.&lt;/p&gt;

&lt;p&gt;Surprisingly, I looked around but couldn&amp;rsquo;t find any Go implementation of cityhash. I would have thunk that given Go and Cityhash are both from Google, some Googler would have already ported cityhash over to Go. But no such luck, or maybe I just didn&amp;rsquo;t look hard enough. In any case, I decided to port cityhash over to Go.&lt;/p&gt;

&lt;p&gt;Porting an existing project in C over to Go has a big advantage in that I don&amp;rsquo;t have to invent any new data structures or algorithms. It will allow me to focus on learning the Go syntax and the standard libraries. Many many moons ago (before I converted to the dark side) I was pretty proficient in C and reading cityhash wasn&amp;rsquo;t too difficult, so porting cityhash over to Go should be relatively straightforward.&lt;/p&gt;

&lt;p&gt;In any case, the porting process wasn&amp;rsquo;t too difficult, as it turned out. Overall I was able to do that over a weekend. I was also able to port the test program (city-test.cc) over as well (vim substitution FTW) to validate that my implementation was functionally correct.&lt;/p&gt;

&lt;h3 id=&#34;performance-sucked:c82123e7d16007176a816d99942a250f&#34;&gt;Performance Sucked&lt;/h3&gt;

&lt;p&gt;I had always suspected that my Go cityhash implementation wasn&amp;rsquo;t great performance wise. At the time I hadn&amp;rsquo;t learned how to benchmark or profile Go programs, so I didn&amp;rsquo;t do a whole lot except ensuring functionally the results are correct. Also my data generator at work was working fine so I left the implementation as is.&lt;/p&gt;

&lt;p&gt;In September, I implemented a &lt;a href=&#34;https://github.com/reducedb/bloom&#34;&gt;Bloom Filter package&lt;/a&gt; which required a hash function as part of the implementation. For that package, I &lt;a href=&#34;http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/&#34;&gt;tested different hash functions&lt;/a&gt; to see how they affect the performance of the bloom filter. As you can see from those benchmarks, Cityhash is consistently 3x slower compare to the others. At the time I knew it was because of my implementation but didn&amp;rsquo;t look into it further.&lt;/p&gt;

&lt;h3 id=&#34;profiling-go-cityhash:c82123e7d16007176a816d99942a250f&#34;&gt;Profiling Go Cityhash&lt;/h3&gt;

&lt;p&gt;Since that first project, I have learned quite a bit more about benchmarking and profiling. So this weekend I finally took time to profile the Go implementation and found some interesting results. Now Go experts probably will read this and say &amp;ldquo;of course, you had no idea what you were doing.&amp;rdquo; And that would be true. I had no idea at the time. Hopefully this post will make up for it.&lt;/p&gt;

&lt;p&gt;If you haven&amp;rsquo;t read &lt;a href=&#34;http://blog.golang.org/profiling-go-programs&#34;&gt;this blog post on Go profiling&lt;/a&gt;, you should go read it now before continuing.&lt;/p&gt;

&lt;p&gt;In any case, I wrote a &lt;a href=&#34;https://gist.github.com/zhenjl/7405913&#34;&gt;short program&lt;/a&gt; to test cityhash with a big file. This way it can collect enough samples to tell me where the bottleneck is.&lt;/p&gt;

&lt;p&gt;The original implementation took 45 seconds to hash a 1.1G file. Below is the cpu profile output.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;duration = 45401991546ns
Total: 4295 samples
    2766  64.4%  64.4%     2766  64.4% runtime.memmove
     374   8.7%  73.1%      463  10.8% sweepspan
     259   6.0%  79.1%      383   8.9% MHeap_AllocLocked
     123   2.9%  82.0%      123   2.9% runtime.markspan
      95   2.2%  84.2%     1223  28.5% runtime.mallocgc
      74   1.7%  85.9%       74   1.7% runtime.MSpan_Init
      43   1.0%  86.9%     4234  98.6% github.com/reducedb/cityhash.unalignedLoad64
      40   0.9%  87.9%      595  13.9% runtime.MCache_Alloc
      32   0.7%  88.6%       32   0.7% runtime.markallocated
      31   0.7%  89.3%       56   1.3% MHeap_FreeLocked
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, most of the time were spent copying memory (64.4%). And also the sweepspan (part of GC) is also running quite often (8.7%).&lt;/p&gt;

&lt;p&gt;Before this, I had no idea that there&amp;rsquo;s that much memory being copied. So this is definitely interesting. I then looked the &lt;a href=&#34;http://zhen.org/images/2013-11-10-improving-cithhash-performance-by-go-profiling/before.svg&#34;&gt;graph of the profile data&lt;/a&gt; using the &amp;ldquo;web&amp;rdquo; command.&lt;/p&gt;

&lt;p&gt;It shows clearly that unalignedLoad64, a function that loads a uint64 from the buffer, is causing most of the memmove. Technically, it&amp;rsquo;s calling binary.Read(), which creates an array of 8 bytes, and passes to another function which eventually calls runtime.copy to copy a few bytes of data from the original buffer into the array.&lt;/p&gt;

&lt;p&gt;So now the reason for the large amount of time spent in memmove is clear. Basically, every time I call binary.Read(), it creates an 8 byte array. Up to 8 bytes of data are copied into it. Then the data in the array gets converted into an uint64. After that, the array is thrown away. And this is done over and over again for the whole 1.1G file, which means 1.1G of memory is being created in tiny 8-byte chunks, copied, and thrown away. It&amp;rsquo;s no wonder the program is slow!&lt;/p&gt;

&lt;p&gt;By this time, some of the readers are probably wondering why the heck I am using binary.Read() if I knew that I will be reading a uint64 from a slice. And they would be right again. Only excuse I have is that I had no clue and that was the first thing I found to work a few months back, so I just used it.&lt;/p&gt;

&lt;h3 id=&#34;modifying-the-implementation:c82123e7d16007176a816d99942a250f&#34;&gt;Modifying the Implementation&lt;/h3&gt;

&lt;p&gt;The change turned out to be relatively simple. Instead of using binary.Read(), I used LittleEndian.Uint64() to read the uint64. After the change, I ran the same program again.&lt;/p&gt;

&lt;p&gt;Here are the results from the post-change run. The time it took to hash the 1.1G file is only 2.8 seconds. That&amp;rsquo;s 16X faster than before the change. The &amp;ldquo;top&amp;rdquo; profile output is also a lot more reasonable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;duration = 2718339693ns
Total: 245 samples
      57  23.3%  23.3%       57  23.3% encoding/binary.littleEndian.Uint64
      50  20.4%  43.7%      227  92.7% github.com/reducedb/cityhash.CityHash128WithSeed
      40  16.3%  60.0%       97  39.6% github.com/reducedb/cityhash.unalignedLoad64
      35  14.3%  74.3%      146  59.6% github.com/reducedb/cityhash.fetch64
      28  11.4%  85.7%       28  11.4% github.com/reducedb/cityhash.uint64InExpectedOrder
      27  11.0%  96.7%      132  53.9% github.com/reducedb/cityhash.weakHashLen32WithSeeds_3
       8   3.3% 100.0%        8   3.3% github.com/reducedb/cityhash.weakHashLen32WithSeeds
       0   0.0% 100.0%        1   0.4% MHeap_AllocLarge
       0   0.0% 100.0%      227  92.7% _/Users/jian/Projects/cityhash_test.TestLatencyIntegers
       0   0.0% 100.0%      227  92.7% github.com/reducedb/cityhash.CityHash128
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also, nothing really jumps out when looking at the &lt;a href=&#34;http://zhen.org/images/2013-11-10-improving-cithhash-performance-by-go-profiling/after.svg&#34;&gt;post-change profile data graph&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;bloom-filter-benchmarks:c82123e7d16007176a816d99942a250f&#34;&gt;Bloom Filter Benchmarks&lt;/h3&gt;

&lt;p&gt;Now that the changes are in, I went back and re-ran some of the bloom filter benchmarks. They look a lot more reasonable as well. Below is a comparison of the Scalable Bloom Filter. The post-change run is almost 2.5x faster than the pre-change run. Also, the post-change number (1442 ns/op) is a lot closer to some of the other hash functions (~1100 ns/op).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Scalable Bloom Filter
---------------------
BenchmarkBloomCityHash   1000000              1442 ns/op (after cityhash change)
BenchmarkBloomCityHash   1000000              3375 ns/op (before cityhash change)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;summary:c82123e7d16007176a816d99942a250f&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;The Go authors have made it extermely simple to test, benchmark and profile Go programs so there&amp;rsquo;s really reason for anyone not to do that often. It helps you see how your program works and where the bottlenecks are. It can also help you identify surprises that you may not have though of. A good example is in my case, I had no idea binary.Read() works the way it works until I profiled my program.&lt;/p&gt;

&lt;p&gt;Comments/Feedback on &lt;a href=&#34;https://news.ycombinator.com/item?id=6710115&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qcygc/improving_cityhash_performance_by_go_profiling/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>