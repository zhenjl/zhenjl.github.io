<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Zen 3.1 &middot; Zen 3.1 </title>

  
  <link rel="stylesheet" href="http://zhen.org/css/poole.css">
  <link rel="stylesheet" href="http://zhen.org/css/syntax.css">
  <link rel="stylesheet" href="http://zhen.org/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://zhen.org/index.xml" rel="alternate" type="application/rss+xml" title="Zen 3.1" />

  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-681691-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a href="http://zhen.org">Zen 3.1</a></h1>
      <p class="lead">
       Product. Data. Code 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="http://zhen.org/blog">Archive</a></li>
      <br/>
      <li>Projects</li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/strace/sequence">sequence</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surgemq/surgemq">surgemq</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surge">zhenjl/others</a></li>
      
    </ul>

    <a href="https://twitter.com/zhenjl"><i class="fa fa-twitter-square"></i></a>&nbsp;&nbsp;
    <a href="http://linkedin.com/in/zhenjl"><i class="fa fa-linkedin-square"></i></a>&nbsp;&nbsp;
    
    
    

    <p class="footnote">Powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    Theme originally made by <a href="http://twitter.com/mdo">@mdo</a> <br/>
    Theme modified by <a href="http://npf.io">Nate Finch</a> <br/>
    &copy; 2015 Jian Zhen. All Rights Reserved.</p>
    
  </div>
</div>

    <div class="content container">
<div class="posts">

  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">
        Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns
      </a>
    </h1>

    <span class="post-date">Tue, Feb 10, 2015</span>

    

<p><a href="http://godoc.org/github.com/strace/sequence"><img src="http://godoc.org/github.com/strace/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="http://godoc.org/github.com/strace/sequence/cmd/sequence"><img src="http://godoc.org/github.com/strace/sequence/cmd/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="https://github.com/strace/sequence">github repo</a></p>

<p>This is part 2 of the <code>sequence</code> series. <a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Part 1</a> is about the high performance parser that can parse 100,000-200,000 MPs.</p>

<h2 id="background:61b1adc09fb9bc31a28d4faabfef3631">Background</h2>

<p>This post really takes me down the memory lane. Back in 2005, while I was at LogLogic, we envisioned an automated approach to tagging, or labeling, log messages. More specifically, we wanted to automatically tag specific components within the log messages with their semantic label, such as a source IP address, or a target user.</p>

<p>At the time, much like it is still today, the message parsing process is performed manually. This means someone has to manually look at the object and decided that the object should be labeled “user” or “targetUser.” An  analyst has to go through the log data, create a regular expression that extracts the useful strings out, and then finally assigning these to a specific label. This is extremely time consuming and error-prone.</p>

<p>At that time, the vision was to provide an automated approach to universally parse and analyze ANY log data. The key phrase being “automated approach.” This means the users should only need to provide minimum guidance to the system, if any, for the platforms to be able to analyze the log data. LogLogic never did much with this, unfortunately.</p>

<p>However, the tagging concept was later on adopted by (and I know how this got into CEE :) the <a href="http://cee.mitre.org/">Common Event Expression, or CEE</a> effort by Mitre. This idea of tags also inspired <a href="http://www.liblognorm.com/">liblognorm</a> to develop their <a href="http://www.libee.org/">libee</a> library and <a href="http://www.liblognorm.com/news/log-classification-with-liblognorm/">tagging system</a>. Rsyslog&rsquo;s <a href="http://www.rsyslog.com/doc/mmnormalize.html">mmnormalize</a> module is based on liblognorm.</p>

<p>And then there&rsquo;s Fedora&rsquo;s <a href="https://fedorahosted.org/lumberjack/">Project Lumberjack</a>, which &ldquo;is an open-source project to update and enhance the event log architecture&rdquo; and &ldquo;aims to improve the creation and standardize the content of event logs by implementing the concepts and specifications proposed by the ​Common Event Expression (CEE).&rdquo;</p>

<p>Then finally <a href="http://logstash.net/">logstash</a> has their <a href="http://logstash.net/docs/1.4.2/filters/grok">grok filter</a> that basically does similar extraction of unstructured data into a structured and queryable format. However, it seems like there might be some <a href="http://ghost.frodux.in/logstash-grok-speeds/">performance bottlenecks</a>.</p>

<p>However, none of these efforts attempted to solve the automated tagging/labeling problem. They mostly just try to provide a parser for log messages.</p>

<p>Also, it looks like many of these efforts have all been abandoned or put in hibernation, and haven&rsquo;t been updated since 2012 or 2013. liblognrom did put out <a href="http://www.liblognorm.com/news/">a couple of updates</a> in the past couple of years. Logstash&rsquo;s grok obviously is being maintained and developed with the <a href="http://www.elasticsearch.com/">Elasticsearch</a> backing.</p>

<p>It is understandable, unfortunately. Log parsing is <strong>BORING</strong>. I mean, who wants to sit there and stare at logs all day and try to come up with regular expressions or other types of parsing rules? LogLogic used to have a team of LogLabs analysts that did that, and I have to say I truly appreciated their effort and patience, because I cannot do that.</p>

<h3 id="the-end-result:61b1adc09fb9bc31a28d4faabfef3631">The End Result</h3>

<p>So instead of writing rules all day long, I decided to create an analyzer that can help us get at least 75% of the way there. The end result is the <code>Analyzer</code>, written in <a href="http://golang.org">Go</a>, in the <a href="https://github.com/strace/sequence">sequence</a> project I created. Here are some preliminary results. Below, we analyzed 2 files. The first is a file with over 200,000 sshd messages. The second is a file with a mixture of ASA, sshd, sudo and su log messages. It contains almost 450,000 messages.</p>

<p>By running the analyzer over these logs, the pure sshd log file returned 45 individual patterns, and the second returned 103 unique patterns.</p>

<pre><code>$ go run sequence.go analyze -i ../../data/sshd.all -o sshd.analyze
Analyzed 212897 messages, found 45 unique patterns, 45 are new.

$ go run sequence.go analyze -i ../../data/asasshsudo.log -o asasshsudo.analyze
Analyzed 447745 messages, found 103 unique patterns, 103 are new.
</code></pre>

<p>And the output file has entries such as:</p>

<pre><code>%msgtime% %apphost% %appname% [ %sessionid% ] : %status% %method% for %srcuser% from %srcipv4% port %srcport% ssh2
# Jan 15 19:39:26 irc sshd[7778]: Accepted password for jlz from 108.61.8.124 port 57630 ssh2

%msgtime% %appipv4% %appname% : %action% outbound %protocol% connection %sessionid% for %string% : %srcipv4% / %srcport% ( %ipv4% / %integer% ) to %string% : %dstipv4% / %dstport% ( %ipv4% / %integer% )
# 2012-04-05 18:46:18   172.23.0.1  %ASA-6-302013: Built outbound TCP connection 1424575 for outside:10.32.0.100/80 (10.32.0.100/80) to inside:172.23.73.72/2522 (10.32.0.1/54702)

%msgtime% %apphost% %appname% : %string% : tty = %string% ; pwd = %string% ; user = %srcuser% ; command = %command% - %string%
# Jan 15 14:09:11 irc sudo:    jlz : TTY=pts/1 ; PWD=/home/jlz ; USER=root ; COMMAND=/bin/su - irc
</code></pre>

<p>As you can see, the output is not 100%, but it gets us pretty close. Once the analyst goes through and updates the rules, he/she can re-run the analyzer anytime with any file to determine if there&rsquo;s new patterns. For example, below, we ran the sshd log file with an existing pattern file, and got 4 new log patterns.</p>

<pre><code>$ go run sequence.go analyze -i ../../data/sshd.all -p ../../patterns/sshd.txt -o sshd.analyze
Analyzed 212897 messages, found 39 unique patterns, 4 are new.
</code></pre>

<h3 id="parser-quick-review:61b1adc09fb9bc31a28d4faabfef3631">Parser - Quick Review</h3>

<p>I wrote about the <a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">sequence parser</a> a couple of weeks back. It is a <em>high performance sequential log parser</em>. It <em>sequentially</em> goes through a log message, <em>parses</em> out the meaningful parts, without the use regular expressions. It can achieve <em>high performance</em> parsing of <strong>100,000 - 200,000 messages per second (MPS)</strong> without the need to separate parsing rules by log source type. Underneath the hood, the <code>sequence</code> parser basically constructs a tree based on the sequential rules, walks the tree to identify all the possible paths, and returns the path that has the best match (highest weight) for the message.</p>

<p>While the analyzer is about reducing a large corupus of raw log messages down to a small set of unique patterns, the parser is all about matching log messages to an existing set of patters and determining whether a specific pattern has matched. Based on the pattern, it returns a sequence of tokens that basically extracts out the important pieces of information from the logs. The analysts can then take this sequence and perform other types of analysis.</p>

<p>The approach taken by the <code>sequence</code> parser is pretty much the same as liblognorm or other tree-based approaches.</p>

<h2 id="sequence-analyzer:61b1adc09fb9bc31a28d4faabfef3631">Sequence Analyzer</h2>

<p>In the following section I will go through additional details of how the <code>sequence</code> analyzer reduces 100 of 1000&rsquo;s of raw log messages down to just 10&rsquo;s of unique patterns, and then determining how to label the individual tokens.</p>

<h3 id="identifying-unique-patterns:61b1adc09fb9bc31a28d4faabfef3631">Identifying Unique Patterns</h3>

<p>Analyzer builds an analysis tree that represents all the Sequences from messages. It can be used to determine all of the unique patterns for a large body of messages.</p>

<p>It&rsquo;s based on a single basic concept, that for multiple log messages, if tokens in the same position shares one same parent and one same child, then the tokens in that position is likely variable string, which means it&rsquo;s something we can extract. For example, take a look at the following two messages:</p>

<pre><code>Jan 12 06:49:42 irc sshd[7034]: Accepted password for root from 218.161.81.238 port 4228 ssh2
Jan 12 14:44:48 jlz sshd[11084]: Accepted publickey for jlz from 76.21.0.16 port 36609 ssh2
</code></pre>

<p>The first token of each message is a timestamp, and the 3rd token of each message is the literal &ldquo;sshd&rdquo;. For the literals &ldquo;irc&rdquo; and &ldquo;jlz&rdquo;, they both share a common parent, which is a timestamp. They also both share a common child, which is &ldquo;sshd&rdquo;. This means token in between these, the 2nd token in each message, likely represents a variable token in this message type. In this case, &ldquo;irc&rdquo; and &ldquo;jlz&rdquo; happens to
represent the syslog host.</p>

<p>Looking further down the message, the literals &ldquo;password&rdquo; and &ldquo;publickey&rdquo; also share a common parent, &ldquo;Accepted&rdquo;, and a common child, &ldquo;for&rdquo;. So that means the token in this position is also a variable token (of type TokenString).</p>

<p>You can find several tokens that share common parent and child in these two messages, which means each of these tokens can be extracted. And finally, we can determine that the single pattern that will match both is:</p>

<pre><code>%time% %string% sshd [ %integer% ] : Accepted %string% for %string% from %ipv4% port %integer% ssh2
</code></pre>

<p>If later we add another message to this mix:</p>

<pre><code>Jan 12 06:49:42 irc sshd[7034]: Failed password for root from 218.161.81.238 port 4228 ssh2
</code></pre>

<p>The Analyzer will determine that the literals &ldquo;Accepted&rdquo; in the 1st message, and &ldquo;Failed&rdquo; in the 3rd message share a common parent &ldquo;:&rdquo; and a common child &ldquo;password&rdquo;, so it will determine that the token in this position is also a variable token. After all three messages are analyzed, the final pattern that will match all three
messages is:</p>

<pre><code>%time% %string% sshd [ %integer% ] : %string% %string% for %string% from %ipv4% port %integer% ssh2
</code></pre>

<p>By applying this concept, we can effectively identify all the unique patterns in a log file.</p>

<h3 id="determining-the-correct-labels:61b1adc09fb9bc31a28d4faabfef3631">Determining the Correct Labels</h3>

<p>Now that we have the unique patterns, we will scan the tokens to determine which labels we should apply to them.</p>

<p>System and network logs are mostly free form text. There&rsquo;s no specific patterns to any of them. So it&rsquo;s really difficult to determine how to label specific parts of the log message automatically. However, over the years, after looking at so many system and network log messages, some patterns will start to emerge.</p>

<p>There&rsquo;s no &ldquo;machine learning&rdquo; here. This section is all about codifying these human learnings. I&rsquo;ve created the following 6 rules to help label tokens in the log messages. By no means are these rules perfect. They are at best just guesses on how to label. But hopefully they can get us 75% of the way there and we human can just take it the rest of the way.</p>

<p><strong>0. Parsing Email and Hostname Formats</strong></p>

<p>This is technically not a labeling step. Before we actually start the labeling process, we wanted to first parse out a couple more formats like email and host names. The message tokenizer doesn&rsquo;t recognize these because they are difficult to parse and will slow down the tokenizer. These specific formats are also not needed by the parser. So because the analyzer doesn&rsquo;t care about performance as much, we can do this as post-processing step.</p>

<p>To recognize the hostname, we try to match the &ldquo;effective TLD&rdquo; using the <a href="https://github.com/surge/xparse/tree/master/etld">xparse/etld</a> package. It is an effective TLD matcher that returns the length of the effective domain name for the given string. It uses the data set from <a href="https://www.publicsuffix.org/list/effective_tld_names.dat">https://www.publicsuffix.org/list/effective_tld_names.dat</a>.</p>

<p><strong>1. Recognizing Syslog Headers</strong></p>

<p>First we will try to see if we can regonize the syslog headers. We try to recogize both RFC5424 and RFC3164 syslog headers:</p>

<pre><code>	// RFC5424
	// - &quot;1 2003-10-11T22:14:15.003Z mymachine.example.com evntslog - ID47 ...&quot;
	// - &quot;1 2003-08-24T05:14:15.000003-07:00 192.0.2.1 myproc 8710 - ...&quot;
	// - &quot;1 2003-10-11T22:14:15.003Z mymachine.example.com su - ID47 ...&quot;
	// RFC3164
	// - &quot;Oct 11 22:14:15 mymachine su: ...&quot;
	// - &quot;Aug 24 05:34:00 CST 1987 mymachine myproc[10]: ...&quot;
	// - &quot;jan 12 06:49:56 irc last message repeated 6 times&quot;
</code></pre>

<p>If the sequence pattern matches any of the above sequence, then we assume the first few tokens belong to the syslog header.</p>

<p><strong>2. Marking Key and Value Pairs</strong></p>

<p>The next step we perform is to mark known &ldquo;keys&rdquo;. There are two types of keys. First, we identify any token before the &ldquo;=&rdquo; as a key. For example, the message <code>fw=TOPSEC priv=6 recorder=kernel type=conn</code> contains 4 keys: <code>fw</code>, <code>priv</code>, <code>recorder</code> and <code>type</code>. These keys should be considered string literals, and should not be extracted. However, they can be used to determine how the value part should be labeled.</p>

<p>The second types of keys are determined by keywords that often appear in front of other tokens, I call these <strong>prekeys</strong>. For example, we know that the prekey <code>from</code> usually appears in front of any source host or IP address, and the prekey <code>to</code> usually appears in front of any destination host or IP address. Below are some examples of these prekeys.</p>

<pre><code>from 		= [ &quot;%srchost%&quot;, &quot;%srcipv4%&quot; ]
port 		= [ &quot;%srcport%&quot;, &quot;%dstport%&quot; ]
proto		= [ &quot;%protocol%&quot; ]
sport		= [ &quot;%srcport%&quot; ]
src 		= [ &quot;%srchost%&quot;, &quot;%srcipv4%&quot; ]
to 			= [ &quot;%dsthost%&quot;, &quot;%dstipv4%&quot;, &quot;%dstuser%&quot; ]
</code></pre>

<p>To help identify these prekeys, I wrote a quick program that goes through many of the logs I have to help identify what keywords appears before IP address, mac addresses, and other non-literal tokens. The result is put into the <a href="https://github.com/strace/sequence/blob/master/keymaps.go">keymaps.go</a> file. It&rsquo;s not comprehensive, but it&rsquo;s also not meant to be. We just need enough hints to help with labeling.</p>

<p><strong>3. Labeling &ldquo;Values&rdquo; by Their Keys</strong></p>

<p>Once the keys are labeled, we can label the values based on the mapping described above. For key/value pairs, we try to recognize both <code>key=value</code> or <code>key=&quot;value&quot;</code> formats (or other quote characters like &lsquo; or &lt;).</p>

<p>For the prekeys, we try to find the value token within 2 tokens of the key token. That means sequences such as <code>from 192.168.1.1</code> and <code>from ip 192.168.1.1</code> will identify <code>192.168.1.1</code> as the <code>%srcipv4%</code> based on the above mapping, but we will miss <code>from ip address 192.168.1.1</code>.</p>

<p><strong>4. Identifying Known Keywords</strong></p>

<p>Within most log messages, there are certain keywords that would indicate what actions were performed, what the state/status of the action was, and what objects the actions were performed on. CEE had a list that it identified, so I copied the list and added some of my own.</p>

<pre><code>action = [
	&quot;access&quot;,
	&quot;alert&quot;,
	&quot;allocate&quot;,
	&quot;allow&quot;,
	.
	.
	.
]

status = [
	&quot;accept&quot;,
	&quot;error&quot;,
	&quot;fail&quot;,
	&quot;failure&quot;,
	&quot;success&quot;
]

object = [
	&quot;account&quot;,
	&quot;app&quot;,
	&quot;bios&quot;,
	&quot;driver&quot;,
	.
	.
	.
]
</code></pre>

<p>In our labeling process, we basically goes through and identify all the string literals that are NOT marked as keys, and perform a <a href="https://github.com/surge/porter2">porter2 stemming operation</a> on the literal, then compare to the above list (which is also porter2 stemmed).</p>

<p>If a literal matches one of the above lists, then the corresponding label (<code>action</code>, <code>status</code>, <code>object</code>, <code>srcuser</code>, <code>method</code>, or <code>protocol</code>) is applied.</p>

<p><strong>5. Determining Positions of Specific Types</strong></p>

<p>In this next step, we are basically looking at the position of where some of the token types appear. Specifically, we are looking for <code>%time%</code>, <code>%url%</code>, <code>%mac%</code>, <code>%ipv4%</code>, <code>%host%</code>, and <code>%email%</code> tokens. Assuming the labels have not already been taken with the previous rules, the rules are as follows:</p>

<ul>
<li>The first %time% token is labeled as %msgtime%</li>
<li>The first %url% token is labeled as %object%</li>
<li>The first %mac% token is labeled as %srcmac% and the second is labeld as %dstmac%</li>
<li>The first %ipv4% token is labeled as %srcipv4% and the second is labeld as %dstipv4%</li>
<li>The first %host% token is labeled as %srchost% and the second is labeld as %dsthost%</li>
<li>The first %email% token is labeled as %srcemail% and the second is labeld as %dstemail%</li>
</ul>

<p><strong>6. Scanning for ip/port or ip:port Pairs</strong></p>

<p>Finally, after all that, we scan through the sequence again, and identify any numbers that follow an IP address, but separated by either a &ldquo;/&rdquo; or &ldquo;:&ldquo;. Then we label these numbers as either <code>%srcport%</code> or <code>%dstport%</code> based on how the previous IP address is labeled.</p>

<h3 id="summary:61b1adc09fb9bc31a28d4faabfef3631">Summary</h3>

<p>There are some limitations to the <code>sequence</code> parser and analyzer. For example, currently <code>sequence</code> does not handle multi-line logs. Each log message must appear as a single line. So if there&rsquo;s multi-line logs, they must be first be converted into a single line. Also, <code>sequence</code> has been only tested with a limited set of system (Linux, AIX, sudo, ssh, su, dhcp, etc etc), network (ASA, PIX, Neoteris, CheckPoint, Juniper Firewall) and infrastructure application (apache, bluecoat, etc) logs.</p>

<p>Documentation is available at godoc: <a href="http://godoc.org/github.com/strace/sequence">package</a>, <a href="http://godoc.org/github.com/strace/sequence/cmd/sequence">command</a>.</p>

<p>There are some pattern files developed for ASA, Sudo and SSH in the <code>patterns</code> directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages.</p>

<p>If you have a set of logs you would like me to test out, please feel free to <a href="https://github.com/strace/sequence/issues">open an issue</a> and we can arrange a way for me to download and test your logs.</p>

<p>Stay tuned for more log patterns&hellip;</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/papers-i-read-2015-week-7/">
        Papers I Read: 2015 Week 6
      </a>
    </h1>

    <span class="post-date">Sun, Feb 8, 2015</span>

    

<h3 id="random-ramblings:10248ddcf524aa32198bc5cded1aa098">Random Ramblings</h3>

<p>Athem hack</p>

<p>Another great resource of computer science papers is Adrian Coyler&rsquo;s <a href="http://blog.acolyer.org/">the morning paper</a>. He selects and summarizes &ldquo;an interesting/influential/important paper from the world of CS every weekday morning&rdquo;.</p>

<h3 id="papers-i-read:10248ddcf524aa32198bc5cded1aa098">Papers I Read</h3>

<ul>
<li><a href="http://www.cs.put.poznan.pl/dweiss/site/publications/download/fsacomp.pdf">Smaller Representation of Finite State Automata</a></li>
</ul>

<p>I read this paper when I was trying to figure out how to make the FSAs smaller for the <a href="https://github.com/surge/xparse/tree/master/etld">Effective TLD matcher</a> I created. The FSM I generated is 212,294 lines long. That&rsquo;s just absolutely crazy. This paper seems to present an interesting way of compressing them.</p>

<p>I am not exactly sure if <a href="https://godoc.org/golang.org/x/net/publicsuffix">PublicSuffix</a> uses a similar representation but it basically represents a FSA as an array of bytes, and then walk the bytes like a binary search tree. It&rsquo;s interesting for sure.</p>

<blockquote>
<p>This paper is a follow-up to Jan Daciuk’s experiments on space-efficient finite state automata representation that can be used directly for traversals in main memory [4]. We investigate several techniques of reducing the memory footprint of minimal automata, mainly exploiting the fact that transition labels and transition pointer offset values are not evenly distributed and so are suitable for compression. We achieve a size gain of around 20–30% compared to the original representation given in [4]. This result is comparable to the state-of-the-art dictionary compression techniques like the LZ-trie [12] method, but remains
memory and CPU efficient during construction.</p>
</blockquote>

<ul>
<li><a href="http://arxiv.org/pdf/1409.5942v1.pdf">IP Tracing and Active Network Response</a></li>
</ul>

<blockquote>
<p>This work presents integrated model for active security response model. The proposed model introduces Active Response Mechanism (ARM) for tracing anonymous attacks in the network back to their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or “spoofed”, source addresses. This paper presents within the proposed model two tracing approaches based on:
• Sleepy Watermark Tracing (SWT) for unauthorized access attacks.
• Probabilistic Packet Marking (PPM) in the network for Denial of Service
(DoS) and Distributed Denial of Service (DDoS) attacks.</p>
</blockquote>

<ul>
<li><a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36356.pdf">Dapper, a Large-Scale Distributed Systems Tracing Infrastructure</a></li>
</ul>

<blockquote>
<p>Here we introduce the design of Dapper, Google’s production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design
choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries.</p>
</blockquote>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/papers-i-read-2015-week-6/">
        Papers I Read: 2015 Week 6
      </a>
    </h1>

    <span class="post-date">Sun, Feb 8, 2015</span>

    

<p><a href="http://paperswelove.org/">Papers We Love</a> has been making rounds lately and a lot of people are excited about it. I also think it&rsquo;s kind of cool since I&rsquo;ve been reading a lot of research papers over the past year or so. I have been killing some trees because of that.</p>

<p>My interests have been mostly around data analytics, but the specific focus areas have changed a few times. I have read papers on data structures (bloom filters, skiplist, bitmap compression, etc), security analytics, consumer behavioral analysis, loyalty analytics, and now back to security analytics. In fact, recently I started reading a few security research papers that I found on <a href="http://www.covert.io/">covert.io</a>, put together by Jason Trost.</p>

<p>In any case, I thought it might be an interesting idea to share some of the papers I read/scan/skim on weekly basis. This way I can also track what I read over time.</p>

<h3 id="random-ramblings:e2137c343047358d9912399632750231">Random Ramblings</h3>

<p>This week has been a disaster. I was the last one in the family to catch the cold, but probably lasted the longest. In fact I am still only about 50%. This whole week I have been having headaches, body aches, and haven&rsquo;t been able to concentrate. My body must be trying to catch up on sleep or something. For a couple days I actually slept for almost 12 hours a night!</p>

<p>I&rsquo;ve been meaning to work on <a href="https://github.com/strace/sequence">sequence</a> and finish updating the analyzer, but really had a hard time concentrating. Any non-working hours are basically spent in bed if I could.</p>

<p>So this is probably the worst week to start the &ldquo;Papers I Read&rdquo; series since I only technically read 1 paper. But I am going to cheat a little, and list the papers I read over the past couple of weeks, pretty much all in my spare time.</p>

<p>This week we also saw Sony&rsquo;s accouncement that last year&rsquo;s hack cost them <a href="http://www.sony.net/SonyInfo/IR/financial/fr/150204_sony.pdf">$15 million</a> to investigate and remediate. It&rsquo;s pretty crazy if you think about it.</p>

<p>Let&rsquo;s assume that they hired a bunch of high-priced consultants, say $250/hour, to help comb through the logs and clean the systems. And let&rsquo;s say <sup>2</sup>&frasl;<sub>3</sub> of the $15m is spent on these consultants. That&rsquo;s <code>$10m / $250 = 40,000 hours</code>.</p>

<p>Let&rsquo;s say these consultants worked full time, non-stop, no weekends, no breaks, for 2 months since the announcement on Nov 24, 2014, that would be a team of 56 people (<code>40,000 hours / 60 days / 12 hours/day = 56</code>) working 12 hour days!</p>

<p>I&rsquo;ll tell ya, these security guys are raking it in. They make money upfront by selling products/services to protect the company, then they make money in the back by selling forensic services to clean up after the hack.</p>

<p>[Disclaimer: any mistake in my calculations/assumptions I blame on my drugged brain cells.]</p>

<h3 id="papers-i-read:e2137c343047358d9912399632750231">Papers I Read</h3>

<ul>
<li><a href="http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf">Beehive: Large-Scale Log Analysis for Detecting Suspicious Activity in Enterprise Networks</a></li>
</ul>

<blockquote>
<p>We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents.</p>
</blockquote>

<ul>
<li><a href="http://minds.cs.umn.edu/publications/chapter.pdf">Data Mining for Cyber Security</a></li>
</ul>

<blockquote>
<p>This chapter provides an overview of the Minnesota Intrusion Detection System (MINDS), which uses a suite of data mining based algorithms to address different aspects of cyber security. The various components of MINDS such as the scan detector, anomaly detector and the profiling module detect different types of attacks
and intrusions on a computer network.</p>
</blockquote>

<ul>
<li><a href="http://www.covert.io/research-papers/security/VAST-%20Network%20Visibility%20Across%20Space%20and%20Time.pdf">VAST: Network Visibility Across Space and Time</a></li>
</ul>

<blockquote>
<p>Key operational networking tasks, such as troubleshooting and defending against attacks, greatly benefit from attaining views of network activity that are unified across space and time. This means that data from heterogeneous devices and systems is treated in a uniformfashion, and that analyzing past activity and detecting future instances follow the same procedures. Based on previous ideas that formulated principles for comprehensive
network visibility [AKP+08], we present the design and architecture of Visibility Across Space and Time (VAST), an intelligent database that serves as a single vantage point into the network. The system is based on a generic event model to handle network data from disparate sources and provides a query architecture that allows operators or remote applications to extract events matching a given condition. We implemented a proof-of-principle prototype that can archive and index events from a wide range of sources. Moreover, we conducted a preliminary performance evaluation to verify that our implementation works efficient and as expected.</p>
</blockquote>

<ul>
<li><a href="http://www.covert.io/research-papers/security/Finding%20The%20Needle-%20Suppression%20of%20False%20Alarms%20in%20Large%20Intrusion%20Detection%20Data%20Sets.pdf">Finding The Needle: Suppression of False Alarms in Large Intrusion Detection Data Sets</a></li>
</ul>

<blockquote>
<p>Managed security service providers (MSSPs) must manage and monitor thousands of intrusion detection sensors.
The sensors often vary by manufacturer and software version, making the problem of creating generalized tools to separate true attacks from false positives particularly difficult. Often times it is useful from an operations perspective to know if a particular sensor is acting out of character. We propose a solution to this problem using anomaly detection techniques over the set of alarms produced by the sensors. Similar to the manner in which an anomaly based sensor detects deviations from normal user or system behavior, we establish the baseline
behavior of a sensor and detect deviations from this baseline. We show that departures from this profile by a sensor have a high probability of being artifacts of genuine attacks. We evaluate a set of time-based Markovian heuristics against a simple compression algorithm and show that we are able to detect the existence of all attacks which were manually identified by security personnel, drastically reduce the number of false positives, and identify attacks which were overlooked during manual evaluation.</p>
</blockquote>

<ul>
<li><a href="http://user.informatik.uni-goettingen.de/~krieck/docs/2013a-aisec.pdf">A Close Look on n-Grams in Intrusion Detection: Anomaly Detection vs. Classification</a></li>
</ul>

<blockquote>
<p>Detection methods based on n-gram models have been widely studied for the identification of attacks and malicious software. These methods usually build on one of two learning schemes: anomaly detection, where a model of normality is constructed from n-grams, or classification, where a discrimination between benign and malicious n-grams is learned. Although successful in many security domains, previous work falls short of explaining why a particular scheme is used and more importantly what renders one favorable over the other for a given type of data. In this paper we provide a close look on n-gram models for intrusion detection. We specifically study anomaly detection and classification using n-grams and develop criteria for data being used in one or the other
scheme. Furthermore, we apply these criteria in the scope of web intrusion detection and empirically validate their effectiveness with different learning-based detection methods for client-side and service-side attacks.</p>
</blockquote>

<ul>
<li><a href="http://blog.scalyr.com/2014/05/searching-20-gbsec-systems-engineering-before-algorithms/">Searching 20 GB/sec: Systems Engineering Before Algorithms</a></li>
</ul>

<p>Ok, this is a blog post, not a research paper, but it&rsquo;s somewhat interesting nonetheless.</p>

<blockquote>
<p>This article describes how we met that challenge using an “old school”, brute-force approach, by eliminating layers and avoiding complex data structures. There are lessons here that you can apply to your own engineering challenges.</p>
</blockquote>

  </div>
  
</div>

<div class="posts">
  <h1 class="post-title">Archive</h1>
  <ul class="posts">
      
      <li><span><a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns</a> - <time class="pull-right post-list">Tue, Feb 10, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/papers-i-read-2015-week-7/">Papers I Read: 2015 Week 6</a> - <time class="pull-right post-list">Sun, Feb 8, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/papers-i-read-2015-week-6/">Papers I Read: 2015 Week 6</a> - <time class="pull-right post-list">Sun, Feb 8, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS</a> - <time class="pull-right post-list">Sun, Feb 1, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">Generating Porter2 FSM For Fun and Performance in Go</a> - <time class="pull-right post-list">Wed, Jan 21, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/golang-from-a-non-programmers-perspective/">Go: From a Non-Programmer&#39;s Perspective</a> - <time class="pull-right post-list">Tue, Jan 13, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">PingMQ: A SurgeMQ-based ICMP Monitoring Tool</a> - <time class="pull-right post-list">Thu, Dec 25, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">SurgeMQ: High Performance MQTT Server and Client Libraries in Go</a> - <time class="pull-right post-list">Wed, Dec 24, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/">SurgeMQ: MQTT Message Queue @ 750,000 MPS</a> - <time class="pull-right post-list">Thu, Dec 4, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/graceful-shutdown-of-go-net-dot-listeners/">Graceful Shutdown of Go net.Listeners</a> - <time class="pull-right post-list">Thu, Dec 12, 2013</h4></time></span></li>
      
  </ul>
</div>

</div>

  </body>

</html>
