<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Zen 3.1 &middot; Zen 3.1 </title>

  
  <link rel="stylesheet" href="http://zhen.org/css/poole.css">
  <link rel="stylesheet" href="http://zhen.org/css/syntax.css">
  <link rel="stylesheet" href="http://zhen.org/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://zhen.org/index.xml/" rel="alternate" type="application/rss+xml" title="Zen 3.1" />

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-681691-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a href="http://zhen.org">Zen 3.1</a></h1>
      <p class="lead">
       Product. Data. Code 
      </p>
    </div>



    <ul class="sidebar-nav">
      <li><a href="http://zhen.org/blog">Archive</a></li>
      
    </ul>
      <a href="https://twitter.com/zhenjl"><i class="fa fa-twitter-square"></i></a>&nbsp;&nbsp;
      
      
      <a href="https://github.com/zhenjl"><i class="fa fa-github-square"></i></a>&nbsp;&nbsp;
      

    <p class="footnote">Powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    Theme originally made by <a href="http://twitter.com/mdo">@mdo</a> <br/>
    Theme modified by <a href="http://npf.io">Nate Finch</a> <br/>
    &copy; 2015 Jian Zhen. All rights reserved.</p>
    
  </div>
</div>

    <div class="content container">
<div class="posts">

  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log-analyzer-parser/">
        Sequence: A High Performance Sequential Semantic Log Analyzer and Parser
      </a>
    </h1>

    <span class="post-date">Mon, Jan 5, 2015</span>

    

<p><code>sequence</code> is a <em>high performance sequential semantic log message analyzer and parser</em>.</p>

<ul>
<li>It is <em>sequential</em> because it goes through a log message sequentially and does not use regular expressions.</li>
<li>It is <em>semantic</em> because it tries to extract meaningful information out of the log messages and give them semantic indicators, e.g., src IPv4 or dst IPv4.</li>
<li>It is an <em>analyzer</em> because analyzes a large corpus of text-based log messages and try to determine the unique patterns that would represent all of them.</li>
<li>It is a <em>parser</em> because it will take a message and parses out the meaningful parts.</li>
<li>It is <em>high performance</em> because it can parse 100K+ messages per second without the need to separate parsing rules by log source type.</li>
</ul>

<p><strong><code>sequence</code> is currently under active development and should be considered unstable until further notice.</strong></p>

<h3 id="toc_0">Motivation</h3>

<p>Log messages are notoriusly difficult to parse because they all have different formats. Industries (see Splunk, ArcSight, Tibco LogLogic, Sumo Logic, Logentries, Loggly, LogRhythm, etc etc etc) have been built to solve the problems of parsing, understanding and analyzing log messages.</p>

<p>Let&rsquo;s say you have a bunch of log files you like to parse. The first problem you will typically run into is you have no way of telling how many DIFFERENT types of messages there are, so you have no idea how much work there will be to develop rules to parse all the messages. Not only that, you have hundreds of thousands, if not  millions of messages, in front of you, and you have no idea what messages are worth parsing, and what&rsquo;s not.</p>

<p>The typical workflow is develop a set of regular expressions and keeps testing against the logs until some magical moment where all the logs you want parsed are parsed. Ask anyone who does this for a living and they will tell you this process is long, frustrating and error-prone.</p>

<p>Even after you have developed a set of regular expressions that match the original set of messages, if new messages come in, you will have to determine which of the new messages need to be parsed. And if you develop a new set of regular expressions to parse those new messages, you still have no idea if the regular expressions will conflict with the ones you wrote before. If you write your regex parsers too liberally, it can easily parse the wrong messages.</p>

<p>After all that, you will end up finding out the regex parsers are quite slow. It can typically parse several thousands messages per second. Given enough CPU resources on a large enough machine, regex parsers can probably parse tens of thousands of messages per second. Even to achieve this type of performance, you will likely need to limit the number of regular expressions the parser has. The more regex rules, the slower the parser will go.</p>

<p>To work around this performance issue, companies have tried to separate the regex rules for different log message types into different parsers. For example, they will have a parser for Cisco ASA logs, a parser for sshd logs, a parser for Apache logs, etc etc. And then they will require the users to tell them which parser to use (usually by indicating the log source type of the originating IP address or host.)</p>

<p>Sequence is developed to make analyzing and parsing log messages a lot easier and faster.</p>

<h3 id="toc_1">Performance</h3>

<p>The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro. The first file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message.</p>

<pre><code>  $ go version
  go version go1.4 darwin/amd64

  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 2.65 secs, ~ 80449.93 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 4.42 secs, ~ 53081.36 msgs/sec
</code></pre>

<p>Performance can be improved by adding more cores:</p>

<pre><code>  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.52 secs, ~ 140139.27 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 2.51 secs, ~ 93614.09 msgs/sec
</code></pre>

<h3 id="toc_2">License</h3>

<p>Copyright &copy; 2014 Dataence, LLC. All rights reserved.</p>

<p>Licensed under the Apache License, Version 2.0 (the &ldquo;License&rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>

<p><a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>

<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &ldquo;AS IS&rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>

<h3 id="toc_3">Roadmap / Futures</h3>

<p>There are some pattern files developed for ASA, Sudo and SSH in the <code>patterns</code> directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages. So currently there&rsquo;s not a set roadmap.</p>

<h2 id="toc_4">Concepts</h2>

<p>The following concepts are part of the package:</p>

<ul>
<li><p>A <em>Token</em> is a piece of information extracted from the original log message. It is a struct that contains fields for <em>TokenType</em>, <em>FieldType</em>, <em>Value</em>, and indicators of whether it&rsquo;s a key or value in the key=value pair.</p></li>

<li><p>A <em>TokenType</em> indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.</p></li>

<li><p>A <em>FieldType</em> indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).</p></li>

<li><p>A <em>Sequence</em> is a list of Tokens. It is returned by the <em>Scanner</em>, the <em>Analyzer</em>, and the <em>Parser</em>.</p></li>

<li><p>A <em>Scanner</em> is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, IPv4 addresses, URLs, MAC addresses,
integers and floating point numbers. It also recgonizes key=value or key=&ldquo;value&rdquo; or key=&lsquo;value&rsquo; or key=<value> pairs.</p></li>

<li><p>A <em>Analyzer</em> builds an analysis tree that represents all the Sequences from messages. It can be used to determine all of the unique patterns for a large body of messages.</p></li>

<li><p>A <em>Parser</em> is a tree-based parsing engine for log messages. It builds a parsing tree based on pattern sequence supplied, and for each message sequence, returns the matching pattern sequence. Each of the message tokens will be marked with the semantic field types.</p></li>
</ul>

<h2 id="toc_5">Sequence Command</h2>

<p>The typical workflow of using sequence is to first analyze all of the log messages to determine the unique patterns. This could easily reduce millions of log messages down to maybe 30-50 formats.</p>

<p>Then the analyst can look through these formats and annotate the patterns with the semantic meanings. Once that&rsquo;s done, the analyst can run the parser with these annotated rules and outcome the parsed tokens.</p>

<p>The <code>sequence</code> command is developed to demonstrate the use of this package. You can find it in the cmd/sequence directory.</p>

<pre><code>   Usage:
     sequence [command]

   Available Commands:
     scan                      scan will tokenize a log file or message and output a list of tokens
     analyze                   analyze will analyze a log file and output a list of patterns that will match all the log messages
     parse                     parse will parse a log file and output a list of parsed tokens for each of the log messages
     bench                     benchmark the parsing of a log file, no output is provided
     help [command]            Help about any command
</code></pre>

<h3 id="toc_6">Scan</h3>

<pre><code>  Usage:
    sequence scan [flags]

   Available Flags:
    -h, --help=false: help for scan
    -m, --msg=&quot;&quot;: message to tokenize
</code></pre>

<p>Example</p>

<pre><code>  $ ./sequence scan -m &quot;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream&quot;
  #   0: { Field=&quot;%funknown%&quot;, Type=&quot;%ts%&quot;, Value=&quot;jan 14 10:15:56&quot; }
  #   1: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;testserver&quot; }
  #   2: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;sudo&quot; }
  #   3: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   4: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;gonner&quot; }
  #   5: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   6: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;tty&quot; }
  #   7: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #   8: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;pts/3&quot; }
  #   9: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  10: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;pwd&quot; }
  #  11: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  12: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/home/gonner&quot; }
  #  13: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  14: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;user&quot; }
  #  15: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  16: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;root&quot; }
  #  17: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  18: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;command&quot; }
  #  19: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  20: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/bin/su&quot; }
  #  21: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;-&quot; }
  #  22: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;ustream&quot; }
</code></pre>

<h3 id="toc_7">Analyze</h3>

<pre><code>  Usage:
    sequence analyze [flags]

   Available Flags:
    -h, --help=false: help for analyze
    -i, --infile=&quot;&quot;: input file, required
    -o, --outfile=&quot;&quot;: output file, if empty, to stdout
    -d, --patdir=&quot;&quot;: pattern directory,, all files in directory will be used, optional
    -p, --patfile=&quot;&quot;: initial pattern file, optional
</code></pre>

<p>The following command analyzes a set of sshd log messages, and output the
patterns to the sshd.pat file. In this example, <code>sequence</code> analyzed over 200K
messages and found 45 unique patterns. Notice we are not supplying an existing
pattern file, so it treats all the patters as new.</p>

<pre><code>  $ ./sequence analyze -i ../../data/sshd.all  -o sshd.pat
  Analyzed 212897 messages, found 45 unique patterns, 45 are new.
</code></pre>

<p>And the output file has entries such as:</p>

<pre><code>  %ts% %string% sshd [ %integer% ] : %string% ( sshd : %string% ) : session %string% for user %string% by ( uid = %integer% )
  # Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
</code></pre>

<p>In the following command, we added an existing pattern file to the mix, which has
a set of existing rules. Notice now there are only 35 unique patterns, and we were
able to parse all of the log messages (no new patterns). There are fewer patterns
because some of the patterns were combined.</p>

<pre><code>  $ ./sequence analyze -d ../../patterns -i ../../data/sshd.all  -o sshd.pat
  Analyzed 212897 messages, found 35 unique patterns, 0 are new.
</code></pre>

<p>The same log message we saw above now has an entry like the following:</p>

<pre><code>  %createtime% %apphost% %appname% [ %sessionid% ] : %string% ( sshd : %string% ) : %object% %action% for user %dstuser% by ( uid = %integer% )
  # Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
</code></pre>

<h3 id="toc_8">Parse</h3>

<pre><code>  Usage:
    sequence parse [flags]

   Available Flags:
    -h, --help=false: help for parse
    -i, --infile=&quot;&quot;: input file, required
    -o, --outfile=&quot;&quot;: output file, if empty, to stdout
    -d, --patdir=&quot;&quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&quot;&quot;: initial pattern file, required
</code></pre>

<p>The following command parses a file based on existing rules. Note that the
performance number (9570.20 msgs/sec) is mostly due to reading/writing to disk.
To get a more realistic performance number, see the benchmark section below.</p>

<pre><code>  $ ./sequence parse -d ../../patterns -i ../../data/sshd.all  -o parsed.sshd
  Parsed 212897 messages in 22.25 secs, ~ 9570.20 msgs/sec
</code></pre>

<p>This is an entry from the output file:</p>

<pre><code>  Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
  #   0: { Field=&quot;%createtime%&quot;, Type=&quot;%ts%&quot;, Value=&quot;jan 15 19:39:26&quot; }
  #   1: { Field=&quot;%apphost%&quot;, Type=&quot;%string%&quot;, Value=&quot;jlz&quot; }
  #   2: { Field=&quot;%appname%&quot;, Type=&quot;%string%&quot;, Value=&quot;sshd&quot; }
  #   3: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;[&quot; }
  #   4: { Field=&quot;%sessionid%&quot;, Type=&quot;%integer%&quot;, Value=&quot;7778&quot; }
  #   5: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;]&quot; }
  #   6: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   7: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;pam_unix&quot; }
  #   8: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;(&quot; }
  #   9: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;sshd&quot; }
  #  10: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #  11: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;session&quot; }
  #  12: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;)&quot; }
  #  13: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #  14: { Field=&quot;%object%&quot;, Type=&quot;%string%&quot;, Value=&quot;session&quot; }
  #  15: { Field=&quot;%action%&quot;, Type=&quot;%string%&quot;, Value=&quot;opened&quot; }
  #  16: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;for&quot; }
  #  17: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;user&quot; }
  #  18: { Field=&quot;%dstuser%&quot;, Type=&quot;%string%&quot;, Value=&quot;jlz&quot; }
  #  19: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;by&quot; }
  #  20: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;(&quot; }
  #  21: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;uid&quot; }
  #  22: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  23: { Field=&quot;%funknown%&quot;, Type=&quot;%integer%&quot;, Value=&quot;0&quot; }
  #  24: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;)&quot; }
</code></pre>

<h3 id="toc_9">Benchmark</h3>

<pre><code>  Usage:
    sequence bench [flags]

   Available Flags:
    -c, --cpuprofile=&quot;&quot;: CPU profile filename
    -h, --help=false: help for bench
    -i, --infile=&quot;&quot;: input file, required
    -d, --patdir=&quot;&quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&quot;&quot;: pattern file, required
    -w, --workers=1: number of parsing workers
</code></pre>

<p>The following command will benchmark the parsing of two files. First file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA
log file, averaging 180 bytes per message.</p>

<pre><code>  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 2.65 secs, ~ 80449.93 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 4.42 secs, ~ 53081.36 msgs/sec
</code></pre>

<p>Performance can be improved by adding more cores:</p>

<pre><code>  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.52 secs, ~ 140139.27 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 2.51 secs, ~ 93614.09 msgs/sec
</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">
        PingMQ: A SurgeMQ-based ICMP Monitoring Tool
      </a>
    </h1>

    <span class="post-date">Thu, Dec 25, 2014</span>

    

<p><a href="https://github.com/surge/surgemq/tree/master/cmd/pingmq">pingmq</a> is developed to demonstrate the different use cases one can use <a href="//surgemq.com">SurgeMQ</a>, a high performance MQTT server and client library. In this simplified use case, a network administrator can setup server uptime monitoring system by periodically sending ICMP ECHO_REQUEST to all the IPs in their network, and send the results to SurgeMQ.</p>

<p>Then multiple clients can subscribe to results based on their different needs. For example, a client maybe only interested in any failed ping attempts, as that would indicate a host might be down. After a certain number of failures the client may then raise some type of flag to indicate host down.</p>

<p>There are three benefits of using SurgeMQ for this use case.</p>

<ul>
<li>First, with all the different monitoring tools out there that wants to know if hosts are up or down, they can all now subscribe to a single source of information. They no longer need to write their own uptime tools.</li>
<li>Second, assuming there are 5 monitoring tools on the network that wants to ping each and every host, the small packets are going to congest the network. The company can save 80% on their uptime monitoring bandwidth by having a single tool that pings the hosts, and have the rest subscribe to the results.</li>
<li>Third/last, the company can enhance their security posture by placing tighter restrictions on their firewalls if there&rsquo;s only a single host that can send ICMP ECHO_REQUESTS to all other hosts.</li>
</ul>

<p>The following commands will run pingmq as a server, pinging the 8.8.8.0/28 CIDR block, and publishing the results to /ping/success/{ip} and /ping/failure/{ip} topics every 30 seconds. <code>sudo</code> is needed because we are using RAW sockets and that requires root privilege.</p>

<pre><code>$ go build
$ sudo ./pingmq server -p 8.8.8.0/28 -i 30
</code></pre>

<p>The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.</p>

<pre><code>$ ./pingmq client -t /ping/failure/+
8.8.8.6: Request timed out for seq 1
</code></pre>

<p>The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.</p>

<pre><code>$ ./pingmq client -t /ping/success/+
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
</code></pre>

<p>One can also subscribe to a specific IP by using the following command.</p>

<pre><code>$ ./pingmq client -t /ping/+/8.8.8.8
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
</code></pre>

<h3 id="toc_0">Commands</h3>

<p>There are two builtin commands for <code>pingmq</code>.</p>

<p><strong><code>pingmq server</code></strong></p>

<pre><code>Usage:
  pingmq server [flags]

 Available Flags:
  -h, --help=false: help for server
  -i, --interval=60: ping interval in seconds
  -p, --ping=[]: Comma separated list of IPv4 addresses to ping
  -q, --quiet=false: print out ping results
  -u, --uri=&quot;tcp://:5836&quot;: URI to run the server on
</code></pre>

<p><strong><code>pingmq client</code></strong></p>

<pre><code>Usage:
  pingmq client [flags]

 Available Flags:
  -h, --help=false: help for client
  -s, --server=&quot;tcp://127.0.0.1:5836&quot;: PingMQ server to connect to
  -t, --topic=[]: Comma separated list of topics to subscribe to
</code></pre>

<h3 id="toc_1">IP Addresses</h3>

<p>To list IPs you like to use with <code>pingmq</code>, you can use the following formats:</p>

<pre><code>10.1.1.1      -&gt; 10.1.1.1
10.1.1.1,2    -&gt; 10.1.1.1, 10.1.1.2
10.1.1,2.1    -&gt; 10.1.1.1, 10.1.2.1
10.1.1,2.1,2  -&gt; 10.1.1.1, 10.1.1.2 10.1.2.1, 10.1.2.2
10.1.1.1-2    -&gt; 10.1.1.1, 10.1.1.2
10.1.1.-2     -&gt; 10.1.1.0, 10.1.1.1, 10.1.1.2
10.1.1.1-10   -&gt; 10.1.1.1, 10.1.1.2 ... 10.1.1.10
10.1.1.1-     -&gt; 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-3.1    -&gt; 10.1.1.1, 10.1.2.1, 10.1.3.1
10.1-3.1-3.1  -&gt; 10.1.1.1, 10.1.2.1, 10.1.3.1, 10.2.1.1, 10.2.2.1, 10.2.3.1, 10.3.1.1, 10.3.2.1, 10.3.3.1
10.1.1        -&gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-2      -&gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1-2        -&gt; 10.1.0.0, 10.1.0,1 ... 10.2.255.254, 10..2.255.255
10            -&gt; 10.0.0.0 ... 10.255.255.255
10.1.1.2,3,4  -&gt; 10.1.1.1, 10.1.1.2, 10.1.1.3, 10.1.1.4
10.1.1,2      -&gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1.1/28     -&gt; 10.1.1.0 ... 10.1.1.255
10.1.1.0/28   -&gt; 10.1.1.0 ... 10.1.1.15
10.1.1.0/30   -&gt; 10.1.1.0, 10.1.1.1, 10.1.1.2, 10.1.1.3
10.1.1.128/25 -&gt; 10.1.1.128 ... 10.1.1.255
</code></pre>

<h3 id="toc_2">Topic Format</h3>

<p>TO subscribe to the <code>pingmq</code> results, you can use the following formats:</p>

<ul>
<li><code>/ping/#</code> will subscribe to both success and failed pings for all IP addresses</li>
<li><code>ping/success/+</code> will subscribe to success pings for all IP addresses</li>
<li><code>ping/failure/+</code> will subscribe to failed pings for all IP addresses</li>
<li><code>ping/+/8.8.8.8</code> will subscribe to both success and failed pings for all IP 8.8.8.8</li>
</ul>

<h3 id="toc_3">Building</h3>

<p>To build <code>pingmq</code>, you need to have installed <a href="http://golang.org">Go 1.3.3 or 1.4</a>. Then run the following:</p>

<pre><code># go get github.com/surge/surgemq
# cd surgemq/examples/pingmq
# go build
</code></pre>

<p>After that, you should see the <code>pingmq</code> command in the pingmq directory.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">
        SurgeMQ: High Performance MQTT Server and Client Libraries in Go
      </a>
    </h1>

    <span class="post-date">Wed, Dec 24, 2014</span>

    

<p><strong>Happy Holidays!</strong></p>

<p>This is more of an announcement post as SurgeMQ is now compatibility-tested with some of the popular MQTT clients out there, and it&rsquo;s reaching <em>playable</em> state.</p>

<p>For completeness sake, please bear with some of the duplicate content in this post. The <a href="//blog/surgemq-mqtt-message-queue-750k-mps/">last post</a> made front page of <a href="https://news.ycombinator.com/item?id=8708921">Hacker News</a> and generated some great comments and discussions.</p>

<hr />

<p>SurgeMQ is a high performance MQTT broker and client library that aims to be fully compliant with MQTT 3.1 and 3.1.1 specs. The primary package that&rsquo;s of interest is package <a href="http://godoc.org/github.com/surge/surgemq/service">service</a>. It provides the MQTT Server and Client services in a library form.</p>

<p><strong>SurgeMQ is currently under active development and should be considered unstable until further notice.</strong></p>

<h3 id="toc_0">MQTT</h3>

<p>According to the <a href="http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html">MQTT spec</a>:</p>

<blockquote>
<p>MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.</p>

<p>The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include:</p>

<ul>
<li>Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications.</li>
<li>A messaging transport that is agnostic to the content of the payload.</li>
<li>Three qualities of service for message delivery:

<ul>
<li>&ldquo;At most once&rdquo;, where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after.</li>
<li>&ldquo;At least once&rdquo;, where messages are assured to arrive but duplicates can occur.</li>
<li>&ldquo;Exactly once&rdquo;, where message are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied.</li>
</ul></li>
<li>A small transport overhead and protocol exchanges minimized to reduce network traffic.</li>
<li>A mechanism to notify interested parties when an abnormal disconnection occurs.</li>
</ul>
</blockquote>

<p>There&rsquo;s some very large implementation of MQTT such as <a href="https://www.facebook.com/notes/facebook-engineering/building-facebook-messenger/10150259350998920">Facebook Messenger</a>. There&rsquo;s also an active Eclipse project, <a href="https://eclipse.org/paho/">Paho</a>, that provides scalable open-source client implementations for many different languages, including C/C++, Java, Python, JavaScript, C# .Net and Go.</p>

<h3 id="toc_1">Features, Limitations, and Future</h3>

<p><strong>Features</strong></p>

<ul>
<li>Supports QOS 0, 1 and 2 messages</li>
<li>Supports will messages</li>
<li>Supports retained messages (add/remove)</li>
<li>Pretty much everything in the spec except for the list below</li>
</ul>

<p><strong>Limitations</strong></p>

<ul>
<li>All features supported are in memory only. Once the server restarts everything is cleared.

<ul>
<li>However, all the components are written to be pluggable so one can write plugins based on the Go interfaces defined.</li>
</ul></li>
<li>Message redelivery on reconnect is not currently supported.</li>
<li>Message offline queueing on disconnect is not supported. Though this is also not a specific requirement for MQTT.</li>
</ul>

<p><strong>Future</strong></p>

<ul>
<li>Message re-delivery (DUP)</li>
<li>$SYS topics</li>
<li>Server bridge</li>
<li>Ack timeout/retry</li>
<li>Session persistence</li>
<li>Better authentication modules</li>
</ul>

<h3 id="toc_2">Performance</h3>

<p>Current performance benchmark of SurgeMQ, running all publishers, subscribers and broker on a single 4-core (2.8Ghz i7) MacBook Pro, is able to achieve:</p>

<ul>
<li>over <strong>400,000 MPS</strong> in a 1:1 single publisher and single producer configuration</li>
<li>over <strong>450,000 MPS</strong> in a 20:1 fan-in configuration</li>
<li>over <strong>750,000 MPS</strong> in a 1:20 fan-out configuration</li>
<li>over <strong>700,000 MPS</strong> in a full mesh configuration with 20 clients</li>
</ul>

<h3 id="toc_3">Compatibility</h3>

<p>In addition, SurgeMQ has been tested with the following client libraries and it <em>seems</em> to work:</p>

<ul>
<li><em>libmosquitto 1.3.5 (in C).</em> Tested with the bundled test programs msgsps_pub and msgsps_sub</li>
<li><em>Paho MQTT Conformance/Interoperability Testing Suite (in Python).</em> Tested with all 10 test cases, 3 did not pass. They are

<ol>
<li>&ldquo;offline messages queueing test&rdquo; which is not supported by SurgeMQ</li>
<li>&ldquo;redelivery on reconnect test&rdquo; which is not yet implemented by SurgeMQ</li>
<li>&ldquo;run subscribe failure test&rdquo; which is not a valid test</li>
</ol></li>
<li><em>Paho Go Client Library (in Go).</em> Tested with one of the tests in the library, in fact, that tests is now part of the tests for SurgeMQ.</li>
<li><em>Paho C Client library (in C).</em> Tested with most of the test cases and failed the same ones as the conformance test because the features are not yet implemented. Actually I think there&rsquo;s a bug in the test suite as it calls the PUBLISH handler function for non-PUBLISH messages.</li>
</ul>

<h3 id="toc_4">Documentation</h3>

<p>Documentation is available at <a href="http://godoc.org/github.com/surge/surgemq">godoc</a>.</p>

<p>More information regarding the design of the SurgeMQ is available at <a href="http://surgemq.com">zen 3.1</a>.</p>

<h3 id="toc_5">License</h3>

<p>Copyright &copy; 2014 Dataence, LLC. All rights reserved.</p>

<p>Licensed under the Apache License, Version 2.0 (the &ldquo;License&rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>

<p><a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>

<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &ldquo;AS IS&rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>

<h3 id="toc_6">Examples</h3>

<h4 id="toc_7">Server Example</h4>

<pre><code>// Create a new server
svr := &amp;service.Server{
    KeepAlive:        300,               // seconds
    ConnectTimeout:   2,                 // seconds
    SessionsProvider: &quot;mem&quot;,             // keeps sessions in memory
    Authenticator:    &quot;mockSuccess&quot;,     // always succeed
    TopicsProvider:   &quot;mem&quot;,             // keeps topic subscriptions in memory
}

// Listen and serve connections at localhost:1883
svr.ListenAndServe(&quot;tcp://:1883&quot;)
</code></pre>

<h4 id="toc_8">Client Example</h4>

<pre><code>// Instantiates a new Client
c := &amp;Client{}

// Creates a new MQTT CONNECT message and sets the proper parameters
msg := message.NewConnectMessage()
msg.SetWillQos(1)
msg.SetVersion(4)
msg.SetCleanSession(true)
msg.SetClientId([]byte(&quot;surgemq&quot;))
msg.SetKeepAlive(10)
msg.SetWillTopic([]byte(&quot;will&quot;))
msg.SetWillMessage([]byte(&quot;send me home&quot;))
msg.SetUsername([]byte(&quot;surgemq&quot;))
msg.SetPassword([]byte(&quot;verysecret&quot;))

// Connects to the remote server at 127.0.0.1 port 1883
c.Connect(&quot;tcp://127.0.0.1:1883&quot;, msg)

// Creates a new SUBSCRIBE message to subscribe to topic &quot;abc&quot;
submsg := message.NewSubscribeMessage()
submsg.AddTopic([]byte(&quot;abc&quot;), 0)

// Subscribes to the topic by sending the message. The first nil in the function
// call is a OnCompleteFunc that should handle the SUBACK message from the server.
// Nil means we are ignoring the SUBACK messages. The second nil should be a
// OnPublishFunc that handles any messages send to the client because of this
// subscription. Nil means we are ignoring any PUBLISH messages for this topic.
c.Subscribe(submsg, nil, nil)

// Creates a new PUBLISH message with the appropriate contents for publishing
pubmsg := message.NewPublishMessage()
pubmsg.SetPacketId(pktid)
pubmsg.SetTopic([]byte(&quot;abc&quot;))
pubmsg.SetPayload(make([]byte, 1024))
pubmsg.SetQoS(qos)

// Publishes to the server by sending the message
c.Publish(pubmsg, nil)

// Disconnects from the server
c.Disconnect()
</code></pre>

  </div>
  
</div>

<div class="posts">
  <h1 class="post-title">Archive</h1>
  <ul class="posts">
      
      <li><span><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log-analyzer-parser/">Sequence: A High Performance Sequential Semantic Log Analyzer and Parser</a> - <time class="pull-right post-list">Mon, Jan 5, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">PingMQ: A SurgeMQ-based ICMP Monitoring Tool</a> - <time class="pull-right post-list">Thu, Dec 25, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">SurgeMQ: High Performance MQTT Server and Client Libraries in Go</a> - <time class="pull-right post-list">Wed, Dec 24, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/">SurgeMQ: MQTT Message Queue @ 750,000 MPS</a> - <time class="pull-right post-list">Thu, Dec 4, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/graceful-shutdown-of-go-net-dot-listeners/">Graceful Shutdown of Go net.Listeners</a> - <time class="pull-right post-list">Thu, Dec 12, 2013</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/">Ring Buffer - Variable-Length, Low-Latency, Lock-Free, Disruptor-Style</a> - <time class="pull-right post-list">Sat, Nov 30, 2013</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/">Go vs Java: Decoding Billions of Integers Per Second</a> - <time class="pull-right post-list">Thu, Nov 14, 2013</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/">Improving Cityhash Performance by Go Profiling</a> - <time class="pull-right post-list">Sun, Nov 10, 2013</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/benchmarking-integer-compression-in-go/">Benchmarking Integer Compression in Go</a> - <time class="pull-right post-list">Fri, Oct 11, 2013</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/bitmap-compression-using-ewah-in-go/">Bitmap Compression using EWAH in Go</a> - <time class="pull-right post-list">Sun, Sep 15, 2013</h4></time></span></li>
      
  </ul>
</div>

</div>

  </body>

</html>
