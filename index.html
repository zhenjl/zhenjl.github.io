<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Zen 3.1 &middot; Zen 3.1 </title>

  
  <link rel="stylesheet" href="http://zhen.org/css/poole.css">
  <link rel="stylesheet" href="http://zhen.org/css/syntax.css">
  <link rel="stylesheet" href="http://zhen.org/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://zhen.org/index.xml/" rel="alternate" type="application/rss+xml" title="Zen 3.1" />

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-681691-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a href="http://zhen.org">Zen 3.1</a></h1>
      <p class="lead">
       Product. Data. Code 
      </p>
    </div>



    <ul class="sidebar-nav">
      <li><a href="http://zhen.org/blog">Archive</a></li>
      
    </ul>
      <a href="https://twitter.com/zhenjl"><i class="fa fa-twitter-square"></i></a>&nbsp;&nbsp;
      
      
      <a href="https://github.com/zhenjl"><i class="fa fa-github-square"></i></a>&nbsp;&nbsp;
      

    <p class="footnote">Powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    Theme originally made by <a href="http://twitter.com/mdo">@mdo</a> <br/>
    Theme modified by <a href="http://npf.io">Nate Finch</a> <br/>
    &copy; 2015 Jian Zhen. All rights reserved.</p>
    
  </div>
</div>

    <div class="content container">
<div class="posts">

  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/papers-i-read-2015-week-6/">
        Papers I Read: 2015 Week 6
      </a>
    </h1>

    <span class="post-date">Sun, Feb 8, 2015</span>

    

<p><a href="http://paperswelove.org/">Papers We Love</a> has been making rounds lately and a lot of people are excited about it. I also think it&rsquo;s kind of cool since I&rsquo;ve been reading a lot of research papers over the past year or so. I have been killing some trees because of that.</p>

<p>My interests have been mostly around data analytics, but the specific focus areas have changed a few times. I have read papers on data structures (bloom filters, skiplist, bitmap compression, etc), security analytics, consumer behavioral analysis, loyalty analytics, and now back to security analytics. In fact, recently I started reading a few security research papers that I found on <a href="http://www.covert.io/">covert.io</a>, put together by Jason Trost.</p>

<p>In any case, I thought it might be an interesting idea to share some of the papers I read/scan/skim on weekly basis. This way I can also track what I read over time.</p>

<h3 id="toc_0">Random Ramblings</h3>

<p>This week has been a disaster. I was the last one in the family to catch the cold, but probably lasted the longest. In fact I am still only about 50%. This whole week I have been having headaches, body aches, and haven&rsquo;t been able to concentrate. My body must be trying to catch up on sleep or something. For a couple days I actually slept for almost 12 hours a night!</p>

<p>I&rsquo;ve been meaning to work on <a href="https://github.com/strace/sequence">sequence</a> and finish updating the analyzer, but really had a hard time concentrating. Any non-working hours are basically spent in bed if I could.</p>

<p>So this is probably the worst week to start the &ldquo;Papers I Read&rdquo; series since I only technically read 1 paper. But I am going to cheat a little, and list the papers I read over the past couple of weeks, pretty much all in my spare time.</p>

<p>This week we also saw Sony&rsquo;s accouncement that last year&rsquo;s hack cost them <a href="http://www.sony.net/SonyInfo/IR/financial/fr/150204_sony.pdf">$15 million</a> to investigate and remediate. It&rsquo;s pretty crazy if you think about it.</p>

<p>Let&rsquo;s assume that they hired a bunch of high-priced consultants, say $250/hour, to help comb through the logs and clean the systems. And let&rsquo;s say <sup>2</sup>&frasl;<sub>3</sub> of the $15m is spent on these consultants. That&rsquo;s <code>$10m / $250 = 40,000 hours</code>.</p>

<p>Let&rsquo;s say these consultants worked full time, non-stop, no weekends, no breaks, for 2 months since the announcement on Nov 24, 2014, that would be a team of 56 people (<code>40,000 hours / 60 days / 12 hours/day = 56</code>) working 12 hour days!</p>

<p>I&rsquo;ll tell ya, these security guys are raking it in. They make money upfront by selling products/services to protect the company, then they make money in the back by selling forensic services to clean up after the hack.</p>

<p>[Disclaimer: any mistake in my calculations/assumptions I blame on my drugged brain cells.]</p>

<h3 id="toc_1">Papers I Read</h3>

<ul>
<li><a href="http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf">Beehive: Large-Scale Log Analysis for Detecting Suspicious Activity in Enterprise Networks</a></li>
</ul>

<blockquote>
<p>We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents.</p>
</blockquote>

<ul>
<li><a href="http://minds.cs.umn.edu/publications/chapter.pdf">Data Mining for Cyber Security</a></li>
</ul>

<blockquote>
<p>This chapter provides an overview of the Minnesota Intrusion Detection System (MINDS), which uses a suite of data mining based algorithms to address different aspects of cyber security. The various components of MINDS such as the scan detector, anomaly detector and the profiling module detect different types of attacks
and intrusions on a computer network.</p>
</blockquote>

<ul>
<li><a href="http://www.covert.io/research-papers/security/VAST-%20Network%20Visibility%20Across%20Space%20and%20Time.pdf">VAST: Network Visibility Across Space and Time</a></li>
</ul>

<blockquote>
<p>Key operational networking tasks, such as troubleshooting and defending against attacks, greatly benefit from attaining views of network activity that are unified across space and time. This means that data from heterogeneous devices and systems is treated in a uniformfashion, and that analyzing past activity and detecting future instances follow the same procedures. Based on previous ideas that formulated principles for comprehensive
network visibility [AKP+08], we present the design and architecture of Visibility Across Space and Time (VAST), an intelligent database that serves as a single vantage point into the network. The system is based on a generic event model to handle network data from disparate sources and provides a query architecture that allows operators or remote applications to extract events matching a given condition. We implemented a proof-of-principle prototype that can archive and index events from a wide range of sources. Moreover, we conducted a preliminary performance evaluation to verify that our implementation works efficient and as expected.</p>
</blockquote>

<ul>
<li><a href="http://www.covert.io/research-papers/security/Finding%20The%20Needle-%20Suppression%20of%20False%20Alarms%20in%20Large%20Intrusion%20Detection%20Data%20Sets.pdf">Finding The Needle: Suppression of False Alarms in Large Intrusion Detection Data Sets</a></li>
</ul>

<blockquote>
<p>Managed security service providers (MSSPs) must manage and monitor thousands of intrusion detection sensors.
The sensors often vary by manufacturer and software version, making the problem of creating generalized tools to separate true attacks from false positives particularly difficult. Often times it is useful from an operations perspective to know if a particular sensor is acting out of character. We propose a solution to this problem using anomaly detection techniques over the set of alarms produced by the sensors. Similar to the manner in which an anomaly based sensor detects deviations from normal user or system behavior, we establish the baseline
behavior of a sensor and detect deviations from this baseline. We show that departures from this profile by a sensor have a high probability of being artifacts of genuine attacks. We evaluate a set of time-based Markovian heuristics against a simple compression algorithm and show that we are able to detect the existence of all attacks which were manually identified by security personnel, drastically reduce the number of false positives, and identify attacks which were overlooked during manual evaluation.</p>
</blockquote>

<ul>
<li><a href="http://user.informatik.uni-goettingen.de/~krieck/docs/2013a-aisec.pdf">A Close Look on n-Grams in Intrusion Detection: Anomaly Detection vs. Classification</a></li>
</ul>

<blockquote>
<p>Detection methods based on n-gram models have been widely studied for the identification of attacks and malicious software. These methods usually build on one of two learning schemes: anomaly detection, where a model of normality is constructed from n-grams, or classification, where a discrimination between benign and malicious n-grams is learned. Although successful in many security domains, previous work falls short of explaining why a particular scheme is used and more importantly what renders one favorable over the other for a given type of data. In this paper we provide a close look on n-gram models for intrusion detection. We specifically study anomaly detection and classification using n-grams and develop criteria for data being used in one or the other
scheme. Furthermore, we apply these criteria in the scope of web intrusion detection and empirically validate their effectiveness with different learning-based detection methods for client-side and service-side attacks.</p>
</blockquote>

<ul>
<li><a href="http://blog.scalyr.com/2014/05/searching-20-gbsec-systems-engineering-before-algorithms/">Searching 20 GB/sec: Systems Engineering Before Algorithms</a></li>
</ul>

<p>Ok, this is a blog post, not a research paper, but it&rsquo;s somewhat interesting nonetheless.</p>

<blockquote>
<p>This article describes how we met that challenge using an “old school”, brute-force approach, by eliminating layers and avoiding complex data structures. There are lessons here that you can apply to your own engineering challenges.</p>
</blockquote>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">
        Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS
      </a>
    </h1>

    <span class="post-date">Sun, Feb 1, 2015</span>

    

<p><a href="http://godoc.org/github.com/strace/sequence"><img src="http://godoc.org/github.com/strace/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="http://godoc.org/github.com/strace/sequence/sequence"><img src="http://godoc.org/github.com/strace/sequence/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="https://github.com/strace/sequence">github repo</a></p>

<p><code>sequence</code> is a <em>high performance sequential semantic log parser</em>.</p>

<ul>
<li>It is <em>sequential</em> because it goes through a log message sequentially and does not use regular expressions.</li>
<li>It is <em>semantic</em> because it tries to extract meaningful information out of the log messages and give them semantic indicators, e.g., src IPv4 or dst IPv4.</li>
<li>It is a <em>parser</em> because it will take a message and parses out the meaningful parts.</li>
<li>It is <em>high performance</em> because it can parse 100K+ messages per second without the need to separate parsing rules by log source type.</li>
</ul>

<p><strong><code>sequence</code> is currently under active development and should be considered unstable until further notice.</strong></p>

<h3 id="toc_0">Motivation</h3>

<p>Log messages are notoriusly difficult to parse because they all have different formats. Industries (see Splunk, ArcSight, Tibco LogLogic, Sumo Logic, Logentries, Loggly, LogRhythm, etc etc etc) have been built to solve the problems of parsing, understanding and analyzing log messages.</p>

<p>Let&rsquo;s say you have a bunch of log files you like to parse. The first problem you will typically run into is you have no way of telling how many DIFFERENT types of messages there are, so you have no idea how much work there will be to develop rules to parse all the messages. Not only that, you have hundreds of thousands, if not  millions of messages, in front of you, and you have no idea what messages are worth parsing, and what&rsquo;s not.</p>

<p>The typical workflow is develop a set of regular expressions and keeps testing against the logs until some magical moment where all the logs you want parsed are parsed. Ask anyone who does this for a living and they will tell you this process is long, frustrating and error-prone.</p>

<p>Even after you have developed a set of regular expressions that match the original set of messages, if new messages come in, you will have to determine which of the new messages need to be parsed. And if you develop a new set of regular expressions to parse those new messages, you still have no idea if the regular expressions will conflict with the ones you wrote before. If you write your regex parsers too liberally, it can easily parse the wrong messages.</p>

<p>After all that, you will end up finding out the regex parsers are quite slow. It can typically parse several thousands messages per second. Given enough CPU resources on a large enough machine, regex parsers can probably parse tens of thousands of messages per second. Even to achieve this type of performance, you will likely need to limit the number of regular expressions the parser has. The more regex rules, the slower the parser will go.</p>

<p>To work around this performance issue, companies have tried to separate the regex rules for different log message types into different parsers. For example, they will have a parser for Cisco ASA logs, a parser for sshd logs, a parser for Apache logs, etc etc. And then they will require the users to tell them which parser to use (usually by indicating the log source type of the originating IP address or host.)</p>

<p>Sequence is developed to make analyzing and parsing log messages a lot easier and faster.</p>

<h3 id="toc_1">Performance</h3>

<p>The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro. The first file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message.</p>

<pre><code>  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 1.69 secs, ~ 126319.27 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 2.89 secs, ~ 81323.41 msgs/sec

  $ ./sequence bench -d ../patterns -i ../data/asasshsudo.log
  Parsed 447745 messages in 4.47 secs, ~ 100159.65 msgs/sec
</code></pre>

<p>Performance can be improved by adding more cores:</p>

<pre><code>  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.00 secs, ~ 212711.83 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 1.56 secs, ~ 150769.68 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -d ../patterns -i ../data/asasshsudo.log -w 2
  Parsed 447745 messages in 2.52 secs, ~ 177875.94 msgs/sec
</code></pre>

<h3 id="toc_2">Documentation</h3>

<p>Documentation is available at godoc: <a href="http://godoc.org/github.com/strace/sequence">package</a>, <a href="http://godoc.org/github.com/strace/sequence/sequence">command</a>.</p>

<h3 id="toc_3">License</h3>

<p>Copyright &copy; 2014 Dataence, LLC. All rights reserved.</p>

<p>Licensed under the Apache License, Version 2.0 (the &ldquo;License&rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>

<p><a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>

<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &ldquo;AS IS&rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>

<h3 id="toc_4">Roadmap / Futures</h3>

<p>There are some pattern files developed for ASA, Sudo and SSH in the <code>patterns</code> directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages. So currently there&rsquo;s not a set roadmap.</p>

<h2 id="toc_5">Concepts</h2>

<p>The following concepts are part of the package:</p>

<ul>
<li><p>A <em>Token</em> is a piece of information extracted from the original log message. It is a struct that contains fields for <em>TokenType</em>, <em>FieldType</em>, <em>Value</em>, and indicators of whether it&rsquo;s a key or value in the key=value pair.</p></li>

<li><p>A <em>TokenType</em> indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.</p></li>

<li><p>A <em>FieldType</em> indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).</p></li>

<li><p>A <em>Sequence</em> is a list of Tokens. It is returned by the <em>Tokenizer</em>, and the <em>Parser</em>.</p></li>

<li><p>A <em>Scanner</em> is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, IPv4 addresses, URLs, MAC addresses,
integers and floating point numbers. It also recgonizes key=value or key=&ldquo;value&rdquo; or key=&lsquo;value&rsquo; or key=<value> pairs.</p></li>

<li><p>A <em>Parser</em> is a tree-based parsing engine for log messages. It builds a parsing tree based on pattern sequence supplied, and for each message sequence, returns the matching pattern sequence. Each of the message tokens will be marked with the semantic field types.</p></li>
</ul>

<h2 id="toc_6">Sequence Command</h2>

<p>The <code>sequence</code> command is developed to demonstrate the use of this package. You can find it in the <code>sequence</code> directory. The <code>sequence</code> command implements the <em>sequential semantic log parser</em>.</p>

<pre><code>   Usage:
     sequence [command]

   Available Commands:
     scan                      scan will tokenize a log file or message and output a list of tokens
     parse                     parse will parse a log file and output a list of parsed tokens for each of the log messages
     bench                     benchmark the parsing of a log file, no output is provided
     help [command]            Help about any command
</code></pre>

<h3 id="toc_7">Scan</h3>

<pre><code>  Usage:
    sequence scan [flags]

   Available Flags:
    -h, --help=false: help for scan
    -m, --msg=&quot;&quot;: message to tokenize
</code></pre>

<p>Example</p>

<pre><code>  $ ./sequence scan -m &quot;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream&quot;
  #   0: { Field=&quot;%funknown%&quot;, Type=&quot;%ts%&quot;, Value=&quot;jan 14 10:15:56&quot; }
  #   1: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;testserver&quot; }
  #   2: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;sudo&quot; }
  #   3: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   4: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;gonner&quot; }
  #   5: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   6: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;tty&quot; }
  #   7: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #   8: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;pts/3&quot; }
  #   9: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  10: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;pwd&quot; }
  #  11: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  12: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/home/gonner&quot; }
  #  13: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  14: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;user&quot; }
  #  15: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  16: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;root&quot; }
  #  17: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  18: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;command&quot; }
  #  19: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  20: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/bin/su&quot; }
  #  21: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;-&quot; }
  #  22: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;ustream&quot; }
</code></pre>

<h3 id="toc_8">Parse</h3>

<pre><code>  Usage:
    sequence parse [flags]

   Available Flags:
    -h, --help=false: help for parse
    -i, --infile=&quot;&quot;: input file, required
    -o, --outfile=&quot;&quot;: output file, if empty, to stdout
    -d, --patdir=&quot;&quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&quot;&quot;: initial pattern file, required
</code></pre>

<p>The following command parses a file based on existing rules. Note that the
performance number (9570.20 msgs/sec) is mostly due to reading/writing to disk.
To get a more realistic performance number, see the benchmark section below.</p>

<pre><code>  $ ./sequence parse -d ../../patterns -i ../../data/sshd.all  -o parsed.sshd
  Parsed 212897 messages in 22.25 secs, ~ 9570.20 msgs/sec
</code></pre>

<p>This is an entry from the output file:</p>

<pre><code>  Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
  #   0: { Field=&quot;%createtime%&quot;, Type=&quot;%ts%&quot;, Value=&quot;jan 15 19:39:26&quot; }
  #   1: { Field=&quot;%apphost%&quot;, Type=&quot;%string%&quot;, Value=&quot;jlz&quot; }
  #   2: { Field=&quot;%appname%&quot;, Type=&quot;%string%&quot;, Value=&quot;sshd&quot; }
  #   3: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;[&quot; }
  #   4: { Field=&quot;%sessionid%&quot;, Type=&quot;%integer%&quot;, Value=&quot;7778&quot; }
  #   5: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;]&quot; }
  #   6: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   7: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;pam_unix&quot; }
  #   8: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;(&quot; }
  #   9: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;sshd&quot; }
  #  10: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #  11: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;session&quot; }
  #  12: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;)&quot; }
  #  13: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #  14: { Field=&quot;%object%&quot;, Type=&quot;%string%&quot;, Value=&quot;session&quot; }
  #  15: { Field=&quot;%action%&quot;, Type=&quot;%string%&quot;, Value=&quot;opened&quot; }
  #  16: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;for&quot; }
  #  17: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;user&quot; }
  #  18: { Field=&quot;%dstuser%&quot;, Type=&quot;%string%&quot;, Value=&quot;jlz&quot; }
  #  19: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;by&quot; }
  #  20: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;(&quot; }
  #  21: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;uid&quot; }
  #  22: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  23: { Field=&quot;%funknown%&quot;, Type=&quot;%integer%&quot;, Value=&quot;0&quot; }
  #  24: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;)&quot; }
</code></pre>

<h3 id="toc_9">Benchmark</h3>

<pre><code>  Usage:
    sequence bench [flags]

   Available Flags:
    -c, --cpuprofile=&quot;&quot;: CPU profile filename
    -h, --help=false: help for bench
    -i, --infile=&quot;&quot;: input file, required
    -d, --patdir=&quot;&quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&quot;&quot;: pattern file, required
    -w, --workers=1: number of parsing workers
</code></pre>

<p>The following command will benchmark the parsing of two files. First file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA
log file, averaging 180 bytes per message.</p>

<pre><code>  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 1.69 secs, ~ 126319.27 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 2.89 secs, ~ 81323.41 msgs/sec
</code></pre>

<p>Performance can be improved by adding more cores:</p>

<pre><code>  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.00 secs, ~ 212711.83 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 1.56 secs, ~ 150769.68 msgs/sec
</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">
        Generating Porter2 FSM For Fun and Performance in Go
      </a>
    </h1>

    <span class="post-date">Wed, Jan 21, 2015</span>

    

<p><a href="http://godoc.org/github.com/surgebase/porter2"><img src="http://godoc.org/github.com/surgebase/porter2?status.svg" alt="GoDoc" />
</a></p>

<h2 id="toc_0">tl;dr</h2>

<ul>
<li>This post describes the <a href="https://github.com/surgebase/porter2">Porter2</a> package I implemented. It is written in Go (#golang).</li>
<li>By using a <a href="http://en.wikipedia.org/wiki/Finite-state_machine">finite-state-machine</a> approach to <a href="http://snowball.tartarus.org/algorithms/english/stemmer.html">Porter2</a> <a href="http://en.wikipedia.org/wiki/Stemming">stemming</a>, I was able to achieve 660% better performance compare to other Go implementations.</li>
<li>FSM-based approach is great for known/fixed data set, but obviously not workable if the data set changes at runtime.</li>
<li>Hand-coding FSM is a PITA!!! <a href="https://github.com/surgebase/porter2/tree/master/cmd/suffixfsm">Automate</a> if possible.</li>
</ul>

<h2 id="toc_1">Introduction</h2>

<p>In a personal project I am working on, I had the need to perform word stemming in two scenarios. First, I need to perform stemming for all the string literals in a LARGE corpus and then determine if the words are in a fixed set of literals. Second, I need to perform stemming for a subset of words in real-time, as messages stream in.</p>

<p>In the first case, performance is important but not critical; in the second case, performance is a huge factor.</p>

<h3 id="toc_2">Stemming</h3>

<p>To start, according to <a href="http://en.wikipedia.org/wiki/Stemming">wikipedia</a>:</p>

<blockquote>
<p>Stemming is the term used in linguistic morphology and information retrieval to describe the process for reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.</p>
</blockquote>

<p>As a quick example, the words <em>fail</em>, <em>failed</em>, and <em>failing</em> all mean something has <em>failed</em>. By stemming these three words, I will get a single form which is <em>fail</em>. I can then just use <em>fail</em> going forward instead of having to compare all three forms all the time.</p>

<p>The <a href="http://tartarus.org/martin/PorterStemmer/def.txt">Porter</a> stemming algorithm is by far the most commonly used stemmer and also considered to be one of the most gentle stemmers. The Porter stemming algorithm (or ‘Porter stemmer’) works by removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems. (<a href="http://tartarus.org/martin/PorterStemmer/">ref</a>)</p>

<p><a href="http://snowball.tartarus.org/algorithms/english/stemmer.html">Porter2</a> is universally considered to be an enhancement over the original Porter algorithm. Porter2 has an improved set of rules and it&rsquo;s widely used as well.</p>

<h2 id="toc_3">Implementation</h2>

<p>This package, <a href="https://github.com/surgebase/porter2">Porter2</a>, implements the Porter2 stemmer. It is written completely using finite state machines to perform suffix comparison, rather than the usual string-based or tree-based approaches. As a result, it is 660% faster compare to string comparison-based approach written in the same (Go) language.</p>

<p>This implementation has been successfully validated with the dataset from <a href="http://snowball.tartarus.org/algorithms/english/">http://snowball.tartarus.org/algorithms/english/</a>, so it should be in a usable state. If you encounter any issues, please feel free to <a href="https://github.com/surgebase/porter2/issues">open an issue</a>.</p>

<p>Usage is fairly simple:</p>

<pre><code>import &quot;github.com/surgebase/porter2&quot;

fmt.Println(porter2.Stem(&quot;seaweed&quot;)) // should get seawe
</code></pre>

<h3 id="toc_4">Performance</h3>

<p>This implementation by far has the highest performance of the various Go-based implementations, AFAICT. I tested a few of the implementations and the results are below.</p>

<table>
<thead>
<tr>
<th>Implementation</th>
<th>Time</th>
<th>Algorithm</th>
</tr>
</thead>

<tbody>
<tr>
<td><a href="https://github.com/surgebase/porter2">surgebase</a></td>
<td>319.009358ms</td>
<td>Porter2</td>
</tr>

<tr>
<td><a href="https://github.com/dchest/stemmer">dchest</a></td>
<td>2.106912401s</td>
<td>Porter2</td>
</tr>

<tr>
<td><a href="https://github.com/kljensen/snowball">kljensen</a></td>
<td>5.725917198s</td>
<td>Porter2</td>
</tr>
</tbody>
</table>

<p>To run the test again, you can run <a href="https://github.com/surgebase/porter2/tree/master/cmd/compare">compare.go</a> (<code>go run compare.go</code>).</p>

<h3 id="toc_5">State Machines</h3>

<p>Most of the implementations, like the ones in the table above, rely completely on suffix string comparison. Basically there&rsquo;s a list of suffixes, and the code will loop through the list to see if there&rsquo;s a match. Given most of the time you are looking for the longest match, so you order the list so the longest is the first one. So if you are luckly, the match will be early on the list. But regardless that&rsquo;s a huge performance hit.</p>

<p>This implementation is based completely on finite state machines to perform suffix comparison. You compare each chacter of the string starting at the last character going backwards. The state machines will determine what the longest suffix is.</p>

<p>As an example, let&rsquo;s look at the 3 suffixes from step0 of the porte2 algorithm. The goal, and it&rsquo;s the same for all the other steps, it&rsquo;s to find the longest matching suffix.</p>

<pre><code>'
's
's'
</code></pre>

<p>If you were to build a non-space-optimized <a href="http://en.wikipedia.org/wiki/Suffix_tree">suffix tree</a>, you would get this, where R is the root of the tree, and any node with * is designated as a final state:</p>

<pre><code>        R
       / \
      '*  s
     /     \
    s       '*
   /
  '*
</code></pre>

<p>This is a fairly easy tree to build, and we actually did that in the FSM generator we will talk about later. However, to build a working suffix tree in Go, we would need to use a <code>map[rune]*node</code> structure at each of the nodes. And then search the map for each rune we encounter.</p>

<p>To test the performance of using a switch statement vs using a map, I wrote a <a href="https://github.com/surgebase/porter2/tree/master/cmd/switchvsmap">quick test</a>:</p>

<pre><code>switch: 4.956523ms
   map: 10.016601ms
</code></pre>

<p>The test basically runs a switch statement and a map each for 1,000,000 times. So it seems like using a switch statement is faster than a map. Though I think the compiler basically builds a map for all the switch case statements.  (Maybe we should call this post <em>Microbenchmarking for fun and performance</em>?)</p>

<p>In any case, let&rsquo;s go with the switch approach. We basically need to unroll the suffix tree into a finite state machine.</p>

<pre><code>        R0
       / \
      '1* s2
     /     \
    s3      '4*
   /
  '5*
</code></pre>

<p>To do that, we need to assign a state number to each of the nodes in the suffix tree, and output each of the states and the transitions based on the rune encountered. The tree above is the same as the one before, but now has a state number assigned to each node.</p>

<h3 id="toc_6">Generator</h3>

<p>I actually started building all the porter2 FSMs manually with a completely different approach than what I am describing here. I won&rsquo;t go into details here but needless to say, it was disastrous. Not only was hand coding state machines extremely error-prone, the approach I was taking also had a lot of potential for bugs. It took me MANY HOURS to hand build those FSMs but at the end, I was happy to abandon all of them for the approach I am taking now.</p>

<p>To reduce errors and make updating the FSM easier, I wrote a quick tool called <a href="https://github.com/surgebase/porter2/tree/master/cmd/suffixfsm">suffixfsm</a> to generate the FSMs. The tool basically takes a list of suffixes, creates a suffix tree as described above, and unrolls the tree into a set of states using the <code>switch</code> statement.</p>

<p>It took me just a couple hours to write and debug the tool, and I was well on my way to fixing other bugs!</p>

<p>For example, running the command <code>go run suffixfsm.go step0.txt</code> generated the following code. This is a complete function for step0 of the porter2 algorithm. The only thing missing is what to do with each of the final states, which are in the last <code>switch</code> statement.</p>

<pre><code>var (
		l int = len(rs) // string length
		m int			// suffix length
		s int			// state
		f int			// end state of longgest suffix
		r rune			// current rune
	)

loop:
	for i := 0; i &lt; l; i++ {
		r = rs[l-i-1]

		switch s {
		case 0:
			switch r {
			case '\'':
				s = 1
				m = 1
				f = 1
				// ' - final
			case 's':
				s = 2
			default:
				break loop
			}
		case 1:
			switch r {
			case 's':
				s = 4
			default:
				break loop
			}
		case 2:
			switch r {
			case '\'':
				s = 3
				m = 2
				f = 3
				// 's - final
			default:
				break loop
			}
		case 4:
			switch r {
			case '\'':
				s = 5
				m = 3
				f = 5
				// 's' - final
			default:
				break loop
			}
		default:
			break loop
		}
	}

	switch f {
	case 1:
		// ' - final

	case 3:
		// 's - final

	case 5:
		// 's' - final

	}

	return rs
</code></pre>

<h2 id="toc_7">Finally</h2>

<p>This is a technique that can probably be applied to any fixed data set. The performance may vary based on the size of the state machine so test it with both maps and FSM to see what works best.</p>

<p>Happy Go&rsquo;ing!</p>

  </div>
  
</div>

<div class="posts">
  <h1 class="post-title">Archive</h1>
  <ul class="posts">
      
      <li><span><a href="http://zhen.org/blog/papers-i-read-2015-week-6/">Papers I Read: 2015 Week 6</a> - <time class="pull-right post-list">Sun, Feb 8, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS</a> - <time class="pull-right post-list">Sun, Feb 1, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">Generating Porter2 FSM For Fun and Performance in Go</a> - <time class="pull-right post-list">Wed, Jan 21, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/golang-from-a-non-programmers-perspective/">Go: From a Non-Programmer&#39;s Perspective</a> - <time class="pull-right post-list">Tue, Jan 13, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">PingMQ: A SurgeMQ-based ICMP Monitoring Tool</a> - <time class="pull-right post-list">Thu, Dec 25, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">SurgeMQ: High Performance MQTT Server and Client Libraries in Go</a> - <time class="pull-right post-list">Wed, Dec 24, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/">SurgeMQ: MQTT Message Queue @ 750,000 MPS</a> - <time class="pull-right post-list">Thu, Dec 4, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/graceful-shutdown-of-go-net-dot-listeners/">Graceful Shutdown of Go net.Listeners</a> - <time class="pull-right post-list">Thu, Dec 12, 2013</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/">Ring Buffer - Variable-Length, Low-Latency, Lock-Free, Disruptor-Style</a> - <time class="pull-right post-list">Sat, Nov 30, 2013</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/">Go vs Java: Decoding Billions of Integers Per Second</a> - <time class="pull-right post-list">Thu, Nov 14, 2013</h4></time></span></li>
      
  </ul>
</div>

</div>

  </body>

</html>
