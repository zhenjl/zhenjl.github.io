<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Zen 3.1 &middot; Zen 3.1 </title>

  
  <link rel="stylesheet" href="http://zhen.org/css/poole.css">
  <link rel="stylesheet" href="http://zhen.org/css/syntax.css">
  <link rel="stylesheet" href="http://zhen.org/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://zhen.org/index.xml" rel="alternate" type="application/rss+xml" title="Zen 3.1" />

  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-681691-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a href="http://zhen.org">Zen 3.1</a></h1>
      <p class="lead">
       Product. Data. Code 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="http://zhen.org/blog">Archive</a></li>
      <br/>
      <li>Projects</li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/strace/sequence">sequence</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surgemq/surgemq">surgemq</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surge">zhenjl/others</a></li>
      
    </ul>

    <a href="https://twitter.com/zhenjl"><i class="fa fa-twitter-square"></i></a>&nbsp;&nbsp;
    <a href="http://linkedin.com/in/zhenjl"><i class="fa fa-linkedin-square"></i></a>&nbsp;&nbsp;
    
    
    

    <p class="footnote">Powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    Theme originally made by <a href="http://twitter.com/mdo">@mdo</a> <br/>
    Theme modified by <a href="http://npf.io">Nate Finch</a> <br/>
    &copy; 2015 Jian Zhen. All Rights Reserved.</p>
    
  </div>
</div>

    <div class="content container">
<div class="posts">

  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">
        Sequence: Optimizing Go For the High Performance Log Scanner
      </a>
    </h1>

    <span class="post-date">Fri, Feb 13, 2015</span>

    

<p>Really appreciate everyone upvoting my previous <code>sequence</code> posts on <a href="http://www.reddit.com/r/golang/">reddit</a> and <a href="https://news.ycombinator.com/newest">Hacker News</a>. I hope you can <a href="https://news.ycombinator.com/item?id=9047795">upvote this post</a> as well!</p>

<p><a href="http://godoc.org/github.com/strace/sequence"><img src="http://godoc.org/github.com/strace/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="http://godoc.org/github.com/strace/sequence/cmd/sequence"><img src="http://godoc.org/github.com/strace/sequence/cmd/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="https://github.com/strace/sequence">github repo</a></p>

<p>This is part 3 of the <a href="http://strace.io/sequence">sequence</a> series.</p>

<ul>
<li><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Part 1</a> is about the high performance parser that can parse 100,000-200,000 MPs.</li>
<li><a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Part 2</a> is about automating the process of reducing 100 of 1000&rsquo;s of log messages down to dozens of unique patterns.</li>
<li><a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">Part3</a> is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size and core count) for scanning and tokenizing log messages</li>
</ul>

<p>I would love to learn more about the state-of-the-art approaches that log vendors are using. These attempts are about scratching my own itch and trying to realize ideas I&rsquo;ve had in my mind. Given some of these ideas are 5 to 10 years old, they may already be outdated. Personally I just haven&rsquo;t heard of any groundbreaking approaches.</p>

<p>In any case, if you know of some of the more innovative ways people are approaching these problems, please please please comment below as I would love to hear from you.</p>

<h3 id="tl-dr:cb54f18a9944e1962d3fe8e3f09ea809">tl;dr</h3>

<ul>
<li>The <code>sequence</code> scanner is designed to tokenize free-form log messages.

<ul>
<li>It can scan between 200K to 500K log messages per second depending on message size and core count.</li>
<li>It recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.</li>
<li>The design is based mostly on finite-state machines.</li>
</ul></li>
<li>The performance was achieved by the following techniques:

<ol>
<li>Go Through the String Once and Only Once</li>
<li>Avoid Indexing into the String</li>
<li>Reduce Heap Allocation</li>
<li>Reduce Data Copying</li>
<li>Mind the Data Struture</li>
<li>Avoid Interfaces If Possible</li>
<li>Find Ways to Short Circuit Checks</li>
</ol></li>
</ul>

<h2 id="background:cb54f18a9944e1962d3fe8e3f09ea809">Background</h2>

<blockquote>
<p>In computer science, lexical analysis is the process of converting a sequence of characters into a sequence of tokens, i.e. meaningful character strings. A program or function that performs lexical analysis is called a lexical analyzer, lexer, tokenizer, or scanner. - <a href="http://en.wikipedia.org/wiki/Lexical_analysis">Wikipedia</a></p>
</blockquote>

<p>One of the most critical functions in the <code>sequence</code> parser is the message tokenization. At a very high level, message tokenization means taking a single log message and breaking it into a list of tokens.</p>

<h3 id="functional-requirements:cb54f18a9944e1962d3fe8e3f09ea809">Functional Requirements</h3>

<p>The challenge is knowing where the token break points are. Most log messages are free-form text, which means there&rsquo;s no common structure to them.</p>

<p>As an example, the following log message can be tokenized into the sequence of tokens below. As you can see, one cannot depend on white spaces to tokenize, as the timestamp would be broken into 3 parts; nor can one use punctuations like &ldquo;;&rdquo; or &ldquo;:&ldquo;, as they would break the log mesage into useless parts.</p>

<pre><code>jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream

  #   0: { Field=&quot;%funknown%&quot;, Type=&quot;%ts%&quot;, Value=&quot;jan 14 10:15:56&quot; }
  #   1: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;testserver&quot; }
  #   2: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;sudo&quot; }
  #   3: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   4: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;gonner&quot; }
  #   5: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   6: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;tty&quot; }
  #   7: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #   8: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;pts/3&quot; }
  #   9: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  10: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;pwd&quot; }
  #  11: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  12: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/home/gonner&quot; }
  #  13: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  14: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;user&quot; }
  #  15: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  16: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;root&quot; }
  #  17: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  18: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;command&quot; }
  #  19: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  20: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/bin/su&quot; }
  #  21: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;-&quot; }
  #  22: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;ustream&quot; }
</code></pre>

<p>So a log message <em>scanner</em> or <em>tokenizer</em> (we will use these terms interchangeably) must understand common components such as timestamp, URL, hex strings, IP addresses (v4 or v6), and mac addresses, so it can break the messages into <em>meaningful components</em>.</p>

<h3 id="performance-requirements:cb54f18a9944e1962d3fe8e3f09ea809">Performance Requirements</h3>

<p>From a performance requirements perspective, I really didn&rsquo;t start out with any expectations. However, after achieving 100-200K MPS for parsing (not just tokenizing), I have a strong desire to keep the performance at that level. So the more I can optimize the scanner to tokenize faster, the more head room I have for parsing.</p>

<p>One may ask, who can POSSIBLY use such performance? Many organizations that I know are generating between 50-100M messages per second (MPS), that&rsquo;s only 1,200 MPS. Some larger organizations I know are generating 60GB of Bluecoat logs per day, <strong>8 years ago</strong>!! That&rsquo;s a good 3,000 MPS assuming an average of 250 bytes per message. Even if log rate grows at 15%, that&rsquo;s still only 10K MPS today.</p>

<p>To run through an example, <a href="http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf">at EMC, 1.4 billion log messages are generated daily on average, at a rate of one terabyte a day</a>. That&rsquo;s 16,200 messages per second, and about 714 bytes per message. (Btw, what system can possibly generate messages that are 714 bytes long? That&rsquo;s crazy and completely inefficient!) These EMC numbers are from 2013, so they have likely increased by now.</p>

<p>The <code>sequence</code> parser, with a single CPU core, can process about 270,000 MPS for messages averaging 98 bytes. Assuming the performance is linear compare to the message size (which is pretty close to the truth), we can process 37,000 MPS for messages averaging 714 bytes. That&rsquo;s just enough to parse the 16,2000 MPS, with a little head room to do other types of analysis or future growth.</p>

<p>Obviously one can throw more hardware at solving the scale problem, but then again, why do that if you don&rsquo;t need to. Just because you have the hardware doesn&rsquo;t mean you should waste the money! Besides, there are much more interesting analytics problems your hardware can be used for than just tokenizing a message.</p>

<p>In any case, I want to squeeze every oz of performance out of the scanner so I can have more time in the back to parse and analyze. So let&rsquo;s set a goal of keeping at least 200,000 MPS for 100 bytes per message (BPM).</p>

<p>Yes, go ahead and tell me I shouldn&rsquo;t worry about micro-optimization, because this post is all about that. :)</p>

<h2 id="sequence-scanner:cb54f18a9944e1962d3fe8e3f09ea809">Sequence Scanner</h2>

<p>In the <code>sequence</code> package, we implemented a general log message scanner, called <a href="https://github.com/strace/sequence/blob/master/scanner.go">GeneralScanner</a>. GeneralScanner is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.</p>

<p>This implementation was able to achieve both the functional and performance requirements. The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro, although the tests were only using 1 or 2 cores. The first file is a bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message. Last is a mix of ASA, sshd and sudo logs, averaging 136 bytes per message.</p>

<pre><code>  $ ./sequence bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272869.35 msgs/sec

  $ ./sequence bench scan -i ../../data/allasa.log
  Scanned 234815 messages in 1.43 secs, ~ 163827.61 msgs/sec

  $ ./sequence bench scan -i ../../data/allasassh.log
  Scanned 447745 messages in 2.27 secs, ~ 197258.42 msgs/sec
</code></pre>

<p>Performance can be improved by adding more cores:</p>

<pre><code>  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/sshd.all -w 2
  Scanned 212897 messages in 0.43 secs, ~ 496961.52 msgs/sec

  $ GOMAXPROCS=2 ./sequenceo bench scan -i ../../data/allasa.log -w 2
  Scanned 234815 messages in 0.80 secs, ~ 292015.98 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/allasassh.log -w 2
  Scanned 447745 messages in 1.20 secs, ~ 373170.45 msgs/sec
</code></pre>

<h3 id="concepts:cb54f18a9944e1962d3fe8e3f09ea809">Concepts</h3>

<p>To understand the scanner, you have to understand the following concepts that are part of the package.</p>

<ul>
<li><p>A <em>Token</em> is a piece of information extracted from the original log message. It is a struct that contains fields for <em>TokenType</em>, <em>FieldType</em>, and <em>Value</em>.</p></li>

<li><p>A <em>TokenType</em> indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.</p></li>

<li><p>A <em>FieldType</em> indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).</p></li>

<li><p>A <em>Sequence</em> is a list of Tokens. It is the key data structure consumed and returned by the <em>Scanner</em>, <em>Analyzer</em>, and the <em>Parser</em>.</p></li>
</ul>

<p>Basically, the scanner takes a log message string, tokenizes it and returns a <em>Sequence</em> with the recognized <em>TokenType</em> marked. This <em>Sequence</em> is then fed into the analyzer or parser, and the analyzer or parser in turn returns another <em>Sequence</em> that has the recognized <em>FieldType</em> marked.</p>

<h3 id="design:cb54f18a9944e1962d3fe8e3f09ea809">Design</h3>

<p>Tokenizers or scanners are usually implemented using finite-state machines. Each FSM (or FSA, finite state automata) understands a specific sequences of characters that make up a type of token.</p>

<p>In the <code>sequence</code> scanner, there are three FSMs: Time, HexString and General.</p>

<ul>
<li>The Time FSM understands a list of <a href="https://github.com/strace/sequence/blob/master/time.go">time formats</a>. This list of time formats are commonly seen in log messages. It is also fairly easy to add to this list if needed.</li>
<li>The HexString FSM is designed to understand IPv6 addresses (dead:beef:1234:5678:223:32ff:feb1:2e50 or f0f0:f::1), MAC addresses (00:04:c1:8b:d8:82), fingerprints or signatures (de:ad:be:ef:74:a6:bb:45:45:52:71:de:b2:12:34:56).</li>
<li>The General FSM that recognizes URLs, IPv4 addresses, and any literal or strings.</li>
</ul>

<p>Each character in the log string are run through all three FSMs.</p>

<ol>
<li>If a time format is matched, that&rsquo;s what it will be returned.</li>
<li>Next if a hex string is matched, it is also returned.

<ul>
<li>We mark anything with 5 colon characters and no successive colons like &ldquo;::&rdquo; to be a MAC address.</li>
<li>Anything that has 7 colons and no successive colons are marked as IPv6 address.</li>
<li>Anything that has less than 7 colons but has only 1 set of successive colons like &ldquo;::&rdquo; are marked as IPv6 address.</li>
<li>Everything else is just a literal.</li>
</ul></li>
<li>Finally if neither of the above matched, we return what the general FSM has matched.

<ul>
<li>The general FSM recognizes these quote characters: &ldquo;, &lsquo; and &lt;. If these characters are encountered, then it will consider anything between the quotes to be a single token.</li>
<li>Anything that starts with http:// or https:// are considered URLs.</li>
<li>Anything that matches 4 integer octets are considered IP addresses.</li>
<li>Anything that matches two integers with a dot in between are considered floats.</li>
<li>Anything that matches just numbers are considered integers.</li>
<li>Everything else are literals.</li>
</ul></li>
</ol>

<h3 id="performance:cb54f18a9944e1962d3fe8e3f09ea809">Performance</h3>

<p>To achieve the performance requirements, the following rules and optimizations are followed. Some of these are Go specific, and some are general recommendations.</p>

<p><strong>1. Go Through the String Once and Only Once</strong></p>

<p>This is a hard requirement, otherwise we can&rsquo;t call this project a <em>sequential</em> parser. :)</p>

<p>This is probably a pretty obvious technique. The more times you loop through loop through a string, the lower the performance. If you used regular expressions to parse logs, you will likely go through parts of the log message multiple times due to back tracking or look forward, etc.</p>

<p>I took great pain to ensure that I don&rsquo;t need to look forward or look backward in the log string to determine the current token type, and I think the effort paid off.</p>

<p>In reality though, while I am only looping through the log string once, and only once, I do run each character through three different FSMs. However, it is still much less expensive than looping through three times, each time checking a single FSM. However, the more FSMs I run the characters through, the slower it gets.</p>

<p>This was apparently when I <a href="https://github.com/strace/sequence/commit/a5447814f43b4b9b7e804b14dde38e88fd53e6d0">updated the scanner to support IPv6 and hex strings</a>. I tried a couple of different approaches. First, I added an IPv6 specific FSM. So in addition to the original time, mac and general FSMs, there are now 4. That dropped performance by like 15%!!! That&rsquo;s just unacceptable.</p>

<p>The second approach, which is the one I checked in, combines the MAC, IPv6 and general hex strings into a single FSM. That helped somewhat. I was able to regain about 5% of the performance hit. However, because I can no longer short circuit the MAC address check (by string length and colon positions), I was still experiencing a 8-10% hit.</p>

<p>What this means is that for most tokens, instead of checking just 2 FSMs because I can short circuit the MAC check pretty early, I have to now check all 3 FSMs.</p>

<p>So the more FSMs, the more comlicated the FSMs, the more performance hits there will be.</p>

<p><strong>2. Avoid Indexing into the String</strong></p>

<p>This is really a Go-specific recommentation. Each time you index into a slice or string, Go will perform bounds checking for you, which means there&rsquo;s extra operations it&rsquo;s doing, and also means lower performance. As an example, here are results from two benchmark runs. The first is with bounds checking enabled, which is default Go behavior. The second disables bounds checking.</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.79 secs, ~ 268673.91 msgs/sec

  $ go run -gcflags=-B ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 277479.58 msgs/sec
</code></pre>

<p>The performance difference is approximately 3.5%! However, while it&rsquo;s fun to see the difference, I would never recommend disable bounds checking in production. So the next best thing is to remove as many operations that index into a string or slice as possible. Specifically:</p>

<ol>
<li>Use &ldquo;range&rdquo; in the loops, e.g. <code>for i, r := range str</code> instead of <code>for i := 0; i &lt; len(str); i++ { if str[i] == ... }</code></li>
<li>If you are checking a specific character in the string/slice multiple times, assign it to a variable and use the variable instead. This will avoid indexing into the slice/string multiple times.</li>
<li>If there are multiple conditions in an <code>if</code> statement, try to move (or add) the non-indexing checks to the front of the statement. This sometimes will help short circuit the checks and avoid the slice-indexing checks.</li>
</ol>

<p>One might question if this is worth optimizing, but like I said, I am trying to squeeze every oz of performance so 3.5% is still good for me. Unfornately I do know I won&rsquo;t get 3.5% since I can&rsquo;t remove every operation that index into slice/string.</p>

<p><strong>3. Reduce Heap Allocation</strong></p>

<p>This is true for all languages (where you can have some control of stack vs heap allocation), and it&rsquo;s even more true in Go. Mainly in Go, if you allocate a new slice, Go will &ldquo;zero&rdquo; out the allocated memory. This is great from a safety perspective, but it does add to the overhead.</p>

<p>As an example, in the scanner I originally allocated a new <em>Sequence</em> (slice of <em>Token</em>) for every new message. However, when i changed it to re-use the existing slice, the performance increased by over 10%!</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.87 secs, ~ 246027.12 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 275038.83 msgs/sec
</code></pre>

<p>The best thing to do is to run Go&rsquo;s builtin CPU profiler, and look at the numbers for Go internal functions such as <code>runtime.makeslice</code>, <code>runtime.markscan</code>, and <code>runtime.memclr</code>. Large percentages and numbers for these internal functions are dead giveaway that you are probably allocating too much stuff on the heap.</p>

<p>I religiously go through the SVGs generated from the Go profiler to help me identify hot spots where I can optimize.</p>

<p>Here&rsquo;s also a couple of tips I picked up from the <a href="https://groups.google.com/forum/#!topic/golang-nuts/baU4PZFyBQQ">go-nuts mailing list</a>:</p>

<ul>
<li>Maps are bad&ndash;even if they&rsquo;re storing integers or other non-pointer structs. The implementation appears to have lots of pointers inside which must be evaluated and followed during mark/sweep GC.  Using structures with pointers magnifies the expense.</li>
<li>Slices are surprisingly bad (including strings and substrings of existing strings). A slice is a pointer to the backing array with a length and capacity. It would appear that the internal pointer that causes the trouble because GC wants to inspect it.</li>
</ul>

<p><strong>4. Reduce Data Copying</strong></p>

<p>Data copying is expensive. It means the run time has to allocate new space and copy the data over. It&rsquo;s even more expensive when you can&rsquo;t have do <code>memcpy</code> of a slice in Go like you can in C. Again, direct memory copying is not Go&rsquo;s design goal. It is also much safer if you can prevent users from playing with memory directly too much. However, it is still a good idea to avoid any copying of data, whether it&rsquo;s string or slice.</p>

<p>As much as I can, I try to do in place processing of the data. Every <em>Sequence</em> is worked on locally and I try not to copy <em>Sequence</em> or string unless I absolutely have to.</p>

<p>Unfortunately I don&rsquo;t have any comparison numbers for this one, because I learned from <a href="http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/">previous projects</a> that I should avoid copying as much as possible.</p>

<p><strong>5. Mind the Data Struture</strong></p>

<p>If there&rsquo;s one thing I learned over the past year, is to use the right data structure for the right job. I&rsquo;ve written about other data structures such as <a href="http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/">ring buffer</a>, <a href="http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/">bloom filters</a>, and <a href="http://zhen.org/blog/go-skiplist/">skiplist</a> before.</p>

<p>However, <a href="http://en.wikipedia.org/wiki/Finite-state_machine">finite-state automata or machine</a> is my latest love and I&rsquo;ve been using it at various projects such as my <a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">porter2</a> and <a href="https://github.com/surge/xparse/tree/master/etld">effective TLD</a>. Ok, technical FSM itself is not a data structure and can be implemented in different ways. In the <code>sequence</code> project, I used both a tree representation as well as a bunch of switch-case statements. For the <a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">porter2</a> FSMs, I used switch-case to implement them.</p>

<p>Interestingly, swtich-case doesn&rsquo;t always win. I tested the time FSM using both tree and switch-case implementations, and the tree actually won out. (Below, 1 is tree, 2 is switch-case.) So guess which one is checked in?</p>

<pre><code>BenchmarkTimeStep1   2000000         696 ns/op
BenchmarkTimeStep2   2000000         772 ns/op
</code></pre>

<p>Writing this actually reminds me that in the parser, I am currently using a tree to parse the sequences. While parsing, there could be multiple paths that the sequence will match. Currently I walk all the matched paths fully, before choosing one that has the highest score. What I should do is to do a weighted walk, and always walk the highest score nodes first. If at the end I get a perfect score, I can just return that path and not have to walk the other paths. (Note to self, more parser optimization to do).</p>

<p><strong>6. Avoid Interfaces If Possible</strong></p>

<p>This is probably not a great advice to give to Go developers. Interface is probably one of the best Go features and everyone should learn to use it. However, if you want high performane, avoid interfaces as it provides additional layers of indirection. I don&rsquo;t have performance numbers for the <code>sequence</code> project since I tried to avoid interfaces in high performance areas from the start. However, previous in the <a href="http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/">ring buffer</a> project, the version that uses interface is 140% slower than the version that didn&rsquo;t.</p>

<p>I don&rsquo;t have the direct link but someone on the go-nuts mailing list also said:</p>

<blockquote>
<p>If you really want high performance, I would suggest avoiding interfaces and, in general, function calls like the plague, since they are quite expensive in Go (compared to C). We have implemented basically the same for our internal web framework (to be released some day) and we&rsquo;re almost 4x faster than encoding/json without doing too much optimization. I&rsquo;m sure we could make this even faster.</p>
</blockquote>

<p><strong>7. Find Ways to Short Circuit Checks</strong></p>

<p>Find ways to quickly eliminate the need to run a section of the code has been tremendously helpful to improve performance. For example, here are a couple of place where I tried to do that.</p>

<p>In <a href="https://github.com/strace/sequence/blob/master/scanner.go#L223">this first example</a>, I simply added <code>l == 1</code> before the actual equality check of the string values. The first output is before the add, the second is after. The difference is about 2% performance increase.</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272303.79 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 278433.34 msgs/sec
</code></pre>

<p>In <a href="https://github.com/strace/sequence/blob/master/scanner.go#L282">the second example</a>, I added a quick check to make sure the remaining string is at least as long as the shortest time format. If there&rsquo;s not enough characters, then don&rsquo;t run the time FSM. The performance difference is about 2.5%.</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272059.04 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 279388.47 msgs/sec
</code></pre>

<p>So by simply adding a couple of checks, I&rsquo;ve increased perfromance by close to 5%.</p>

<h2 id="conclusion:cb54f18a9944e1962d3fe8e3f09ea809">Conclusion</h2>

<p>At this point I think I have squeezed every bit of performance out of the scanner, to the extend of my knowledge. It&rsquo;s performing relatively well and it&rsquo;s given the parser plenty of head room to do other things. I hope some of these lessons are helpful to whatever you are doing.</p>

<p>Feel free to take a look at the <a href="https://github.com/strace/sequence">sequence</a> project and try it out if you. If you have any issues/comments, please don&rsquo;t hestiate to <a href="https://github.com/strace/sequence/issues">open a github issue</a>.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">
        Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns
      </a>
    </h1>

    <span class="post-date">Tue, Feb 10, 2015</span>

    

<p><a href="http://godoc.org/github.com/strace/sequence"><img src="http://godoc.org/github.com/strace/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="http://godoc.org/github.com/strace/sequence/cmd/sequence"><img src="http://godoc.org/github.com/strace/sequence/cmd/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="https://github.com/strace/sequence">github repo</a></p>

<p>This is part 2 of the <code>sequence</code> series. [</p>

<ul>
<li><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Part 1</a> is about the high performance parser that can parse 100,000-200,000 MPs.</li>
<li><a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Part 2</a> is about automating the process of reducing 100 of 1000&rsquo;s of log messages down to dozens of unique patterns.</li>
<li><a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">Part3</a> is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size) for scanning and tokenizing log messages</li>
</ul>

<h2 id="background:61b1adc09fb9bc31a28d4faabfef3631">Background</h2>

<p>This post really takes me down the memory lane. Back in 2005, while I was at LogLogic, we envisioned an automated approach to tagging, or labeling, log messages. More specifically, we wanted to automatically tag specific components within the log messages with their semantic label, such as a source IP address, or a target user.</p>

<p>At the time, much like it is still today, the message parsing process is performed manually. This means someone has to manually look at the object and decided that the object should be labeled “user” or “targetUser.” An  analyst has to go through the log data, create a regular expression that extracts the useful strings out, and then finally assigning these to a specific label. This is extremely time consuming and error-prone.</p>

<p>At that time, the vision was to provide an automated approach to universally parse and analyze ANY log data. The key phrase being “automated approach.” This means the users should only need to provide minimum guidance to the system, if any, for the platforms to be able to analyze the log data. LogLogic never did much with this, unfortunately.</p>

<p>However, the tagging concept was later on adopted by (and I know how this got into CEE :) the <a href="http://cee.mitre.org/">Common Event Expression, or CEE</a> effort by Mitre. This idea of tags also inspired <a href="http://www.liblognorm.com/">liblognorm</a> to develop their <a href="http://www.libee.org/">libee</a> library and <a href="http://www.liblognorm.com/news/log-classification-with-liblognorm/">tagging system</a>. Rsyslog&rsquo;s <a href="http://www.rsyslog.com/doc/mmnormalize.html">mmnormalize</a> module is based on liblognorm.</p>

<p>And then there&rsquo;s Fedora&rsquo;s <a href="https://fedorahosted.org/lumberjack/">Project Lumberjack</a>, which &ldquo;is an open-source project to update and enhance the event log architecture&rdquo; and &ldquo;aims to improve the creation and standardize the content of event logs by implementing the concepts and specifications proposed by the ​Common Event Expression (CEE).&rdquo;</p>

<p>Then finally <a href="http://logstash.net/">logstash</a> has their <a href="http://logstash.net/docs/1.4.2/filters/grok">grok filter</a> that basically does similar extraction of unstructured data into a structured and queryable format. However, it seems like there might be some <a href="http://ghost.frodux.in/logstash-grok-speeds/">performance bottlenecks</a>.</p>

<p>However, none of these efforts attempted to solve the automated tagging/labeling problem. They mostly just try to provide a parser for log messages.</p>

<p>Also, it looks like many of these efforts have all been abandoned or put in hibernation, and haven&rsquo;t been updated since 2012 or 2013. liblognrom did put out <a href="http://www.liblognorm.com/news/">a couple of updates</a> in the past couple of years. Logstash&rsquo;s grok obviously is being maintained and developed with the <a href="http://www.elasticsearch.com/">Elasticsearch</a> backing.</p>

<p>It is understandable, unfortunately. Log parsing is <strong>BORING</strong>. I mean, who wants to sit there and stare at logs all day and try to come up with regular expressions or other types of parsing rules? LogLogic used to have a team of LogLabs analysts that did that, and I have to say I truly appreciated their effort and patience, because I cannot do that.</p>

<h3 id="the-end-result:61b1adc09fb9bc31a28d4faabfef3631">The End Result</h3>

<p>So instead of writing rules all day long, I decided to create an analyzer that can help us get at least 75% of the way there. The end result is the <code>Analyzer</code>, written in <a href="http://golang.org">Go</a>, in the <a href="https://github.com/strace/sequence">sequence</a> project I created. Here are some preliminary results. Below, we analyzed 2 files. The first is a file with over 200,000 sshd messages. The second is a file with a mixture of ASA, sshd, sudo and su log messages. It contains almost 450,000 messages.</p>

<p>By running the analyzer over these logs, the pure sshd log file returned 45 individual patterns, and the second returned 103 unique patterns.</p>

<pre><code>$ go run sequence.go analyze -i ../../data/sshd.all -o sshd.analyze
Analyzed 212897 messages, found 45 unique patterns, 45 are new.

$ go run sequence.go analyze -i ../../data/asasshsudo.log -o asasshsudo.analyze
Analyzed 447745 messages, found 103 unique patterns, 103 are new.
</code></pre>

<p>And the output file has entries such as:</p>

<pre><code>%msgtime% %apphost% %appname% [ %sessionid% ] : %status% %method% for %srcuser% from %srcipv4% port %srcport% ssh2
# Jan 15 19:39:26 irc sshd[7778]: Accepted password for jlz from 108.61.8.124 port 57630 ssh2

%msgtime% %appipv4% %appname% : %action% outbound %protocol% connection %sessionid% for %string% : %srcipv4% / %srcport% ( %ipv4% / %integer% ) to %string% : %dstipv4% / %dstport% ( %ipv4% / %integer% )
# 2012-04-05 18:46:18   172.23.0.1  %ASA-6-302013: Built outbound TCP connection 1424575 for outside:10.32.0.100/80 (10.32.0.100/80) to inside:172.23.73.72/2522 (10.32.0.1/54702)

%msgtime% %apphost% %appname% : %string% : tty = %string% ; pwd = %string% ; user = %srcuser% ; command = %command% - %string%
# Jan 15 14:09:11 irc sudo:    jlz : TTY=pts/1 ; PWD=/home/jlz ; USER=root ; COMMAND=/bin/su - irc
</code></pre>

<p>As you can see, the output is not 100%, but it gets us pretty close. Once the analyst goes through and updates the rules, he/she can re-run the analyzer anytime with any file to determine if there&rsquo;s new patterns. For example, below, we ran the sshd log file with an existing pattern file, and got 4 new log patterns.</p>

<pre><code>$ go run sequence.go analyze -i ../../data/sshd.all -p ../../patterns/sshd.txt -o sshd.analyze
Analyzed 212897 messages, found 39 unique patterns, 4 are new.
</code></pre>

<h3 id="parser-quick-review:61b1adc09fb9bc31a28d4faabfef3631">Parser - Quick Review</h3>

<p>I wrote about the <a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">sequence parser</a> a couple of weeks back. It is a <em>high performance sequential log parser</em>. It <em>sequentially</em> goes through a log message, <em>parses</em> out the meaningful parts, without the use regular expressions. It can achieve <em>high performance</em> parsing of <strong>100,000 - 200,000 messages per second (MPS)</strong> without the need to separate parsing rules by log source type. Underneath the hood, the <code>sequence</code> parser basically constructs a tree based on the sequential rules, walks the tree to identify all the possible paths, and returns the path that has the best match (highest weight) for the message.</p>

<p>While the analyzer is about reducing a large corupus of raw log messages down to a small set of unique patterns, the parser is all about matching log messages to an existing set of patters and determining whether a specific pattern has matched. Based on the pattern, it returns a sequence of tokens that basically extracts out the important pieces of information from the logs. The analysts can then take this sequence and perform other types of analysis.</p>

<p>The approach taken by the <code>sequence</code> parser is pretty much the same as liblognorm or other tree-based approaches.</p>

<h2 id="sequence-analyzer:61b1adc09fb9bc31a28d4faabfef3631">Sequence Analyzer</h2>

<p>In the following section I will go through additional details of how the <code>sequence</code> analyzer reduces 100 of 1000&rsquo;s of raw log messages down to just 10&rsquo;s of unique patterns, and then determining how to label the individual tokens.</p>

<h3 id="identifying-unique-patterns:61b1adc09fb9bc31a28d4faabfef3631">Identifying Unique Patterns</h3>

<p>Analyzer builds an analysis tree that represents all the Sequences from messages. It can be used to determine all of the unique patterns for a large body of messages.</p>

<p>It&rsquo;s based on a single basic concept, that for multiple log messages, if tokens in the same position shares one same parent and one same child, then the tokens in that position is likely variable string, which means it&rsquo;s something we can extract. For example, take a look at the following two messages:</p>

<pre><code>Jan 12 06:49:42 irc sshd[7034]: Accepted password for root from 218.161.81.238 port 4228 ssh2
Jan 12 14:44:48 jlz sshd[11084]: Accepted publickey for jlz from 76.21.0.16 port 36609 ssh2
</code></pre>

<p>The first token of each message is a timestamp, and the 3rd token of each message is the literal &ldquo;sshd&rdquo;. For the literals &ldquo;irc&rdquo; and &ldquo;jlz&rdquo;, they both share a common parent, which is a timestamp. They also both share a common child, which is &ldquo;sshd&rdquo;. This means token in between these, the 2nd token in each message, likely represents a variable token in this message type. In this case, &ldquo;irc&rdquo; and &ldquo;jlz&rdquo; happens to
represent the syslog host.</p>

<p>Looking further down the message, the literals &ldquo;password&rdquo; and &ldquo;publickey&rdquo; also share a common parent, &ldquo;Accepted&rdquo;, and a common child, &ldquo;for&rdquo;. So that means the token in this position is also a variable token (of type TokenString).</p>

<p>You can find several tokens that share common parent and child in these two messages, which means each of these tokens can be extracted. And finally, we can determine that the single pattern that will match both is:</p>

<pre><code>%time% %string% sshd [ %integer% ] : Accepted %string% for %string% from %ipv4% port %integer% ssh2
</code></pre>

<p>If later we add another message to this mix:</p>

<pre><code>Jan 12 06:49:42 irc sshd[7034]: Failed password for root from 218.161.81.238 port 4228 ssh2
</code></pre>

<p>The Analyzer will determine that the literals &ldquo;Accepted&rdquo; in the 1st message, and &ldquo;Failed&rdquo; in the 3rd message share a common parent &ldquo;:&rdquo; and a common child &ldquo;password&rdquo;, so it will determine that the token in this position is also a variable token. After all three messages are analyzed, the final pattern that will match all three
messages is:</p>

<pre><code>%time% %string% sshd [ %integer% ] : %string% %string% for %string% from %ipv4% port %integer% ssh2
</code></pre>

<p>By applying this concept, we can effectively identify all the unique patterns in a log file.</p>

<h3 id="determining-the-correct-labels:61b1adc09fb9bc31a28d4faabfef3631">Determining the Correct Labels</h3>

<p>Now that we have the unique patterns, we will scan the tokens to determine which labels we should apply to them.</p>

<p>System and network logs are mostly free form text. There&rsquo;s no specific patterns to any of them. So it&rsquo;s really difficult to determine how to label specific parts of the log message automatically. However, over the years, after looking at so many system and network log messages, some patterns will start to emerge.</p>

<p>There&rsquo;s no &ldquo;machine learning&rdquo; here. This section is all about codifying these human learnings. I&rsquo;ve created the following 6 rules to help label tokens in the log messages. By no means are these rules perfect. They are at best just guesses on how to label. But hopefully they can get us 75% of the way there and we human can just take it the rest of the way.</p>

<p><strong>0. Parsing Email and Hostname Formats</strong></p>

<p>This is technically not a labeling step. Before we actually start the labeling process, we wanted to first parse out a couple more formats like email and host names. The message tokenizer doesn&rsquo;t recognize these because they are difficult to parse and will slow down the tokenizer. These specific formats are also not needed by the parser. So because the analyzer doesn&rsquo;t care about performance as much, we can do this as post-processing step.</p>

<p>To recognize the hostname, we try to match the &ldquo;effective TLD&rdquo; using the <a href="https://github.com/surge/xparse/tree/master/etld">xparse/etld</a> package. It is an effective TLD matcher that returns the length of the effective domain name for the given string. It uses the data set from <a href="https://www.publicsuffix.org/list/effective_tld_names.dat">https://www.publicsuffix.org/list/effective_tld_names.dat</a>.</p>

<p><strong>1. Recognizing Syslog Headers</strong></p>

<p>First we will try to see if we can regonize the syslog headers. We try to recogize both RFC5424 and RFC3164 syslog headers:</p>

<pre><code>	// RFC5424
	// - &quot;1 2003-10-11T22:14:15.003Z mymachine.example.com evntslog - ID47 ...&quot;
	// - &quot;1 2003-08-24T05:14:15.000003-07:00 192.0.2.1 myproc 8710 - ...&quot;
	// - &quot;1 2003-10-11T22:14:15.003Z mymachine.example.com su - ID47 ...&quot;
	// RFC3164
	// - &quot;Oct 11 22:14:15 mymachine su: ...&quot;
	// - &quot;Aug 24 05:34:00 CST 1987 mymachine myproc[10]: ...&quot;
	// - &quot;jan 12 06:49:56 irc last message repeated 6 times&quot;
</code></pre>

<p>If the sequence pattern matches any of the above sequence, then we assume the first few tokens belong to the syslog header.</p>

<p><strong>2. Marking Key and Value Pairs</strong></p>

<p>The next step we perform is to mark known &ldquo;keys&rdquo;. There are two types of keys. First, we identify any token before the &ldquo;=&rdquo; as a key. For example, the message <code>fw=TOPSEC priv=6 recorder=kernel type=conn</code> contains 4 keys: <code>fw</code>, <code>priv</code>, <code>recorder</code> and <code>type</code>. These keys should be considered string literals, and should not be extracted. However, they can be used to determine how the value part should be labeled.</p>

<p>The second types of keys are determined by keywords that often appear in front of other tokens, I call these <strong>prekeys</strong>. For example, we know that the prekey <code>from</code> usually appears in front of any source host or IP address, and the prekey <code>to</code> usually appears in front of any destination host or IP address. Below are some examples of these prekeys.</p>

<pre><code>from 		= [ &quot;%srchost%&quot;, &quot;%srcipv4%&quot; ]
port 		= [ &quot;%srcport%&quot;, &quot;%dstport%&quot; ]
proto		= [ &quot;%protocol%&quot; ]
sport		= [ &quot;%srcport%&quot; ]
src 		= [ &quot;%srchost%&quot;, &quot;%srcipv4%&quot; ]
to 			= [ &quot;%dsthost%&quot;, &quot;%dstipv4%&quot;, &quot;%dstuser%&quot; ]
</code></pre>

<p>To help identify these prekeys, I wrote a quick program that goes through many of the logs I have to help identify what keywords appears before IP address, mac addresses, and other non-literal tokens. The result is put into the <a href="https://github.com/strace/sequence/blob/master/keymaps.go">keymaps.go</a> file. It&rsquo;s not comprehensive, but it&rsquo;s also not meant to be. We just need enough hints to help with labeling.</p>

<p><strong>3. Labeling &ldquo;Values&rdquo; by Their Keys</strong></p>

<p>Once the keys are labeled, we can label the values based on the mapping described above. For key/value pairs, we try to recognize both <code>key=value</code> or <code>key=&quot;value&quot;</code> formats (or other quote characters like &lsquo; or &lt;).</p>

<p>For the prekeys, we try to find the value token within 2 tokens of the key token. That means sequences such as <code>from 192.168.1.1</code> and <code>from ip 192.168.1.1</code> will identify <code>192.168.1.1</code> as the <code>%srcipv4%</code> based on the above mapping, but we will miss <code>from ip address 192.168.1.1</code>.</p>

<p><strong>4. Identifying Known Keywords</strong></p>

<p>Within most log messages, there are certain keywords that would indicate what actions were performed, what the state/status of the action was, and what objects the actions were performed on. CEE had a list that it identified, so I copied the list and added some of my own.</p>

<pre><code>action = [
	&quot;access&quot;,
	&quot;alert&quot;,
	&quot;allocate&quot;,
	&quot;allow&quot;,
	.
	.
	.
]

status = [
	&quot;accept&quot;,
	&quot;error&quot;,
	&quot;fail&quot;,
	&quot;failure&quot;,
	&quot;success&quot;
]

object = [
	&quot;account&quot;,
	&quot;app&quot;,
	&quot;bios&quot;,
	&quot;driver&quot;,
	.
	.
	.
]
</code></pre>

<p>In our labeling process, we basically goes through and identify all the string literals that are NOT marked as keys, and perform a <a href="https://github.com/surge/porter2">porter2 stemming operation</a> on the literal, then compare to the above list (which is also porter2 stemmed).</p>

<p>If a literal matches one of the above lists, then the corresponding label (<code>action</code>, <code>status</code>, <code>object</code>, <code>srcuser</code>, <code>method</code>, or <code>protocol</code>) is applied.</p>

<p><strong>5. Determining Positions of Specific Types</strong></p>

<p>In this next step, we are basically looking at the position of where some of the token types appear. Specifically, we are looking for <code>%time%</code>, <code>%url%</code>, <code>%mac%</code>, <code>%ipv4%</code>, <code>%host%</code>, and <code>%email%</code> tokens. Assuming the labels have not already been taken with the previous rules, the rules are as follows:</p>

<ul>
<li>The first %time% token is labeled as %msgtime%</li>
<li>The first %url% token is labeled as %object%</li>
<li>The first %mac% token is labeled as %srcmac% and the second is labeld as %dstmac%</li>
<li>The first %ipv4% token is labeled as %srcipv4% and the second is labeld as %dstipv4%</li>
<li>The first %host% token is labeled as %srchost% and the second is labeld as %dsthost%</li>
<li>The first %email% token is labeled as %srcemail% and the second is labeld as %dstemail%</li>
</ul>

<p><strong>6. Scanning for ip/port or ip:port Pairs</strong></p>

<p>Finally, after all that, we scan through the sequence again, and identify any numbers that follow an IP address, but separated by either a &ldquo;/&rdquo; or &ldquo;:&ldquo;. Then we label these numbers as either <code>%srcport%</code> or <code>%dstport%</code> based on how the previous IP address is labeled.</p>

<h3 id="summary:61b1adc09fb9bc31a28d4faabfef3631">Summary</h3>

<p>There are some limitations to the <code>sequence</code> parser and analyzer. For example, currently <code>sequence</code> does not handle multi-line logs. Each log message must appear as a single line. So if there&rsquo;s multi-line logs, they must be first be converted into a single line. Also, <code>sequence</code> has been only tested with a limited set of system (Linux, AIX, sudo, ssh, su, dhcp, etc etc), network (ASA, PIX, Neoteris, CheckPoint, Juniper Firewall) and infrastructure application (apache, bluecoat, etc) logs.</p>

<p>Documentation is available at godoc: <a href="http://godoc.org/github.com/strace/sequence">package</a>, <a href="http://godoc.org/github.com/strace/sequence/cmd/sequence">command</a>.</p>

<p>There are some pattern files developed for ASA, Sudo and SSH in the <code>patterns</code> directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages.</p>

<p>If you have a set of logs you would like me to test out, please feel free to <a href="https://github.com/strace/sequence/issues">open an issue</a> and we can arrange a way for me to download and test your logs.</p>

<p>Stay tuned for more log patterns&hellip;</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/papers-i-read-2015-week-6/">
        Papers I Read: 2015 Week 6
      </a>
    </h1>

    <span class="post-date">Sun, Feb 8, 2015</span>

    

<p><a href="http://paperswelove.org/">Papers We Love</a> has been making rounds lately and a lot of people are excited about it. I also think it&rsquo;s kind of cool since I&rsquo;ve been reading a lot of research papers over the past year or so. I have been killing some trees because of that.</p>

<p>My interests have been mostly around data analytics, but the specific focus areas have changed a few times. I have read papers on data structures (bloom filters, skiplist, bitmap compression, etc), security analytics, consumer behavioral analysis, loyalty analytics, and now back to security analytics. In fact, recently I started reading a few security research papers that I found on <a href="http://www.covert.io/">covert.io</a>, put together by Jason Trost.</p>

<p>In any case, I thought it might be an interesting idea to share some of the papers I read/scan/skim on weekly basis. This way I can also track what I read over time.</p>

<h3 id="random-ramblings:e2137c343047358d9912399632750231">Random Ramblings</h3>

<p>This week has been a disaster. I was the last one in the family to catch the cold, but probably lasted the longest. In fact I am still only about 50%. This whole week I have been having headaches, body aches, and haven&rsquo;t been able to concentrate. My body must be trying to catch up on sleep or something. For a couple days I actually slept for almost 12 hours a night!</p>

<p>I&rsquo;ve been meaning to work on <a href="https://github.com/strace/sequence">sequence</a> and finish updating the analyzer, but really had a hard time concentrating. Any non-working hours are basically spent in bed if I could.</p>

<p>So this is probably the worst week to start the &ldquo;Papers I Read&rdquo; series since I only technically read 1 paper. But I am going to cheat a little, and list the papers I read over the past couple of weeks, pretty much all in my spare time.</p>

<p>This week we also saw Sony&rsquo;s accouncement that last year&rsquo;s hack cost them <a href="http://www.sony.net/SonyInfo/IR/financial/fr/150204_sony.pdf">$15 million</a> to investigate and remediate. It&rsquo;s pretty crazy if you think about it.</p>

<p>Let&rsquo;s assume that they hired a bunch of high-priced consultants, say $250/hour, to help comb through the logs and clean the systems. And let&rsquo;s say <sup>2</sup>&frasl;<sub>3</sub> of the $15m is spent on these consultants. That&rsquo;s <code>$10m / $250 = 40,000 hours</code>.</p>

<p>Let&rsquo;s say these consultants worked full time, non-stop, no weekends, no breaks, for 2 months since the announcement on Nov 24, 2014, that would be a team of 56 people (<code>40,000 hours / 60 days / 12 hours/day = 56</code>) working 12 hour days!</p>

<p>I&rsquo;ll tell ya, these security guys are raking it in. They make money upfront by selling products/services to protect the company, then they make money in the back by selling forensic services to clean up after the hack.</p>

<p>[Disclaimer: any mistake in my calculations/assumptions I blame on my drugged brain cells.]</p>

<h3 id="papers-i-read:e2137c343047358d9912399632750231">Papers I Read</h3>

<ul>
<li><a href="http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf">Beehive: Large-Scale Log Analysis for Detecting Suspicious Activity in Enterprise Networks</a></li>
</ul>

<blockquote>
<p>We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents.</p>
</blockquote>

<ul>
<li><a href="http://minds.cs.umn.edu/publications/chapter.pdf">Data Mining for Cyber Security</a></li>
</ul>

<blockquote>
<p>This chapter provides an overview of the Minnesota Intrusion Detection System (MINDS), which uses a suite of data mining based algorithms to address different aspects of cyber security. The various components of MINDS such as the scan detector, anomaly detector and the profiling module detect different types of attacks
and intrusions on a computer network.</p>
</blockquote>

<ul>
<li><a href="http://www.covert.io/research-papers/security/VAST-%20Network%20Visibility%20Across%20Space%20and%20Time.pdf">VAST: Network Visibility Across Space and Time</a></li>
</ul>

<blockquote>
<p>Key operational networking tasks, such as troubleshooting and defending against attacks, greatly benefit from attaining views of network activity that are unified across space and time. This means that data from heterogeneous devices and systems is treated in a uniformfashion, and that analyzing past activity and detecting future instances follow the same procedures. Based on previous ideas that formulated principles for comprehensive
network visibility [AKP+08], we present the design and architecture of Visibility Across Space and Time (VAST), an intelligent database that serves as a single vantage point into the network. The system is based on a generic event model to handle network data from disparate sources and provides a query architecture that allows operators or remote applications to extract events matching a given condition. We implemented a proof-of-principle prototype that can archive and index events from a wide range of sources. Moreover, we conducted a preliminary performance evaluation to verify that our implementation works efficient and as expected.</p>
</blockquote>

<ul>
<li><a href="http://www.covert.io/research-papers/security/Finding%20The%20Needle-%20Suppression%20of%20False%20Alarms%20in%20Large%20Intrusion%20Detection%20Data%20Sets.pdf">Finding The Needle: Suppression of False Alarms in Large Intrusion Detection Data Sets</a></li>
</ul>

<blockquote>
<p>Managed security service providers (MSSPs) must manage and monitor thousands of intrusion detection sensors.
The sensors often vary by manufacturer and software version, making the problem of creating generalized tools to separate true attacks from false positives particularly difficult. Often times it is useful from an operations perspective to know if a particular sensor is acting out of character. We propose a solution to this problem using anomaly detection techniques over the set of alarms produced by the sensors. Similar to the manner in which an anomaly based sensor detects deviations from normal user or system behavior, we establish the baseline
behavior of a sensor and detect deviations from this baseline. We show that departures from this profile by a sensor have a high probability of being artifacts of genuine attacks. We evaluate a set of time-based Markovian heuristics against a simple compression algorithm and show that we are able to detect the existence of all attacks which were manually identified by security personnel, drastically reduce the number of false positives, and identify attacks which were overlooked during manual evaluation.</p>
</blockquote>

<ul>
<li><a href="http://user.informatik.uni-goettingen.de/~krieck/docs/2013a-aisec.pdf">A Close Look on n-Grams in Intrusion Detection: Anomaly Detection vs. Classification</a></li>
</ul>

<blockquote>
<p>Detection methods based on n-gram models have been widely studied for the identification of attacks and malicious software. These methods usually build on one of two learning schemes: anomaly detection, where a model of normality is constructed from n-grams, or classification, where a discrimination between benign and malicious n-grams is learned. Although successful in many security domains, previous work falls short of explaining why a particular scheme is used and more importantly what renders one favorable over the other for a given type of data. In this paper we provide a close look on n-gram models for intrusion detection. We specifically study anomaly detection and classification using n-grams and develop criteria for data being used in one or the other
scheme. Furthermore, we apply these criteria in the scope of web intrusion detection and empirically validate their effectiveness with different learning-based detection methods for client-side and service-side attacks.</p>
</blockquote>

<ul>
<li><a href="http://blog.scalyr.com/2014/05/searching-20-gbsec-systems-engineering-before-algorithms/">Searching 20 GB/sec: Systems Engineering Before Algorithms</a></li>
</ul>

<p>Ok, this is a blog post, not a research paper, but it&rsquo;s somewhat interesting nonetheless.</p>

<blockquote>
<p>This article describes how we met that challenge using an “old school”, brute-force approach, by eliminating layers and avoiding complex data structures. There are lessons here that you can apply to your own engineering challenges.</p>
</blockquote>

  </div>
  
</div>

<div class="posts">
  <h1 class="post-title">Archive</h1>
  <ul class="posts">
      
      <li><span><a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">Sequence: Optimizing Go For the High Performance Log Scanner</a> - <time class="pull-right post-list">Fri, Feb 13, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns</a> - <time class="pull-right post-list">Tue, Feb 10, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/papers-i-read-2015-week-6/">Papers I Read: 2015 Week 6</a> - <time class="pull-right post-list">Sun, Feb 8, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS</a> - <time class="pull-right post-list">Sun, Feb 1, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">Generating Porter2 FSM For Fun and Performance in Go</a> - <time class="pull-right post-list">Wed, Jan 21, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/golang-from-a-non-programmers-perspective/">Go: From a Non-Programmer&#39;s Perspective</a> - <time class="pull-right post-list">Tue, Jan 13, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">PingMQ: A SurgeMQ-based ICMP Monitoring Tool</a> - <time class="pull-right post-list">Thu, Dec 25, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">SurgeMQ: High Performance MQTT Server and Client Libraries in Go</a> - <time class="pull-right post-list">Wed, Dec 24, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/">SurgeMQ: MQTT Message Queue @ 750,000 MPS</a> - <time class="pull-right post-list">Thu, Dec 4, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/graceful-shutdown-of-go-net-dot-listeners/">Graceful Shutdown of Go net.Listeners</a> - <time class="pull-right post-list">Thu, Dec 12, 2013</h4></time></span></li>
      
  </ul>
</div>

</div>

  </body>

</html>
