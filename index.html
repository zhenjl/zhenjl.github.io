<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Zen 3.1 &middot; Zen 3.1 </title>

  
  <link rel="stylesheet" href="http://zhen.org/css/poole.css">
  <link rel="stylesheet" href="http://zhen.org/css/syntax.css">
  <link rel="stylesheet" href="http://zhen.org/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://zhen.org/index.xml/" rel="alternate" type="application/rss+xml" title="Zen 3.1" />
</head>
<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a href="http://zhen.org">Zen 3.1</a></h1>
      <p class="lead">
       Product. Data. Code 
      </p>
    </div>



    <ul class="sidebar-nav">
      <li><a href="http://zhen.org/blog">Posts</a></li>
      
    </ul>
      <a href="https://twitter.com/zhenjl"><i class="fa fa-twitter-square"></i></a>&nbsp;&nbsp;
      
      
      <a href="https://github.com/zhenjl"><i class="fa fa-github-square"></i></a>&nbsp;&nbsp;
      

    <p class="footnote">Powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    Theme originally made by <a href="http://twitter.com/mdo">@mdo</a> <br/>
    Theme modified by <a href="http://npf.io">Nate Finch</a> <br/>
    &copy; 2014 Jian Zhen. All rights reserved.</p>
    
  </div>
</div>

    <div class="content container">
<div class="posts">

      
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/graceful-shutdown-of-go-net-dot-listeners/">
        Graceful Shutdown of Go net.Listeners
      </a>
    </h1>

    <span class="post-date">Thu, Dec 12, 2013</span>

    <p>Comments/Feedbacks at <a href="https://news.ycombinator.com/item?id=6899568">Hacker News</a>, <a href="http://www.reddit.com/r/golang/comments/1ss929/graceful_shutdown_of_go_netlisteners/">Reddit</a></p>

<p>The past few evenings I&rsquo;ve been working on a Go program that listens on a TCP socket, accepts new connections, processes some data and then return some data to the client. This is actually fairly simple in Go. The <a href="http://golang.org/pkg/net">top</a> of the <em>net</em> package has some fairly simple examples of how to do that. Another example can be found about <a href="http://golang.org/pkg/net/#example_Listener">mid-section</a> of the page. You can also find a TON of examples online that shows you how to build a simple TCP listener.</p>

<p>However, one thing I noticed in all these examples is that none of them shows you how to gracefully shutdown the TCP listener. Most of the examples expect to program to exit so there&rsquo;s no need to clean up anything. However, if you have a program that&rsquo;s a long-running server, and need to, for whatever reason, need to shutdown the TCP listener, you will need to clean up after yourself. Otherwise you may leave a bunch of goroutines behind unintentionally.</p>

<p>Another reason I wanted a way to shutdown the TCP listener is I want to be able to start a listener in my tests, then start up a bunch of clients, test some stuff, then afterwards shutdown the server. Then I can start another listener in another test for some other tests.</p>

<p>After some help from jhoto and foobaz on the #go-nuts IRC channel, I wrote the following example to demonstrate the graceful shutdown approach.</p>

<p>The basic idea is to leverage a quit channel to tell the Accept() goroutine that it&rsquo;s time to quit. Using quit channel is a fairly common practice in Go. However, in this case, the Accept() call is blocking waiting for new connections, so closing the quit channel won&rsquo;t have any effect unless the goroutine actually checks it. So to force Accept() to return from blocking, we can close the net.Listener.</p>

<p>The order of the operation matters somewhat. We will want to first close the quit channel, then close the net.Listener. If we reverse the order, you will likely see a few more errors from the Accept() call.</p>

<p>The netgrace_test.go file below shows an example of how to use the quit channel to help gracefully shutdown net.Listeners.</p>

<p>Hopefully you will find this tip useful. You can find it as a <a href="https://gist.github.com/zhenjl/7940977">gist</a> as well.</p>

<script src="https://gist.github.com/zhenjl/7940977.js"></script>

<p>Comments/Feedbacks at <a href="https://news.ycombinator.com/item?id=6899568">Hacker News</a>, <a href="http://www.reddit.com/r/golang/comments/1ss929/graceful_shutdown_of_go_netlisteners/">Reddit</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/">
        Ring Buffer - Variable-Length, Low-Latency, Lock-Free, Disruptor-Style
      </a>
    </h1>

    <span class="post-date">Sat, Nov 30, 2013</span>

    

<p>Comments/feedbacks on <a href="https://news.ycombinator.com/item?id=6831293">Hacker News</a>, <a href="http://www.reddit.com/r/golang/comments/1rvvb6/ring_buffer_variablelength_lowlatency_lockfree/">Reddit</a></p>

<blockquote>
<p>2013-12-04 Update #1: Read a <a href="https://groups.google.com/forum/#!topic/golang-nuts/7tUShPuPfNM">very interesting thread on golang-nuts list</a> on the performance of interface. Seems like using interfaces really affects performance. In Joshua&rsquo;s test he saw a 3-4x performance difference. I decided to try this on the ring buffer implementation since I am currently using interface. A quick test laster, looks like NOT using interface increased performance 2.4x. For now the code is in the &ldquo;<a href="https://github.com/reducedb/ringbuffer/tree/nointerface">nointerface</a>&rdquo; branch.</p>
</blockquote>

<pre><code>Benchmark1ProducerAnd1Consumer-3         5000000               353 ns/op
Benchmark1ProducerAnd1ConsumerInBytes-3 10000000               147 ns/op
</code></pre>

<h3 id="toc_0">tl;dr</h3>

<ul>
<li><a href="https://github.com/reducedb/ringbuffer">This project</a> implements a low-latency lock-free ring buffer for variable length byte slices. It is modeled after the <a href="https://github.com/LMAX-Exchange/disruptor/">LMAX Disruptor</a>, but not a direct port.</li>
<li>If you have only a single core, don&rsquo;t use this. Use Go channels instead! In fact, unless you can spare as many cores as the number of producers and consumers, don&rsquo;t use this ring buffer.</li>
<li>In fact, for MOST use cases, Go channel is a better approach. This ring buffer is really a specialized solution for very specific use cases.</li>
<li>Primary pattern of this ring buffer is single producer and multiple consumer, where the single producer put bytes into the buffer, and each consumer will process ALL of the items in the buffer. (Other patterns can be implemented later but this is what&rsquo;s here now.)</li>
<li>This ring buffer is designed to deal with situations where we don&rsquo;t know the length of the byte slice before hand. It will write the byte slice to the buffer across multiple slots in the ring if necessary.</li>
<li>The ring buffer currently employs a lock-free busy-wait strategy, where the producer and consumers will continue to loop until data is available. As such, it performs very well in a multi-core environment (almost twice as fast as Go channels) if you can spare 1 core per produer/consumer, but extremely poorly in a single-core environment (600 times worse compare to Go channels).</li>
<li>You can find a lot of information on the LMAX Disruptor. The resources I used include <a href="http://mechanitis.blogspot.com/search/label/disruptor">Trisha&rsquo;s Disruptor blog series</a>, <a href="http://lmax-exchange.github.io/disruptor/">LMAX Disruptor main page</a>, <a href="http://lmax-exchange.github.com/disruptor/files/Disruptor-1.0.pdf">Disruptor technical whitepaper</a>, and Martin Fowler&rsquo;s <a href="http://martinfowler.com/articles/lmax.html">LMAX Architecture article</a>.</li>
<li>You can also read more about <a href="http://en.wikipedia.org/wiki/Circular_buffer">circular buffer</a>, <a href="http://en.wikipedia.org/wiki/Producer-consumer_problem">producerâ€“consumer problem</a>.</li>
</ul>

<h3 id="toc_1">Go Learn Project #7 - Ring Buffer</h3>

<p>For the past several projects (<a href="http://zhen.org/blog/benchmarking-integer-compression-in-go/">#6</a>, <a href="http://zhen.org/blog/bitmap-compression-using-ewah-in-go/">#5</a>, <a href="http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/">#4</a>), I&rsquo;ve mostly been hacking bits, and optimizing them as much as possible for a single core.</p>

<p>For project #7, I decided to do something slightly different. This time we will create a ring buffer that can support variable length byte slices, and leverage multi-cores using multiple goroutines.</p>

<p>Primary target use case is for a producer to read bytes from sockets, ZMQ, files, etc that require process, like JSON, csv, and tsv strings, at a very high speed, such as millions of lines per second, and put these lines into a buffer so other consumers can process these.</p>

<p>A good example is when we import files with millions of lines of JSON object. These JSON objects are read from files, inserted into the buffer, and then they are unmarshalled into other structures.</p>

<p>The goal is to process millions of data items per second.</p>

<h4 id="toc_2">Ring Buffer</h4>

<p>There are several ways to tackle this. A queue or ring buffer is usually a good data structure for this. You can think of a ring buffer is just a special type of queue that&rsquo;s just contiguous by wrapping itself. As Wikipedia said,</p>

<blockquote>
<p>A circular buffer, cyclic buffer or ring buffer is a data structure that uses a single, fixed-size buffer as if it were connected end-to-end. This structure lends itself easily to buffering data streams.</p>
</blockquote>

<p>So, implementation-wise, we can tackle this problem using a standard ring buffer. The most common way of implementing a standard ring buffer is to keep track of a head and a tail pointer, and keep putting items into the buffer but ensuring that head and tail don&rsquo;t cross each other.</p>

<p>In most implementations, the pointers are mod&rsquo;ed with the ring size to determine the next slot size. Also, mutex is used to ensure only one thread is modifying these pointers at the same time.</p>

<h4 id="toc_3">Go Channels</h4>

<p>In Go, an idiomatic and fairly common way is to use <a href="http://golang.org/doc/effective_go.html#channels">buffered channels</a> to solve this problem. It is effectively a queue where a producer puts data items into one end of the channel, and the consumer reads the data items on the other end of the channel. There are certainly pros and cons to to this approach.</p>

<p>First, it is Go idiomatic! Need I say more&hellip; :)</p>

<p>This is probably the easiest approach since Go Channel is already a battle-tested data structure and it&rsquo;s readily available. An example is provided below in the examples section. Performance-wise it is actually not too bad. In a multi-core environment, it&rsquo;s about 60% of the performance compare to the ring buffer. However, in a single-core environment, it is MUCH faster. In fact, 1300 times faster than my ring buffer!</p>

<p>There is one major difference between using channels vs this ring buffer. When the channel has multiple consumers, the data is multiplexed to the consumers. So each consumer will get only part of the data rather than going through all the data. This is illustrated by <a href="http://play.golang.org/p/ACC5LIohIe">this play</a>. The current design of the ring buffer allows multiple consumers to go through every item in the queue. A obvious workaround is to send the data items to multiple channels.</p>

<p>One down side with my channel approach is that I end up creating a lot of garbage over time and will need to be GC&rsquo;ed. Specifically, I am creating a new byte slice for each new data item. Again, there is workaround for this. One can implement a <a href="http://golang.org/doc/effective_go.html#leaky_buffer">leaky buffer</a>. However, because we don&rsquo;t know how big the data items are before hand, it&rsquo;s more difficult to preallocate the buffers up front.</p>

<p>There might actually be a way to implement the leaky buffer with a big preallocated slice. I may just do that as the next project. The goal is to see if we can avoid having to allocate individual byte slices and leverage CPU caching for the big buffer.</p>

<h4 id="toc_4">Lock-Free Ring Buffer</h4>

<p>The way that I&rsquo;ve decided to tackle this problem is to model the ring buffer after the LMAX Disruptor. If you haven&rsquo;t read Martin Fowler&rsquo;s <a href="http://martinfowler.com/articles/lmax.html">article on LMAX Architecture</a>, at this time I would recommend that you stop and go read it first. After that, you should go read <a href="http://mechanitis.blogspot.com/search/label/disruptor">Trisha&rsquo;s Disruptor blog series</a> that explains in even more details how the Disruptor works.</p>

<p>One thing to keep in mind is that the Disruptor-style ring buffer has significant resource requirement, i.e., it requires N cores, where N is the number of producers and consumers, to be performant. And it will keep cores busy by busy waiting (looping). So huge downside. If you don&rsquo;t need this type of low latency architecture, it&rsquo;s much better to stay with channels.</p>

<h3 id="toc_5">Performance Comparison</h3>

<p>These benchmarks are performed on a MacBook Pro with 2.8GHz Intel Core i7 procesor (Haswell), and 16 GB 1600MHz DDR3 memory. Go version 1.2rc5.</p>

<p>The 2 channel consumers benchmark is a single producer sending to 2 channels, each channel consumed by a separate goroutine. So it is apples-to-apples compare to the ring buffer 2 consumers benchmark.</p>

<p>You can see clearly the requirement of 1 core per consumer/producer in the ring buffer implementation (first 6 lines). Without that, performance suffer greately!</p>

<p>Also notice that the channel benchmark (last 6 lines) is faster for a single core than multi-cores. This is probably due to your friendly cache at play.</p>

<pre><code>$ go test -bench=. -run=xxx -cpu=1,2,3
PASS
Benchmark1ProducerAnd1Consumer     10000            339807 ns/op
Benchmark1ProducerAnd1Consumer-2        10000000               258 ns/op
Benchmark1ProducerAnd1Consumer-3        10000000               260 ns/op
Benchmark1ProducerAnd2Consumers    10000            341859 ns/op
Benchmark1ProducerAnd2Consumers-2         200000             14967 ns/op
Benchmark1ProducerAnd2Consumers-3        5000000               340 ns/op
BenchmarkChannels1Consumer      10000000               241 ns/op
BenchmarkChannels1Consumer-2     5000000               436 ns/op
BenchmarkChannels1Consumer-3     5000000               446 ns/op
BenchmarkChannels2Consumers      5000000               319 ns/op
BenchmarkChannels2Consumers-2    5000000               647 ns/op
BenchmarkChannels2Consumers-3    5000000               570 ns/op
</code></pre>

<h3 id="toc_6">Examples</h3>

<h4 id="toc_7">Go Channel: 1 Producer and 1 Consumer</h4>

<pre><code>func BenchmarkChannels(b *testing.B) {
	dataSize := 256
	data := make([]byte, dataSize)
	for i := 0; i &lt; dataSize; i++ {
		data[i] = byte(i % 256)
	}

	ch := make(chan []byte, 128)
	go func() {
		for i := 0; i &lt; b.N; i++ {
			// To be fair, we want to make a copy of the data, otherwise we are just
			// sending the same slice header over and over. In the real-world, the
			// original data slice may get over-written by the next set of bytes.
			tmp := make([]byte, dataSize)
			copy(tmp, data)
			ch &lt;- tmp
		}
	}()

	for i := 0; i &lt; b.N; i++ {
		out := &lt;-ch
		if !bytes.Equal(out, data) {
			b.Fatalf(&quot;bytes not the same&quot;)
		}
	}
}
</code></pre>

<h4 id="toc_8">Ring Buffer: 1 Producer and 1 Consumer</h4>

<p>This test function creates a 256-slot ring buffer, with each slot being 128 bytes long. It also creates 1 producer and 1 consumer, where the producer will put the same byte slice into the buffer 10,000 times, and the consumer will read from the buffer and then make sure we read the correct byte slice.</p>

<pre><code>func Test1ProducerAnd1ConsumerAgain(t *testing.T) {
	// Creates a new ring buffer that's 256 slots and each slot 128 bytes long.
	r, err := New(128, 256)
	if err != nil {
		t.Fatal(err)
	}

	// Gets a single producer from the the ring buffer. If NewProducer() is called
	// the second time, an error will be returned.
	p, err := r.NewProducer()
	if err != nil {
		t.Fatal(err)
	}

	// Gets a singel consumer from the ring buffer. You can call NewConsumer() multiple
	// times and get back a new consumer each time. The consumers are independent and will
	// go through the ring buffers separately. In other words, each consumer will have 
	// their own independent sequence tracker.
	c, err := r.NewConsumer()
	if err != nil {
		t.Fatal(err)
	}

	// We are going to write 10,000 items into the buffer.
	var count int64 = 10000

	// Let's prepare the data to write. It's just a basic byte slice that's 256 bytes long.
	dataSize := 256
	data := make([]byte, dataSize)
	for i := 0; i &lt; dataSize; i++ {
		data[i] = byte(i % 256)
	}

	// Producer goroutine
	go func() {
		// Producer will put the same data slice into the buffer _count_ times
		for i := int64(0); i &lt; count; i++ {
			if _, err := p.Put(data); err != nil {
				// Unfortuantely we have an issue here. If the producer gets an error 
				// and exits, the consumer will continue to wait and not exit. In the
				// real-world, we need to notify all the consumers that there's been
				// an error and ensure they exit as well.
				t.Fatal(err)
			}
		}
	}()

	var total int64

	// Consumer goroutine
	
	// The consumer will also read from the buffer _count_ times
	for i := int64(0); i &lt; count; i++ {
		if out, err := c.Get(); err != nil {
			t.Fatal(err)
		} else {
			// Check to see if the byte slice we got is the same as the original data
			if !bytes.Equal(out.([]byte), data) {
				t.Fatalf(&quot;bytes not the same&quot;)
			}

			total++
		}
	}

	// Check to make sure the count matches
	if total != count {
		t.Fatalf(&quot;Expected to have read %d items, got %d\n&quot;, count, total)
	}
}

</code></pre>

<h4 id="toc_9">Ring Buffer: 1 Producer and 2 Consumers</h4>

<p>As mentioned before, the ring buffer supports multiple consumers. <a href="https://github.com/reducedb/ringbuffer/blob/master/bytebuffer/ringbuffer_test.go#L304">This example</a> shows how you would create two consumers.</p>

<h3 id="toc_10">Conclusion</h3>

<p>See tl;dr on top.</p>

<p>Comments/feedbacks on <a href="https://news.ycombinator.com/item?id=6831293">Hacker News</a>, <a href="http://www.reddit.com/r/golang/comments/1rvvb6/ring_buffer_variablelength_lowlatency_lockfree/">Reddit</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/">
        Go vs Java: Decoding Billions of Integers Per Second
      </a>
    </h1>

    <span class="post-date">Thu, Nov 14, 2013</span>

    

<p>Comments/feedbacks on <a href="http://www.reddit.com/r/golang/comments/1qquqz/go_vs_java_decoding_billions_of_integers_per/">reddit</a>, <a href="https://news.ycombinator.com/item?id=6743821">hacker news</a></p>

<blockquote>
<p>2013-11-17 Update #2: Tried another new trick. This time I updated the leading bit position function, when using the gccgo compiler, <a href="https://github.com/reducedb/encoding/blob/master/bitlen_gccgo.go">to use libgcc&rsquo;s <code>__clzdi2</code> routine</a>. This had the same effect as the update #1 except it&rsquo;s for when gccgo is used. Performance increase ranged from 0% to 20% for encoding only. Thanks dgryski on reddit and minux on the golang-nuts mailing list.</p>

<p>2013-11-16 Update #1: Tried a new trick, which is to use an <a href="https://github.com/reducedb/encoding/commit/ea080c479fb4994e400ebba021d13f10c4f3fecc">assembly version of bitlen</a> to calculate the leading bit position. See the section below on &ldquo;Use Some Assembly&rdquo;.</p>
</blockquote>

<p>So, before you crucify me for benchmarking Go vs Java, let me just say that I am not trying to discredit Go. I like Go and will use it for more projects. I am simply trying to show the results as objectively as I can, and hope that the community can help me improve my skills as well as the performance of the libraries.</p>

<p>I don&rsquo;t consider myself a Go expert. I&rsquo;ve been using Go for a few months and have been documenting the projects as I go in this blog. I&rsquo;ve learned a ton and have applied many of the things I learned in optimizing this project. However, I cannot claim that I have done everything possible, so the performance numbers &ldquo;could&rdquo; still be better.</p>

<p>I would like to ask for your help, if you can spare the time, to share some of your optimization tips and secrets. I would love to make this library even faster if possible.</p>

<h3 id="toc_0">tl;dr</h3>

<p>The following chart shows how much (%) faster Java is compare to Go in decoding integers that are encoded using different codecs. It shows results from processing two different files. See below for more details.</p>

<p><img src="/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_faster.png"></p>

<ul>
<li>Daniel Lemire&rsquo;s <a href="https://github.com/lemire/JavaFastPFOR">Java version</a> is anywhere from 12% to 180% faster than my <a href="https://github.com/reducedb/encoding">Go version</a> for decoding integers. I didn&rsquo;t compare the <a href="https://github.com/lemire/fastpfor">C++ version</a> but given that the C++ version has access to SIMD operations, it can be much faster.</li>
<li>I tried many different ways to optimize my Go code for this projects, including using range for looping through slices, inlining simple functions, unrolling simple loops, unrolling even more loops, disabling bound checking (not generally recommended), using <em>gccgo</em> to compile, and used some assembly.</li>
<li>Using <em>gccgo -O3</em> resulted in the highest performance. I tested using standard gc compiler, gc -B, gccgo, and gccgo -O3. The comparison above uses the <em>gccgo -O3</em> numbers.</li>
<li>Using a range loop instead of unrolling a loop in one of the often used functions, AND compiling using <em>gccgo -O3</em>, I was able to get within 6% of Java version for Delta BP32 decoding. However, all of the other Go binaries suffered greatly.</li>
<li>This benchmark is purely a CPU benchmark. The test environment has enough memory to keep all the arrays in memory without causing swap, and there&rsquo;s no disk IO involved in the actual encoding/decoding functions.</li>
</ul>

<h3 id="toc_1">Project Review</h3>

<p>A month ago I wrote about my &ldquo;Go Learn&rdquo; project #6: <a href="http://zhen.org/blog/benchmarking-integer-compression-in-go/">Benchmarking Integer Compression in Go</a> <a href="https://github.com/reducedb/encoding">(github)</a>. In that project I ported 6 different codecs for encoding and decoding 32-bit integers. Since then, I have ported a couple more codecs, cleaned up the directories, and performed a ton of profiling and optimization to increase performance.</p>

<p>There are now a total of 8 codecs available:</p>

<ul>
<li>Binary Packing (BP32), FastPFOR, Variable Byte (varint) (top level directories)

<ul>
<li>Standard codec that encodes/decodes the integers as they are</li>
</ul></li>
<li>Delta BP32, Delta FastPFOR (<strong>new</strong>), Delta Variable Byte (under <em>delta/</em>)

<ul>
<li>Encodes/decodes the deltas of the integers</li>
<li>These codecs generally produce much more compact representations if the integers are sorted</li>
<li>These codecs generally perform much faster, but there are some exceptions</li>
</ul></li>
<li>ZigZag BP32, ZigZag FastPFOR (<strong>new</strong>) (under <em>zigzag/</em>)

<ul>
<li>Encode/decodes the deltas of the integers, where the deltas themselves are encoded using Google&rsquo;s zigzag encoding</li>
</ul></li>
</ul>

<p>In addition, the <em>benchmark</em> program under <em>benchmark/</em> is provided to let users easily test different integer lists and codecs.</p>

<h3 id="toc_2">Techniques Tried</h3>

<p>I cannot say this enough: <strong>benchmark, profile, optimize, rinse, repeat</strong>. Go has made testing, benchmarking, and profiling extremely simple. You owe it to yourself to optimize your code using these tools. Previously I have written about how I was able to <a href="http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/">improve the cityhash Go implementation performance by 3-16X by Go profiling</a>.</p>

<p>To optimize the integer encoding library, I followed the same techniques to profile each codec, and try as much as I can to optimzie the hot spots.</p>

<p>Below are some of the optimizations I&rsquo;ve tried. Some helped, some didn&rsquo;t.</p>

<h4 id="toc_3">For-Range Through Slices</h4>

<p>I learned this when Ian Taylor from Google (and others) helped me optimize one of the functions using range to loop through the slices instead of <code>for i := 0; i &lt; b; i++ {}</code> loops. The for-range method can be 4-7 times faster than the other way. You can see the difference between BenchmarkOffset and BenchmarkRange <a href="https://gist.github.com/zhenjl/7495442">here</a>.</p>

<p>I also found that <em>gccgo -O3</em> can do some really good optimizations with simple range loops. You can see the difference with this <a href="https://gist.github.com/zhenjl/7495442">gist</a>. When using <em>gc</em> the standard Go compiler, BenchmarkRange (31.3 ns/op) is 56% slower than BenchmarkUnrolled (13.9 ns/op). However, then reverse is true when using <em>gccgo -O3</em>. BenchmarkUnrolled (8.92 ns/op) is 100% slower than BenchmarkRange (4.46 ns/op).</p>

<p>Side note: this set of benchmarks are courtesy of DisposaBoy and Athiwat in the #go-nuts IRC channel. Thanks for your help guys.</p>

<h4 id="toc_4">Unroll Simple Loops</h4>

<p>For some simple, known-size, loops, such as initializing a slice with the same initial non-zero value, unrolling the loop makes a big difference. The caveat is that <em>gccgo -O3</em> does an amazing job of optimizing these simple range loops, so in that case unrolling the loop is actually slower.</p>

<p>As an example, the <a href="https://github.com/reducedb/encoding/blob/master/bitpacking/delta_bitpacking.go#L242">following function</a> is unrolled as</p>

<pre><code>func deltaunpack0(initoffset int32, in []int32, inpos int, out []int32, outpos int) {
    out[outpos+0] = initoffset
    out[outpos+1] = initoffset
    out[outpos+2] = initoffset
    .
    .
    .
</code></pre>

<p>It was originally written as</p>

<pre><code>tmp := out[outpos:outpos+32]
for i, _ := range tmp {
    tmp[i] = initoffset
}
</code></pre>

<p>When unrolled, AND using the standard <em>gc</em> compiler, we saw performance increase by almost 45% if this function is called often. However, as we mentioned above, when using <em>gccgo -O3</em>, the for-range loop is 33% faster than the unrolled method.</p>

<p>For now, I am keeping the unrolled version of the function.</p>

<h4 id="toc_5">Unroll Even More Loops</h4>

<p>Given the success of unrolling the above simple loop, I thought I try unrolling <a href="https://github.com/reducedb/encoding/blob/master/util.go#L150">even</a> . <a href="https://github.com/reducedb/encoding/blob/master/util.go#L281">more</a> . <a href="https://github.com/reducedb/encoding/blob/master/util.go#L412">loops</a> to see if it helps.</p>

<p>It turns out performance actually suffered in some cases. I speculated that the reason may have to do with the bound checking when accessing slices. It turns out I might be right. After I disabled bound checking, performance increased when unrolling these loops. See below regarding disable bound checking.</p>

<h4 id="toc_6">Inline Simple Functions</h4>

<p>There are <a href="https://github.com/reducedb/encoding/blob/master/util.go#L53">several</a> . <a href="https://github.com/reducedb/encoding/blob/master/util.go#L60">small</a> functions that gets called quite often in this encoding library. During profiling I see these functions being on top all the time.</p>

<p>I decided to try and inline these functions to see if the reduced function call overhead will help. In general I didn&rsquo;t see much performance improvements using this technique. The most I saw was a 1% increase in performance. I contribute that to noise.</p>

<h4 id="toc_7">Disable Bound Checking (Generally NOT Recommended)</h4>

<p>This integer encoding library operates on large slices of data. There&rsquo;s a TON of slice access using index. Knowing the every slice access using index requires bound checking, I decided to try disabling bound checking using <code>-gcflags -B</code> to compile the code.</p>

<p>Disabling the bound checking for the Go compiler didn&rsquo;t help as much as I hoped. For <em>ts.txt</em>, disabling bound checking increased performance by 10% for Delta BP32 encoding only.</p>

<p>However, if I disabled bound checking AND unrolled even more loops, we saw decoding performance increase anywhere from 10-40%. Encoding performance didn&rsquo;t see much change. The following chart shows the performance increase (%) from the for-range loops to unrolled some additional loops when I disabled bound checking. This is comparing to the standard <em>gc</em> compiler.</p>

<p><img src="/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/no_bound_checking.png"></p>

<p>The question is, is it worth the removal of bound checking? For the raw results, I kept the numbers from the for-range loops.</p>

<h4 id="toc_8">Using gccgo</h4>

<p>When I was looking around for Go optimization techniques, I saw several posts on StackOverflow as well as the Go mailing list suggesting people try <em>gccgo</em> as the compiler if they wanted more performance. So I thought I give it a shot as well. After downloading gcc 4.8.2 and letting it compile overnight, I was finally able to test it out.</p>

<p>Compiling with <em>gccgo</em> without any optimization flags actually saw performance drop by 50-60%. The best performance result was achieved when I compiled using <em>gccgo -O3</em>. The comparison to Java uses the numbers from that binary.</p>

<p>As mentioned above, <em>gccgo -O3</em> seems to do a pretty amazing job of optimizing simple range loops. I was able to achieve 33% performance increase using for-range with <em>gccgo -O3</em> instead of unrolling the simple loop. The final result was within 6% of the Java version for Delta BP32 decoding.</p>

<h4 id="toc_9">Use Some Assembly</h4>

<p>The final trick I tried is to convert one of the often used functions to assembly language. This was suggested to me almost 6 weeks ago by Dave Andersen on the golang-nuts Google group. He suggested that I steal the bitLen function in the math/big/arith files. That&rsquo;s <a href="https://github.com/reducedb/encoding/commit/ea080c479fb4994e400ebba021d13f10c4f3fecc">exactly what I did</a>.</p>

<p>The bitlen function returns the position of the most significant bit that&rsquo;s set to 1. It is most often called by encoding methods to determine how many bits are required to store that integers. So natually one would expect only the encoding functions will be improved.</p>

<p>That&rsquo;s exactly what happened. Using the new bitlen assembly function, I was able to improve encoding performance by anywhere from 3% to 40%, depending on the codec. The most significant improvement was saw when Delta FastPFOR encoding was applied on <em>latency.txt</em>. It consistently saw ~40% performance increase.</p>

<p>As such, the <a href="https://github.com/reducedb/encoding">code</a> has been updated to use the assembly version of the bitlen.</p>

<h3 id="toc_10">Benchmark Environment</h3>

<p>The system I used to run the benchmarks was graciously provided by Dr. Daniel Lemire. Here are the CPU and memory information at the time I ran the benchmarks. As you can see, we have plenty of memory to load the large integer arrays and should cause no swap. (I had this issue running these tests on my little MBA with 4GB of memory. :)</p>

<p>No disk IO is involved in this benchmark.</p>

<h4 id="toc_11">OS</h4>

<pre><code>NAME=&quot;Ubuntu&quot;
VERSION=&quot;12.10, Quantal Quetzal&quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&quot;Ubuntu quantal (12.10)&quot;
VERSION_ID=&quot;12.10&quot;
</code></pre>

<h4 id="toc_12">CPU</h4>

<pre><code>model name      : Intel(R) Xeon(R) CPU E5-1620 0 @ 3.60GHz
cpu MHz         : 3591.566
cache size      : 10240 KB
</code></pre>

<h4 id="toc_13">Memory</h4>

<pre><code>             total       used       free     shared    buffers     cached
Mem:      32872068   13548960   19323108          0     209416   11517476
-/+ buffers/cache:    1822068   31050000
Swap:     33476604          0   33476604
</code></pre>

<h4 id="toc_14">java</h4>

<pre><code>java version &quot;1.7.0_25&quot;
OpenJDK Runtime Environment (IcedTea 2.3.10) (7u25-2.3.10-1ubuntu0.12.10.2)
OpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)
</code></pre>

<h4 id="toc_15">go and gccgo</h4>

<pre><code>go version go1.2rc4 linux/amd64
gcc version 4.8.2 (GCC)
</code></pre>

<h4 id="toc_16">Files</h4>

<p>There are two files used in this benchmark:</p>

<ul>
<li>File 1: <em><a href="https://github.com/reducedb/encoding/tree/master/benchmark/data">ts.txt</a></em> contains 144285498 sorted integers. They are timestamps at 1 second precision. There are a lot of repeats as multiple events are recorded for that second.</li>
<li>File 2: <em>latency.txt</em> contains 144285498 <strong>unsorted</strong> integers. An example, lat.txt.gz, can be seen <a href="https://github.com/reducedb/encoding/tree/master/benchmark/data">here</a>.</li>
</ul>

<h4 id="toc_17">Codecs</h4>

<p>For this benchmark, I used 4 different codecs:</p>

<ul>
<li>Delta Binary Packing (BP32) (<a href="https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/IntegratedBinaryPacking.java">Java</a>, <a href="https://github.com/reducedb/encoding/blob/master/delta/bp32/bp32.go">Go</a>)</li>
<li>Delta FastPFOR (<a href="https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/IntegratedFastPFOR.java">Java</a>, <a href="https://github.com/reducedb/encoding/blob/master/delta/fastpfor/fastpfor.go">Go</a>)</li>
<li>Binary Packing (BP32) (<a href="https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/BinaryPacking.java">Java</a>, <a href="https://github.com/reducedb/encoding/blob/master/bp32/bp32.go">Go</a>)</li>
<li>FastPFOR (<a href="https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/FastPFOR.java">Java</a>, <a href="https://github.com/reducedb/encoding/blob/master/fastpfor/fastpfor.go">Go</a>)</li>
</ul>

<p>Because both BP32 and FastPFOR work on 128 integer blocks, the 58 remaining integers from the test files are encoded using Delta VariableByte and Variable Byte codecs, respectively. This is achieved using a <strong>Composition</strong> (<a href="https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/IntegratedComposition.java">Java</a>, <a href="https://github.com/reducedb/encoding/blob/master/composition/composition.go">Go</a>) codec.</p>

<p>The Java and Go versions of the codecs are almost identical, logic-wise, aside from language differences. These codecs operate on arrays (or slices in Go) of integers. There are a lot of bitwise and shift operations, and lots of loops.</p>

<h3 id="toc_18">Benchmark Results</h3>

<p>The <a href="https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdDRyNEhPWUlqMHZzMG5FWFQzX1ZoZ1E&amp;output=html">raw results</a> from running different Go compilers (and flags) and codes are in the Google spreadsheet at the bottom of the post.</p>

<p>To be consistent, all the percentage numbers presented below are based on dividing the difference between the larger number (A) and the smaller number (B) by the smaller number (B).</p>

<blockquote>
<p>(A - B) / B</p>
</blockquote>

<p>So you can say that A is faster than B by X%.</p>

<h4 id="toc_19">Go Fastest</h4>

<p>I compiled 4 different versions of Go binary for this benchmark:</p>

<ul>
<li><em>benchmark.gc</em>

<ul>
<li>This is built using the standard Go compiler, <em>gc</em>. The command is <code>go build benchmark.o</code>.</li>
</ul></li>
<li><em>benchmark.gc-B</em>

<ul>
<li>This is built using the standard Go compiler, <em>gc</em>, and I turned off bound checking for this version since the codecs deals with slices a lot. The command is <code>go build -gcflags -B benchmark.o</code>.</li>
</ul></li>
<li><em>benchmark.gccgo</em>

<ul>
<li>This is built using the <em>gccgo</em> compiler with no additional flags. The command is <code>go build -compiler gccgo benchmark.o</code>.</li>
</ul></li>
<li><em>benchmark.gccgo-O3</em>

<ul>
<li>This is built using the <em>gccgo</em> compiler with the <em>-O3</em> flag. The command is <code>go build -compiler gccgo -gccgoflags '-O3 -static' benchmark.o</code>.</li>
</ul></li>
</ul>

<p>Surprisingly, disabling the bound checking for the Go compiler didn&rsquo;t help as much as I hoped. For <em>ts.txt</em>, disabling bound checking increased performance by 10-40% for encoding only. For <em>ts.txt</em> decoding and <em>latency.txt</em>, it didn&rsquo;t help at all.</p>

<p>Using the <em>gccgo</em> compiler with no flags had the worst performance. In general we saw that <em>benchmark.gc</em> (standard Go version) is about 110%-130% faster than the <em>gccgo</em> version.</p>

<p>Lastly, the <em>gccgo-O3</em> version is the fastest. This is the version that&rsquo;s compiled using <code>-O3</code> flag for gccgo. We saw that the <em>gccgo-O3</em> version is anywhere from 10% to 60% faster than the <em>gc</em> version.</p>

<p><img src="/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/comparing_go_binaries.png"></p>

<p>Above is a chart that shows the decoding performance of the <em>ts.txt</em> file for the different binaries and codecs.</p>

<p>For the comparison with Java, I am using the <em>gccgo-O3</em> numbers.</p>

<h4 id="toc_20">Bits Per Integer (Lower is Better)</h4>

<p><img src="/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/bits_per_integer.png"></p>

<p>As you can tell, when the integers are sorted (ts.txt), the compression ratio is VERY good. Delta BP32 achieved 0.25 bits per 32-bit integer, and Delta FastPFOR is even better at 0.13 bits per integer. This is also because there are a lot of repeats in ts.txt.</p>

<p>Because the timestamps are rather large numbers, e.g., 1375228800, when the non-delta codecs are used, they did not achieve very good compression ratio. We achieved ~31 bits per 32-bit integer using standard FastPFOR and BP32 codecs.</p>

<p>When the integers are NOT sorted, then we run into trouble. When delta codecs are used, a lot of deltas are negative numbers, which means the MSB for most of the deltas is 1. In this case, it&rsquo;s actually better to use the standard codecs instead of the delta codecs. The standard codecs achieved ~24 bits per 32-bit integer, and the delta codecs were ~32 bits per 32-bit integer.</p>

<p>I also tested the latency file against the <a href="https://developers.google.com/protocol-buffers/docs/encoding#varints">zigzag</a> delta codecs and achieved ~25 bits per 32-bit integer. So it&rsquo;s not much better than the standard codecs. However, zigzag delta comes in extremely handy when the negative numbers are smaller.</p>

<h4 id="toc_21">Java vs. Go</h4>

<p>For this section, we are comparing the decoding speed between the fastest Go version and Java. As you saw at the beginning of this post. Daniel Lemire&rsquo;s <a href="https://github.com/lemire/JavaFastPFOR">Java version</a> is anywhere from 12% to 180% faster than my <a href="https://github.com/reducedb/encoding">Go version</a> for decoding integers.</p>

<p>The following chart shows how much (%) faster Java is compare to Go in decoding integers that are encoded using different codecs. It shows results from processing two different files. See below for more details.</p>

<p><img src="/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_faster.png"></p>

<p>The following chart shows the decoding performance while processing <em>ts.txt</em>.</p>

<p><img src="/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_tstxt.png"></p>

<p>The following chart shows the decoding performance while processing <em>latency.txt</em>.</p>

<p><img src="/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_latency.png"></p>

<h4 id="toc_22">Raw Results</h4>

<p>The <a href="https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdDRyNEhPWUlqMHZzMG5FWFQzX1ZoZ1E&amp;output=html">raw results</a> are in the following Google spreadsheet.</p>

<iframe width='800' height='500' frameborder='0' src='https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdDRyNEhPWUlqMHZzMG5FWFQzX1ZoZ1E&output=html&widget=true'></iframe>

<p>Comments/feedbacks on <a href="http://www.reddit.com/r/golang/comments/1qquqz/go_vs_java_decoding_billions_of_integers_per/">reddit</a>, <a href="https://news.ycombinator.com/item?id=6743821">hacker news</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/">
        Improving Cityhash Performance by Go Profiling
      </a>
    </h1>

    <span class="post-date">Sun, Nov 10, 2013</span>

    

<p>Comments/Feedback on <a href="https://news.ycombinator.com/item?id=6710115">Hacker News</a>, <a href="http://www.reddit.com/r/golang/comments/1qcygc/improving_cityhash_performance_by_go_profiling/">Reddit</a></p>

<blockquote>
<p>2013-11-19 Update #1: After more profiling (see <em>top</em> output in the &ldquo;After modification&rdquo; section), I&rsquo;ve found that these 3 functions, unalignedLoad64, fetch64, and uint64InExpectedOrder,  add up to quite a bit of execution time. I looked at the <a href="https://code.google.com/p/cityhash/source/browse/trunk/src/city.cc">original cityhash implementation</a> and realized that the combination of these functions is basically geting a LittleEndian uint64, which we can read by doing LittleEndian.Uint64(). So I updated fetch64 to do just that. Performance increased by ~20% just because of that. Also, the bloom filter test showed that cityhash is now faster than both FNV64 and CRC64.</p>
</blockquote>

<p>Another month has gone by since the last post. This month has been extremely busy at work in an extraordinary good way. We had a huge POC that went quite succesfully at a large customer site. I also got a chance to visit Lisbon, Portugal as part of this POC. So things overall went pretty well.</p>

<p>However, since family and work pretty much occupied most of my waking hours over the past few weeks, I haven&rsquo;t made much progress on the &ldquo;Go Learn&rdquo; projects. To keep myself going, I picked a smaller task over the weekend and decided to go back to &ldquo;Go Learn Project #1&rdquo;, my <a href="https://github.com/reducedb/cityhash">cityhash</a> Go implementation.</p>

<h3 id="toc_0">Go Learn Project #1</h3>

<p>When I first decided to learn Go, I struggled quite a bit to find a project that I can sink my teeth into. I am not sure if others are the same way, for me to learn a new language, I have to have something meaningful to work on. I can&rsquo;t just write hello world programs or follow tutorials. I can read books and articles, but I will also procrastinate for weeks if I can&rsquo;t find a relevant project.</p>

<p>Luckily, I was thinking about creating a data generator at work and wanted to write that in Go. But in order to write the data generator in Go, I first have to have a cityhash implementation in Go because our backend (C/C++) is using cityhash.</p>

<p>Surprisingly, I looked around but couldn&rsquo;t find any Go implementation of cityhash. I would have thunk that given Go and Cityhash are both from Google, some Googler would have already ported cityhash over to Go. But no such luck, or maybe I just didn&rsquo;t look hard enough. In any case, I decided to port cityhash over to Go.</p>

<p>Porting an existing project in C over to Go has a big advantage in that I don&rsquo;t have to invent any new data structures or algorithms. It will allow me to focus on learning the Go syntax and the standard libraries. Many many moons ago (before I converted to the dark side) I was pretty proficient in C and reading cityhash wasn&rsquo;t too difficult, so porting cityhash over to Go should be relatively straightforward.</p>

<p>In any case, the porting process wasn&rsquo;t too difficult, as it turned out. Overall I was able to do that over a weekend. I was also able to port the test program (city-test.cc) over as well (vim substitution FTW) to validate that my implementation was functionally correct.</p>

<h3 id="toc_1">Performance Sucked</h3>

<p>I had always suspected that my Go cityhash implementation wasn&rsquo;t great performance wise. At the time I hadn&rsquo;t learned how to benchmark or profile Go programs, so I didn&rsquo;t do a whole lot except ensuring functionally the results are correct. Also my data generator at work was working fine so I left the implementation as is.</p>

<p>In September, I implemented a <a href="https://github.com/reducedb/bloom">Bloom Filter package</a> which required a hash function as part of the implementation. For that package, I <a href="http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/">tested different hash functions</a> to see how they affect the performance of the bloom filter. As you can see from those benchmarks, Cityhash is consistently 3x slower compare to the others. At the time I knew it was because of my implementation but didn&rsquo;t look into it further.</p>

<h3 id="toc_2">Profiling Go Cityhash</h3>

<p>Since that first project, I have learned quite a bit more about benchmarking and profiling. So this weekend I finally took time to profile the Go implementation and found some interesting results. Now Go experts probably will read this and say &ldquo;of course, you had no idea what you were doing.&rdquo; And that would be true. I had no idea at the time. Hopefully this post will make up for it.</p>

<p>If you haven&rsquo;t read <a href="http://blog.golang.org/profiling-go-programs">this blog post on Go profiling</a>, you should go read it now before continuing.</p>

<p>In any case, I wrote a <a href="https://gist.github.com/zhenjl/7405913">short program</a> to test cityhash with a big file. This way it can collect enough samples to tell me where the bottleneck is.</p>

<p>The original implementation took 45 seconds to hash a 1.1G file. Below is the cpu profile output.</p>

<pre><code>duration = 45401991546ns
Total: 4295 samples
    2766  64.4%  64.4%     2766  64.4% runtime.memmove
     374   8.7%  73.1%      463  10.8% sweepspan
     259   6.0%  79.1%      383   8.9% MHeap_AllocLocked
     123   2.9%  82.0%      123   2.9% runtime.markspan
      95   2.2%  84.2%     1223  28.5% runtime.mallocgc
      74   1.7%  85.9%       74   1.7% runtime.MSpan_Init
      43   1.0%  86.9%     4234  98.6% github.com/reducedb/cityhash.unalignedLoad64
      40   0.9%  87.9%      595  13.9% runtime.MCache_Alloc
      32   0.7%  88.6%       32   0.7% runtime.markallocated
      31   0.7%  89.3%       56   1.3% MHeap_FreeLocked
</code></pre>

<p>As you can see, most of the time were spent copying memory (64.4%). And also the sweepspan (part of GC) is also running quite often (8.7%).</p>

<p>Before this, I had no idea that there&rsquo;s that much memory being copied. So this is definitely interesting. I then looked the <a href="/images/2013-11-10-improving-cithhash-performance-by-go-profiling/before.svg">graph of the profile data</a> using the &ldquo;web&rdquo; command.</p>

<p>It shows clearly that unalignedLoad64, a function that loads a uint64 from the buffer, is causing most of the memmove. Technically, it&rsquo;s calling binary.Read(), which creates an array of 8 bytes, and passes to another function which eventually calls runtime.copy to copy a few bytes of data from the original buffer into the array.</p>

<p>So now the reason for the large amount of time spent in memmove is clear. Basically, every time I call binary.Read(), it creates an 8 byte array. Up to 8 bytes of data are copied into it. Then the data in the array gets converted into an uint64. After that, the array is thrown away. And this is done over and over again for the whole 1.1G file, which means 1.1G of memory is being created in tiny 8-byte chunks, copied, and thrown away. It&rsquo;s no wonder the program is slow!</p>

<p>By this time, some of the readers are probably wondering why the heck I am using binary.Read() if I knew that I will be reading a uint64 from a slice. And they would be right again. Only excuse I have is that I had no clue and that was the first thing I found to work a few months back, so I just used it.</p>

<h3 id="toc_3">Modifying the Implementation</h3>

<p>The change turned out to be relatively simple. Instead of using binary.Read(), I used LittleEndian.Uint64() to read the uint64. After the change, I ran the same program again.</p>

<p>Here are the results from the post-change run. The time it took to hash the 1.1G file is only 2.8 seconds. That&rsquo;s 16X faster than before the change. The &ldquo;top&rdquo; profile output is also a lot more reasonable.</p>

<pre><code>duration = 2718339693ns
Total: 245 samples
      57  23.3%  23.3%       57  23.3% encoding/binary.littleEndian.Uint64
      50  20.4%  43.7%      227  92.7% github.com/reducedb/cityhash.CityHash128WithSeed
      40  16.3%  60.0%       97  39.6% github.com/reducedb/cityhash.unalignedLoad64
      35  14.3%  74.3%      146  59.6% github.com/reducedb/cityhash.fetch64
      28  11.4%  85.7%       28  11.4% github.com/reducedb/cityhash.uint64InExpectedOrder
      27  11.0%  96.7%      132  53.9% github.com/reducedb/cityhash.weakHashLen32WithSeeds_3
       8   3.3% 100.0%        8   3.3% github.com/reducedb/cityhash.weakHashLen32WithSeeds
       0   0.0% 100.0%        1   0.4% MHeap_AllocLarge
       0   0.0% 100.0%      227  92.7% _/Users/jian/Projects/cityhash_test.TestLatencyIntegers
       0   0.0% 100.0%      227  92.7% github.com/reducedb/cityhash.CityHash128
</code></pre>

<p>Also, nothing really jumps out when looking at the <a href="/images/2013-11-10-improving-cithhash-performance-by-go-profiling/after.svg">post-change profile data graph</a>.</p>

<h3 id="toc_4">Bloom Filter Benchmarks</h3>

<p>Now that the changes are in, I went back and re-ran some of the bloom filter benchmarks. They look a lot more reasonable as well. Below is a comparison of the Scalable Bloom Filter. The post-change run is almost 2.5x faster than the pre-change run. Also, the post-change number (1442 ns/op) is a lot closer to some of the other hash functions (~1100 ns/op).</p>

<pre><code>Scalable Bloom Filter
---------------------
BenchmarkBloomCityHash   1000000              1442 ns/op (after cityhash change)
BenchmarkBloomCityHash   1000000              3375 ns/op (before cityhash change)
</code></pre>

<h3 id="toc_5">Summary</h3>

<p>The Go authors have made it extermely simple to test, benchmark and profile Go programs so there&rsquo;s really reason for anyone not to do that often. It helps you see how your program works and where the bottlenecks are. It can also help you identify surprises that you may not have though of. A good example is in my case, I had no idea binary.Read() works the way it works until I profiled my program.</p>

<p>Comments/Feedback on <a href="https://news.ycombinator.com/item?id=6710115">Hacker News</a>, <a href="http://www.reddit.com/r/golang/comments/1qcygc/improving_cityhash_performance_by_go_profiling/">Reddit</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/benchmarking-integer-compression-in-go/">
        Benchmarking Integer Compression in Go
      </a>
    </h1>

    <span class="post-date">Fri, Oct 11, 2013</span>

    

<p><a href="https://news.ycombinator.com/item?id=6537688">comments/feedback</a></p>

<blockquote>
<p>Updated post: <a href="http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/">Go vs Java: Decoding Billions of Integers Per Second</a></p>
</blockquote>

<h3 id="toc_0">tl;dr</h3>

<ul>
<li><a href="https://github.com/reducedb/encoding">Source code</a></li>
<li>Let me start by saying that I am not that happy with the performance numbers I got. It probably has more to do with my familiarity and expertise with Go than anything else. But still&hellip;</li>
<li>Having said that, I am pleasantly surprised how much faster some of the integer compression algorithms are compare to standard compression algorithms such as gzip, LZW and even snappy.</li>
<li>There&rsquo;s no one size fits all solution. There&rsquo;s always tradeoffs between compression ratio and compression/decompression speed. So depending on the type of integer data and how fast you want to encode/decode, you will need to choose the right solution.</li>
<li>Gzip does probably the best job in compression ratio but worst in terms of encoding/decoding performance.</li>
<li>Delta BP32 (IntegratedBinaryPacking in JavaFastPFOR) performs the best (ratio, encoding/decoding speed) for sorted integer arrays such as timestamps.</li>
<li>I would love it if someone can run these tests on a faster machine and send me the results. I would be happy to put them into the spreadsheet.</li>
</ul>

<h3 id="toc_1">Go Learn Project #6 - Integer Compression</h3>

<p>It&rsquo;s been 4 weeks since my last post and I have been BUSY! My day job has been busier than ever (in a good way) and has taken over many of my nights and weekends. However, I&rsquo;ve not forgotten the &ldquo;Go Learn&rdquo; project and continued to tinker with Go as I find time.</p>

<p>For &ldquo;Go Learn&rdquo; project #6, I decided to port <a href="https://github.com/lemire/JavaFastPFOR">JavaFastPFOR</a> over to Go. The JavaFastPFOR repo actually is a collection of integer encoding/decoding algorithms. However, the more interesting ones are the ones created by Daniel Lemire, including FastPFOR, BP32, BP128, etc. To borrow from the repo&rsquo;s readme:</p>

<blockquote>
<p>[JavaFastPFOR] is a library to compress and uncompress arrays of integers very fast. The assumption is that most (but not all) values in your array use less than 32 bits. These sort of arrays often come up when using differential coding in databases and information retrieval (e.g., in inverted indexes or column stores).</p>

<p>Some CODECs (&ldquo;integrated codecs&rdquo;) assume that the integers are in sorted orders. Most others do not.</p>
</blockquote>

<h3 id="toc_2">Thank You, Daniel Lemire</h3>

<p>As you may have noticed, this is the second Daniel Lemire project that I&rsquo;ve ported over. The previous one is <a href="http://zhen.org/blog/bitmap-compression-using-ewah-in-go/">Bitmap Compression using EWAH in Go</a>.</p>

<p><a href="http://lemire.me/">Danile Lemire</a> is a computer science professor at TELUQ (UniversitÃ© du QuÃ©bec) where he teaches primarily online. He specializes in Databases, Data Warehousing and OLAP, Recommender Systems and Collaborative Filtering, and Information Retrieval.</p>

<p>I won&rsquo;t elaborate on how knowledgeable he is here because you can easily tell by reading his papers and blogs. I do want to mention how Daniel has been extremely helpful on my porting effort. He&rsquo;s provided tremendous guidance and support, and went even as far as providing me access to one of his machines for running performance tests.</p>

<p>So thank you Daniel!</p>

<h4 id="toc_3">Decoding Billions of Integers per Second Through Vectorization</h4>

<p>This project is inspired by Danile&rsquo;s blog post, <a href="http://lemire.me/blog/archives/2012/09/12/fast-integer-compression-decoding-billions-of-integers-per-second/">Fast integer compression: decoding billions of integers per second</a>, and <a href="http://arxiv.org/abs/1209.2137">paper</a>. As the paper states:</p>

<blockquote>
<p>In many important applicationsâ€”such as search engines and relational database systemsâ€”data is stored in the form of arrays of integers. Encoding and, most importantly, decoding of these arrays consumes considerable CPU time. Therefore, substantial effort has been made to reduce costs associated with compression and decompression</p>
</blockquote>

<p>As part of this research, Daniel also made his code available in <a href="https://github.com/lemire/fastpfor">C++</a> and <a href="https://github.com/lemire/JavaFastPFOR">Java</a>. The Go port is based on JavaFastPFOR.</p>

<p>However, because Go has no access to the SSE instruction sets (well, not without getting into C or assembly), I was not able to port over the SIMD versions of the algorithms.</p>

<h3 id="toc_4">The Port</h3>

<p>The Go port is available on <a href="https://github.com/reducedb/encoding">github</a>. In this version, I&rsquo;ve proted over six algorithms, including</p>

<ul>
<li>FastPFOR</li>
<li>BP32 (BinaryPacking in JavaFastPFOR)</li>
<li>Delta BP32 (IntegratedBinaryPacking in JavaFastPFOR)</li>
<li>ZigZag BP32 (BP32 with Delta encoding using <a href="https://developers.google.com/protocol-buffers/docs/encoding#types">ZigZag</a> encoding method.)</li>
<li>VariableBytes</li>
<li>Delta VariableBytes (Integrated VariableBytes in JavaFastPFOR)</li>
</ul>

<p>I won&rsquo;t go into details of how each of these algorithms work. If you are interested, I strongly encourage you to go read Daniel&rsquo;s paper.</p>

<h3 id="toc_5">The Benchmarks</h3>

<p>To benchmark these algorithms, I&rsquo;ve created 5 sets of data.</p>

<ul>
<li><a href="https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/synth/ClusteredDataGenerator.java">Clustered</a> - This is a generated list of random and sorted integers based on the clustered model.</li>
<li><a href="https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/synth/UniformDataGenerator.java">Uniform</a> - This will generate a &ldquo;uniform&rdquo; list of sorted integers.</li>
<li><a href="https://github.com/reducedb/encoding/tree/master/samples">Timestamps</a> - This is a list of timestamps in sorted order, extracted from another data set that&rsquo;s mainly network monitoring data. The timestamps are epoch time (seconds since 1970). It has long runs of the same timestamp because the dataset contains many entries per second.</li>
<li>IP Addresses - This is a data set that contains a 32-bit integer representation of IPv4 addresses. This data set is NOT sorted.

<ul>
<li>In the real-world, one probably wouldn&rsquo;t compress IP addresses like that. One would probably use dictionary encoding for the IPs, then encode the dictionary keys.</li>
<li>The dictionary keys will likely be much smaller numbers, which means it will compress fairly well.</li>
</ul></li>
<li>Latencies - This list of integers represent latencies on the network. It is also unsorted.</li>
</ul>

<p>Timestamps, IP addresses, and Latencies are essentially 3 columns from another dataset.</p>

<p>Along with benchmarking the integer encoding algorithms, I also benchmarked the Go implementations of Gzip, LZW and Snappy. Both Gzip and LZW are part of the Go standard library. Snappy is a <a href="https://code.google.com/p/snappy-go/">separate project</a> implemented by the Go developers.</p>

<h3 id="toc_6">The Results</h3>

<p>All benchmarks are performed on a machine with Intel&reg; Xeon&reg; CPU E5-2609 0 @ 2.40GHz.</p>

<h4 id="toc_7">Compression Ratio (Lower is Better)</h4>

<p><img src="/images/2013-10-11-benchmarking-integer-compression-in-go/Integer-Compression-Ratio.png" alt="Compression Ratio" />
</p>

<p>Compression ratio is measured in bits/integer. Before compression, the integers we are compressing are all 32-bit integers. This chart shows us how many bits are used for each integer after compression.</p>

<p>There are a few things you can observe from this chart:</p>

<ul>
<li>Sorted integer lists ALWAYS perform better (ratio and speed) than unsorted integer lists.</li>
<li>Sometimes the compressed size is LARGER than the uncompressed size. This is because some of these algorithms use extra space to store encoding meta information.</li>
<li>Gzip, in general, has the best compression ratio.</li>
<li>Delta BP32 performs the best for sorted lists, but really has problems with unsorted lists.</li>
</ul>

<h4 id="toc_8">Compression Speed (Higher is Better)</h4>

<p><img src="/images/2013-10-11-benchmarking-integer-compression-in-go/Integer-Compression-Speed.png" alt="Compression Speed" />
</p>

<p>Compression speed is measured in millions of integers per second (MiS). Note that:</p>

<ul>
<li>Delta BP32 by far has the best compression speed across different data sets.</li>
<li>BP32 does a fairly decent job compressing as well, but its compression ratio is fairly poor.</li>
<li>Gzip and LZW perfom the most poorly.</li>
</ul>

<h4 id="toc_9">Decompression Speed</h4>

<p><img src="/images/2013-10-11-benchmarking-integer-compression-in-go/Integer-Decompression-Speed.png" alt="Decompression Ratio" />
</p>

<p>Decompression speed is measured in millions of integers per second (MiS). Note that:</p>

<ul>
<li>Again, Delta BP32 does the best across different data sets, and gzip/LZW did the poorest.</li>
<li>BP32 decoded extremely fast for sorted timestamps that have large runs of the same timestamps.</li>
<li>FastPFOR seems to perform the most consistently for integer compression algorithms. (For compression as well.)</li>
</ul>

<h3 id="toc_10">The Conclusions</h3>

<ul>
<li>For encoding sorted numbers, Delta BP32 has the best combination of compression ratio, and encoding/decoding speed.</li>
<li>For encoding unsorted numbers, Snappy seems like a good alternative.</li>
<li>For long term storage, it might be worth considering gzip. It provides the best compression ratio for data that may not be accessed for a long while.</li>
</ul>

<h3 id="toc_11">The Raw Result</h3>

<p>The following is a Google spreadsheet that contains the raw result set.</p>

<iframe width='800' height='500' frameborder='0' src='https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdEwzMkJBNVQzWkxkOExTLThIbGlCSkE&output=html&widget=true'></iframe>

<p><a href="https://news.ycombinator.com/item?id=6537688">comments/feedback</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/bitmap-compression-using-ewah-in-go/">
        Bitmap Compression using EWAH in Go
      </a>
    </h1>

    <span class="post-date">Sun, Sep 15, 2013</span>

    

<p><a href="https://news.ycombinator.com/item?id=6392197">comments/suggestions</a></p>

<h3 id="toc_0">tl;dr</h3>

<ul>
<li>If you are interested in database bitmap index, then it&rsquo;s definitely worth learning more about bitmap compression.</li>
<li>EWAH is a very interesting way of compressing bitmaps and this article talks about my port of it to Go.</li>
<li>Sorry I think you should read the rest of the article. :)</li>
</ul>

<h3 id="toc_1">The Real Deal</h3>

<p>For &ldquo;Go Learn&rdquo; project #5, I&rsquo;ve decided to continue implementing bit-related data structures. This time I decided to port a bitmap (not image) compression data structure to Go. Unlike bloom filters, which had quite a few implementations, I couldn&rsquo;t really find any compressed bitmap implementations in Go. Most of them are in C/C++ or Java. (For previous projects please see previous blog posts.)</p>

<p>Btw, if you have any suggestions on what project I should do next, feel free to drop me a comment.</p>

<h3 id="toc_2">Bitmaps</h3>

<p>Bitmap compression is often used for database <a href="http://en.wikipedia.org/wiki/Bitmap_index">bitmap indexing</a>. To quote Wikipedia again:</p>

<blockquote>
<p>A bitmap index is a special kind of database index that uses bitmaps. [&hellip;]</p>

<p>Bitmap indexes use bit arrays (commonly called bitmaps) and answer queries by performing bitwise logical operations on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional B-tree indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for online transaction processing applications. [&hellip;]</p>

<p>Some researchers argue that Bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively.</p>
</blockquote>

<p>There are quite a few approaches to bitmap compression. You will find most of the work listed in the wikipedia page. The two I found to be most interesting are <a href="http://ricerca.mat.uniroma3.it/users/colanton/docs/concise.pdf">CONCISE</a> and <a href="https://code.google.com/p/javaewah/">Enhanced Word-Aligned Hybrid (EWAH)</a>. After reading throught both papers and looking at their Java implementations, I decided to start with EWAH. There&rsquo;s no primary reason for starting with EWAH first. But Daniel Lemire&rsquo;s comment intrigued me, &ldquo;Out of the bitmap formats that I have often tested, Concise is the most concise, but JavaEWAH is often faster. So there is a trade-off.&rdquo; I wanted to see how fast the Go implementation can be so I figure I start with EWAH. I may still port CONCISE later. (Note: I am not the best Go programmer nor do I claim that my code is the most performant. This is a learning project so there will definitely be room for improvement.)</p>

<h3 id="toc_3">EWAH</h3>

<p>Enhanced Word-Aligned Hybrid (EWAH) is a bitmap compression algorithm created by Daniel Lemire, Owen Kaser, and Kamel Aouiche. (<a href="http://arxiv.org/pdf/0901.3751v4.pdf">paper</a>)</p>

<p>EWAH uses a method called <a href="http://en.wikipedia.org/wiki/Run-length_encoding">Run-Length Encoding</a> to perform compression. RLE is extremely useful for dataset that tend to have long blocks of the same data. In the case of database bitmap indices, there maybe long blocks of 1&rsquo;s or 0&rsquo;s, depending on the sparsity of the data. In cases where sparsity is low, compression can be extremely high. We will show some data points on this later.</p>

<p>EWAH uses two types of words where the first type is a 64-bit verbatim word. The second type of word is a marker word: the first bit indicates which clean word will follow, half the bits (32 bits) are used to store the number of clean words, and the rest of the bits (31 bits) are used to store the number of dirty words following the clean words. EWAH bitmaps begin with a marker word.</p>

<p>Clean (empty) words are either all 0&rsquo;s or all 1&rsquo;s. Dirty (literal) words are verbatim words that have 1&rsquo;s and 0&rsquo;s. Blocks of clean words can be compressed using RLE. Dirty words cannot be compressed so they are represented vertatim.</p>

<pre><code>3210987654321098765432109876543210987654321098765432109876543210
0000000000000000000000000000010000000000000000000000000000000100
0000000000000000000001000000000000000000000000000000000000000001
0000000000000000000000000000000000000000001000000000000000000100
</code></pre>

<p>For example, the above bits show a marker word. Bits go from right to left. So the first bit being 0 means the clean/empty words all have their bits as 0. The next 32 bits indicate the number of clean words this is encoding. &ldquo;10&rdquo; in this case means this marker word encodes 2 clean words that are all 0&rsquo;s.</p>

<p>The final 31 bits (starting at position 33 from the right) indicates the number of literal words that will follow this marker word. In thise case, there are also 2 dirty (literal) words that follow the marker word.</p>

<p>As you can see, the overall bitmap contains 3 64-bit words. However, it encodes 4 64-bit words. So there&rsquo;s a space saving of 25%.</p>

<p>For more detailed information please read the paper. It&rsquo;s definitely worth the read if you are trying to understand bitmap compression.</p>

<p>There are a few caveats to EWAH:</p>

<ol>
<li>There is no random access to any single bit, unlike a regular bit array. To access a random bit, you need to walk through the whole bitmap and find the word that contains the bit.</li>
<li>You cannot set a bit that&rsquo;s earlier than the last set bit. For example, if you have already set bit 100, you can not go back and set bit 99.</li>
</ol>

<p>Technically you can solve both of these problems, but the cost of doing such operations will be extremely high.</p>

<h3 id="toc_4">Go Port</h3>

<p>Most of my nights and weekends over the past couple of weeks have gone into the porting of JavaEWAH to Go. There&rsquo;s also a C++ version of EWAH which I&rsquo;ve also gone through. Code structure wise it&rsquo;s very similar to the Java version. You can find the <a href="https://github.com/reducedb/bitmap">Go EWAH port on Github</a>.</p>

<p>This is one of the more inovled ports I&rsquo;ve ported to Go because it involved quite a few more classes and also the logic required much more understanding. With previous projects like <a href="https://github.com/reducedb/cityhash">Cityhash</a> and <a href="https://github.com/reducedb/bloom">Bloom Filter</a>, I can get away with either translating directly without complete understanding (in the case of Cityhash) or the logic is fairly simple and can be coded up pretty easily (bloom filters.)</p>

<p>I wanted to be able to implement multiple bitmap compression algorithms down the road, so I started with a simple <a href="https://github.com/reducedb/bitmap/blob/master/bitmap.go">bitmap interface</a>. It contains most the basic bit operations such as Set, Get, And, Not, Or, Xor, AndNot, Cardinality, etc. I then went through the Java source code and ported everything that&rsquo;s required to implement these functions.</p>

<p>What this means is that my Go version is NOT a complete port of JavaEWAH. There are some java classes and methods I chose not to port as they weren&rsquo;t required for satisfying the bit operations.</p>

<p>Initially I ported the classes over exactly as Java version had them. This included classes such as RunningLengthWord, BufferedRunningLengthWord, EWAHIterator, IteratingBufferedRunningLengthWord. These classes formed the bases for iterating over the bitmap and they are heavily used throughout the implementation for all the bit operations. Iterating over a bitmap usually involves multiple layers of nested iterators.</p>

<p>What I found is that in my Go port using this same approach was running fairly slow as there are multiple nested iterators. (Results later.) Now to be completely honest and fair, this does not mean the Java version is also slow. It&rsquo;s just the <strong>My Go version</strong> is slow. As I have learned over the past few projects, there are always ways to make things faster. I tried to apply what I&rsquo;ve learned in previous projects in these new projects but it certainly doesn&rsquo;t mean I always succeed.</p>

<h4 id="toc_5">Cursors</h4>

<p>After porting the Java version to Go directly, I learned a ton about the structure of the algorithm and how the different bit operations are performed. Given that my Go version was running fairly slow, I decided to try a different approach. Instead of the nested iterator approach, I implemented a Cursor structure that basically collapsed RunningLengthWord, BufferedRunningLengthWord, IteratingBufferedRunningLengthWord, and EWAHIterator into a single structure.</p>

<p>Whenever I need to iterate over the bitmap, I create a cursor (you can call it a iterator if you like but it&rsquo;s just a single layer with no nesting) and use that to keep track of where I am. Then I loop over the bitmap and perform the necessary operations.</p>

<p>In many cases during my benchmarks, the cursor-based performance is 2-4x faster over the previous approach.  I took great care to make sure the results of the two different implementations return exactly the same thing. You can see some of that in my tests.</p>

<h4 id="toc_6">Get</h4>

<p>The original Java implementation did not have a Get function, which gets the bit at any random location. This is because bitmap implementation such as CONCISE and EWAH are meant to be used with bitmap indices and in most cases you don&rsquo;t need to check a random bit. The common use cases are to perform bit operations on two bitmaps, then find all the bits that are set.</p>

<p>Regardless, I wanted to have a Get function duing my testing so I can check to see the bits I set are indeed set correctly. So I did a quick Get implemention by looping over the bitmap. Daniel was gracious enough to convert my Go implementation back into Java and <a href="https://github.com/lemire/javaewah/blob/master/src/main/java/com/googlecode/javaewah/EWAHCompressedBitmap.java#L1015">incorporated it into JavaEWAH</a>.</p>

<p>However, walking the bitmap each and every time we want to check a bit is way way way too slow. In some of the use cases I am thinking about I tend to sequentially check bits. For example, I will check bits 100, 159, 302, 405, etc. The bits are almost always increasing.</p>

<p>So taking a page from the <a href="https://github.com/reducedb/skiplist">skiplist</a> implementation I did a few weeks back, I implemented Get with Fingers. The Finger concept is fairly simple. We basically keep a finger on the last checked word, and if the new bit to check is in or after the last checked word, we just move forward from there. If the new bit is BEFORE the current word, we restart from the beginning. The actually Get implementation keeps a getCursor as a finger to the last checked word.</p>

<p>The result is quite an improvement for sequential checks:</p>

<ul>
<li>Get with Finger and Cursor - 125 ns/op</li>
<li>Get without Finger but uses Cursor - 60542 ns/op</li>
<li>Get using the nested iterators approach - 822530 ns/op</li>
</ul>

<h3 id="toc_7">Results</h3>

<p>The <a href="https://github.com/reducedb/bitmap">final product</a> of this project is available on Github. I ran quite a few benchmarks with different bitmap density (1-sparsity) as well as running bit operations with two bitmaps of varying density. Here are some points to note.</p>

<h4 id="toc_8">Density vs Space Saving</h4>

<p>The whole reason for bitmap compression to exist is to save space. So here are some results from that test. Probably not surprisingly, the higher the density, the lower the space savings. The charte below shows how density and space savings correlate for a bitmap that had a cardinality of 10,000.</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0ApDLtJuUH-1rdHh2bzY1d0h3U042UXNkLUlzaENLMUE&oid=1&zx=510m225jyk7b" /></p>

<table>
<thead>
<tr>
<th>Space Saving</th>
<th>Bit Density</th>
</tr>
</thead>

<tbody>
<tr>
<td>-0.62%</td>
<td>50.39%</td>
</tr>

<tr>
<td>-0.04%</td>
<td>6.46%</td>
</tr>

<tr>
<td>32.68%</td>
<td>0.67%</td>
</tr>

<tr>
<td>91.61%</td>
<td>0.07%</td>
</tr>

<tr>
<td>99.15%</td>
<td>0.01%</td>
</tr>
</tbody>
</table>

<p>My test data set is generated fairly uniformly. To get different density, I generate a random number between 1-N, where N is the number at the left-most column in the table. So on average the distance between bits is N/2, well, approximately. This may not be the best real-world scenario so take it with a grain of salt. But it does demonstrate how well EWAH can compress.</p>

<p>At the bottom, you will find the full list of bitmaps I generated for the tests. (Look at the bitmaps tab)</p>

<h4 id="toc_9">Cursor vs Nested Iterators</h4>

<p>Another interesting benchmark I did was test the performance between the cursor-based implementation and the nested-iterators implementation. I tested the two implementations using a wide variaty of combinations, totaling 225 benchmarks per bit operation (AND, OR, XOR, ANDNOT).</p>

<script src="https://gist.github.com/zhenjl/6577789.js"></script>

<p>The above gist is the shell script that generates the wrappers for calling a benchmark function in Go. Btw, I love the testing package for Go as it makes it really easy for me to create tests and benchmarks. However, not having the ability to run benchmarks with different parameters without creating wrappers like these is a huge pain the butt.</p>

<p>The result is quite telling. The cursor-based implementation is 2-4 times faster than the nested-iterator version. See the &ldquo;Performance Results&rdquo; tab below.</p>

<ul>
<li>AND/OR/XOR/ANDNOT 1 - This is the cursor-based implementation</li>
<li>AND/OR/XOR/ANDNOT 2 - This is the nested-iterator-based implementation</li>
</ul>

<h4 id="toc_10">Embedded Spreadsheet</h4>

<iframe width='800' height='500' frameborder='0' src='https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdHh2bzY1d0h3U042UXNkLUlzaENLMUE&output=html&widget=true'></iframe>

<p><a href="https://news.ycombinator.com/item?id=6392197">comments/suggestions</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/">
        Benchmarking Bloom Filters and Hash Functions in Go
      </a>
    </h1>

    <span class="post-date">Wed, Sep 4, 2013</span>

    

<p><a href="https://news.ycombinator.com/item?id=6329616">discussion/comments</a></p>

<p>Update: @bradfitz <a href="https://news.ycombinator.com/item?id=6329616">commented</a> that allocating a new slice during each add/check is probably not good for performance. And he is in fact correct. After removing the allocation for each add/check and doing a single slice make during New(), the performance increased by ~27%!! Here&rsquo;s the gist containing the new ns/op results.</p>

<p>{% gist 6515577 %}</p>

<hr />

<p>Another week, another &ldquo;Go Learn&rdquo; project. This time, in project #4, I implemented a <a href="https://github.com/reducedb/bloom">Go package</a> with several bloom filters and <a href="https://gist.github.com/zhenjl/6433634">benchmarked</a> them with various hash functions. (For previous projects, see <a href="http://zhen.org/blog/go-skiplist/">#3</a>, <a href="http://zhen.org/blog/testing-msgpack-and-bson/">#2</a>, <a href="https://github.com/reducedb/cityhash">#1</a>.)</p>

<p>The goal of this project, for me at least, is to implement a pure Go package that doesn&rsquo;t rely on wrappers to other langagues. There&rsquo;s already <a href="https://github.com/search?l=Go&amp;q=bloom&amp;ref=cmdform&amp;type=Repositories">quite a few</a> bloom filters implemented in Go. But hey, in the name of learning, why not implement another one!</p>

<h3 id="toc_0">Bloom Filters</h3>

<p>Wikipedia says,</p>

<blockquote>
<p>A Bloom filter, conceived by Burton Howard Bloom in 1970 is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not; i.e. a query returns either &ldquo;inside set (may be wrong)&rdquo; or &ldquo;definitely not in set&rdquo;.</p>
</blockquote>

<p>In my little project, I implemented the following three variants:</p>

<ul>
<li>Standard - This is the classic bloom filter as described on <a href="http://en.wikipedia.org/wiki/Bloom_filter">Wikipedia</a>.</li>
<li>Partitioned - This is a variant described by <a href="http://www.ieee-infocom.org/2004/Papers/45_3.PDF">these</a> <a href="http://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf">papers</a>. Basically instead of having a single big bit array, partitioned bloom filter breaks it into <em>k</em> partitions (or slices) s.t. each partition is <em>m/k</em> size, where <em>m</em> is the total number of bits, and <em>k</em> is the number of hashes. Then each hash function is assigned to a single partition.</li>
<li>Scalable - This is yet another variant described by <a href="http://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf">here</a>). The idea is that the standard bloom filter requires that you know <em>m</em>, or the size of the filter a priori. This is not possible for use cases where data continue to come in without bound. So the Scalable bloom filter utilizes multiple bloom filters, each with incresing k, but decreasing <em>P</em> where <em>P</em> is the desired error probability. This bloom filter also introduces <em>r</em>, which is the error tightening ratio, and it&rsquo;s 0 &lt; r &lt; 1.</li>
</ul>

<p>There are a ton more variants for bloom filters. You can raed all about them in <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=5&amp;cad=rja&amp;ved=0CE4QFjAE&amp;url=http%3A%2F%2Fwww.tribler.org%2Ftrac%2Fraw-attachment%2Fwiki%2FJelleRoozenburg%2Fresearch_assignment_jroozenburg_20051108.pdf&amp;ei=JHInUt6oO6GTiQK-44DYDg&amp;usg=AFQjCNG057WVJ2m2QYPuqWzCZ0Vn4JnOug&amp;sig2=NVlad7xGO4S_hFpMU9apGA&amp;bvm=bv.51773540,d.cGE">this paper</a> and <a href="http://www.dca.fee.unicamp.br/~chesteve/pubs/bloom-filter-ieee-survey-preprint.pdf">this paper</a>.</p>

<h3 id="toc_1">Hash Functions</h3>

<p>To add an element, bloom filters hashes the element using <em>k</em> hashing functions, identifying <em>k</em> positions in the bit array, and setting those bits to 1. To check for an element, you essentially do the same thing (hash the element with <em>k</em> hash functions) and then check to see if all the bits at those positions are set. If all set, then most likely the element is available. If any of them are not set, then the element is definitely not avaiable. So bloom filters can have false positives, but not false negatives.</p>

<p>However, actually running <em>k</em> hash functions is quite expensive. So <a href="http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/rsa.pdf">Kirsch and Mitzenmacher</a> determined that by using a single hash function, but using the formula, <em>gi(x) = h1(x) + i * h2(x)</em>, to calculate <em>k</em> hash values, the performance is actually similar. So this is what I used here.</p>

<p>For the values <em>h1</em> and <em>h2</em>, we used the first 4 bytes returned from each hash function for <em>h1</em>, and second 4 bytes for <em>h2</em>.</p>

<p>The following hash functions are used for this little experiement:</p>

<ul>
<li>FNV-64 - There&rsquo;s a built-in Go package for this. So that&rsquo;s what I used.</li>
<li>CRC-64 - Again, using the built-in Go package.</li>
<li>Murmur3-64 - There&rsquo;s no built-in package so I used <a href="https://github.com/spaolacci/murmur3">this one</a>.</li>
<li>CityHash-64 - Again, no built-in so I am using the one <a href="https://github.com/reducedb/cityhash">I implemented</a>.</li>
<li>MD5 - Using the built-in Go implementation.</li>
<li>SHA1 - Using the built-in Go implementation.</li>
</ul>

<p>As a side note, MD5 and SHA1 return 128 bit hash values. Since we only use the first 64 bits, we throw away the last 64 bits.</p>

<h3 id="toc_2">Test Setup</h3>

<p>The machine that ran these tests is a Macbook Air 10.8.4 1.8GHz Intel Core i5 4GB 1600MHz DDR3.</p>

<p>For the false positive test, I added all the words in /usr/share/dict/web2 (on my Mac) into each of the bloom filters. To check for false positives, I check for all the words in /usr/share/dict/web2a in each of the bloom filters. The two files should have completely different set of words (AFAICT).</p>

<pre><code>  235886 /usr/share/dict/web2
   76205 /usr/share/dict/web2a
</code></pre>

<p>For each bloom filter, I ran tests using the following combinations:</p>

<pre><code>for i in element_count[236886, 200000, 100000, 50000]
    for j in hash_functions[fnv64, crc64, murmur3-64, cityhash-64, md5, sha1]
        run test with hash_functions[j] and element_count[i]
    end
end
</code></pre>

<p>Element count in this is the initial size I set for the bloom filter. The bit array size, <em>m</em>, and number of hash values, <em>k</em>, are then calculated from there. I also set <em>P</em> (error probability) to 0.001 (0.1%) and <em>p</em> (fill ratio, or how full should the bit array be) to 0.5. The idea for the element count is to see how the bloom filters will perform when it has a high fill ratio.</p>

<p>For the scalable bloom filter test, I also needed to add another dimension since it uses either standard or partitioned bloom filter internally. So</p>

<pre><code>for i in element_count[236886, 200000, 100000, 50000]
    for j in hash_functions[fnv64, crc64, murmur3-64, cityhash-64, md5, sha1]
        for k in bloom_filter[standard, partitioned]
            run test with hash_functions[j] and element_count[i] and bloom_filter[k]
        end
    end
end
</code></pre>

<p>For the performance test, I added all the words in web2 into the bloom filters. I continue to loop through the file until b.N (Go benchmark framework) is met. So some of the words will be re-added, which should not skew our test results since the operations are the same.</p>

<p>You can see the tests in the <a href="https://github.com/reducedb/bloom">github repo</a>. Just look for all the _test.go files.</p>

<h3 id="toc_3">Test Results</h3>

<p>The following is a summary of the test results. You can also feel free to look at all the <a href="https://gist.github.com/zhenjl/6433634">gory details</a>.</p>

<p>Note: FR = Fill Ratio, FP = False Positive</p>

<p>For the spreadsheet embedded below, here are some observations</p>

<ul>
<li>The MD5 Go implementation I used maybe broken, or I am not using it correctly, or MD5 is just bad. You will see that the fill ration is VERY low, 1-6%. So the false positive rate is very high (89+%).</li>
<li>The CityHash Go implementation seems very slow. Could just be my implementation (anyone want to venture some optimization?). But functionally it&rsquo;s correct.</li>
<li>Both standard and partitioned bloom filters use the same number of bits and there&rsquo;s not a huge difference in fill ratio and false positive rate. (Ignoring the MD5 anomaly.)</li>
<li>Predictably, as fill ratio goes up, so does the false positive rate for both standard and partitioned bloom filters.</li>
<li>Scalable bloom filter uses more bits as it contines to add new base bloom filters when the estimated fill ratio reaches <em>P</em> which is set to 0.5 (50%).</li>
<li>Probably not surprisingly, Scalable bloom filter maintains a fairly low false positive rate.</li>
<li>However, you will also notice that the Scalable FP increases as the total number of base filters increase. This suggests that I may want to try a lower <em>r</em> (error tightening ratio). Currently I use 0.9, but maybe 0.8 is more appropriate.</li>
<li>Overall it seems FNV is probably good enough for most scenarios.</li>
<li>Also Scalable+Standard might be a good choice for anyone doing stream data processing.</li>
</ul>

<iframe width='800' height='600' frameborder='0' src='https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdEEwZ3JwaVVTZGxBX1g1NkthSFVVTXc&single=true&gid=0&output=html&widget=true'></iframe>

<p>This chart shows the performance (ns/op) for adding elements to the bloom filters. Overall the performance is very similar for the different bloom filter implementations.</p>

<p><img src="https://docs.google.com/spreadsheet/oimg?key=0ApDLtJuUH-1rdEEwZ3JwaVVTZGxBX1g1NkthSFVVTXc&oid=1&zx=jbizsaoepc5x" /></p>

<h3 id="toc_4">Feedback</h3>

<p>Feel free to send me any feedback via <a href="https://github.com/reducedb/bloom/issues">github issues</a> or on <a href="https://news.ycombinator.com/item?id=6329616">Hacker News</a>.</p>

<h3 id="toc_5">References</h3>

<p>During this process I referenced and leveraged some of these other projects, so thank you all!</p>

<ul>
<li>Referenced

<ul>
<li><a href="https://github.com/willf/bloom">willf/bloom</a> - Go package implementing Bloom filters</li>
<li><a href="https://github.com/bitly/dablooms">bitly/dablooms</a> - scaling, counting, bloom filter library</li>
<li>There&rsquo;s also a bunch of papers, some of which I linked above.</li>
</ul></li>
<li>Leveraged

<ul>
<li><a href="https://github.com/willf/bitset">willf/bitset</a> - Go package implementing bitsets</li>
<li><a href="https://github.com/spaolacci/murmur3">spaolacci/murmur3</a> - Native MurmurHash3 Go implementation</li>
</ul></li>
</ul>

<p><a href="https://news.ycombinator.com/item?id=6329616">discussion/comments</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/go-skiplist/">
        Go Skiplist
      </a>
    </h1>

    <span class="post-date">Mon, Sep 2, 2013</span>

    

<p>Most of my nights and weekends in the past week have been immersed in my <a href="https://github.com/reducedb/skiplist">new skiplist library</a>, even sacrificing some family time (yikes, not good!). It&rsquo;s one of those projects I picked up in order to learn <a href="http://golang.org">Go</a>.</p>

<p>Wikipedia defines a skip list as</p>

<blockquote>
<p>A skip list is a data structure for storing a sorted list of items using a hierarchy of linked lists that connect increasingly sparse subsequences of the items. These auxiliary lists allow item lookup with efficiency comparable to balanced binary search trees (that is, with number of probes proportional to log n instead of n).</p>
</blockquote>

<p>You can imagine using a skiplist data structure whenver you want to use a binary search tree.</p>

<p>This skiplist implementation also uses search fingers, based on William Pugh&rsquo;s work, <a href="http://drum.lib.umd.edu/bitstream/1903/544/2/CS-TR-2286.1.pdf">A Skiplist Cookbook</a>. So the efficiency is O(log k) where k is the distance between the last searched/updated item and the current one.</p>

<h3 id="toc_0">Performance</h3>

<p>{% gist e8779545c102c69aae10 %}</p>

<p>These numbers are from my Macbook Air 10.8.4 1.8GHz Intel Core i5 4GB 1600MHz DDR3.</p>

<p>Notice the fastest time is BenchmarkInsertTimeDescending. This is because the keys for that test is generated using time.Now().UnixNano(), which is always ascending. And because the sort order of the skiplist is descending, so the new item is ALWAYS inserted at the front of the list. This happens to have the best case of O(1).</p>

<p>The next best time is BenchmarkInsertTimeAscending. This is still pretty good, but because the sort order is ascending, so the new items are ALWAYS inserted at the end. This required the skiplist to walk all the levels so it took a bit longer.</p>

<p>The other benchmarks should have the average O(log k) efficiency.</p>

<h3 id="toc_1">Example (Int)</h3>

<p>You can see additional examples in the <a href="https://github.com/reducedb/skiplist/blob/master/skiplist_test.go">test file</a>.</p>

<pre><code>// Creating a new skiplist, using the built-in Less Than function as the comparator.
// There are also two other built in comparators: BuiltinGreaterThan, BuiltinEqual
list := New(skiplist.BuiltinLessThan)

// Inserting key, value pairs into the skiplist. The skiplist is sorted by key,
// using the comparator function to determine order
list.Insert(1,1)
list.Insert(1,2)
list.Insert(2,1)
list.Insert(2,2)
list.Insert(2,3)
list.Insert(2,4)
list.Insert(2,5)
list.Insert(1,3)
list.Insert(1,4)
list.Insert(1,5)

// Selecting items that have the key == 1. Select returns a Skiplist.Iterator
rIter, err := list.Select(1)

// Iterate through the list of items. Keys and Values are turned as interface{}, so you
// need to type assert them to your type
for rIter.Next() {
	fmt.Println(rIter.Key().(int), rIter.Value().(int))
}

// Delete the items that match key. An iterator is returned with the list of deleted items.
rIter, err = list.Delete(1)

// You can also SelectRange or DeleteRange
rIter, err = list.SelectRange(1, 2)

rIter, err = list.DeleteRange(1, 2)
</code></pre>

<h3 id="toc_2">Bultin Comparators</h3>

<p>There are three built-in comparator functions:</p>

<ul>
<li>BuiltinLessThan: if you want to sort the skiplist in ascending order</li>
<li>BuiltinGreaterThan: if you want to sort the skiplist in descending order</li>
<li>BuiltinEqual: just to compare</li>
</ul>

<p>Currently these built-in comparator functions work for all built-in Go types, including:</p>

<ul>
<li>string</li>
<li>uint64, uint32, uint16, uint8, uint</li>
<li>int64, int32, int16, int8, int</li>
<li>float32, float64</li>
<li>unitptr</li>
</ul>

<p><a href="https://news.ycombinator.com/item?id=6317109">discussion/comments</a></p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/testing-msgpack-and-bson/">
        Testing MsgPack and BSON
      </a>
    </h1>

    <span class="post-date">Wed, Aug 28, 2013</span>

    <p>In one of the projects I am working on we are trying to select a message encoding scheme. The use case is pretty simple. We have a server that accepts data records from different clients, and our goal is support 100K messages per second per node. Our selection criteria are pretty simple as well:</p>

<ul>
<li>Compactness: how compact is the post-encoding byte array</li>
<li>Performance: how fast is marshalling and unmarshalling of data</li>
</ul>

<p>Out of the various encoding schemes out there, including <a href="http://bsonspec.org">BSON</a>, <a href="http://msgpack.org">MessagePack</a>, ProtoBuf, Thrift, etc etc, we decided to test BSON and MessagePack due to their closeness to JSON (we use JSON format quite a bit internally).</p>

<p>Here&rsquo;s a very short and simple <a href="http://golang.org">Go</a> program we wrote to test the performance. The libraries I am using are <a href="http://github.com/ugorji/go/codec">Go codec</a> and <a href="http://labix.org/gobson">Go BSON library</a>. (I am sure one can argue that the library used affects the result, which I would agree. However, these are the best libraries for Go AFAICT.)</p>

<p>The results are as follows:</p>

<ul>
<li>For marshalling, BSON is about 15-20% slower than MsgPack.</li>
<li>For Unmarshalling, BSON is 300% slower than MsgPack.</li>
<li>Size-wise, BSON is about 20% more than MsgPack.</li>
</ul>

<p>So looks like msgpack is the way to go!</p>

<p>Special thanks(!) to realrocker in #go-nuts and <a href="https://github.com/ugorji">Ugorji</a> for their help in cleaning up my code and helping me figure out my problems.</p>

<p>Some notes about the codec library:</p>

<ul>
<li>According to Ugorji, decoding without schema in the codec library usesÂ largest type for decoding, i.e. int64, uint64, float64. So if you don&rsquo;t have a schema, the result will not pass reflect.DeepEqual test unless you initially type convert to those types. This is documented <a href="https://github.com/ugorji/go/commit/7d844bb938783105c48aa9f4a34663f7d4190c32">here</a></li>
<li>Ugorji also quickly fixed a bug where &ldquo;codec should be able to decode into a byte slice from both msgpack str/raw format or msgpack bin format (new in finalized spec).&rdquo; AWESOME response time!</li>
<li>To get string support (encode/decode strings), make sure RawToString is set to true (see below)</li>
</ul>

<p><a href="https://news.ycombinator.com/item?id=6293642">discussion</a></p>

<p>{% gist 6371433 %}</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/migrating-40000-posts-from-wordpress-to-octopress/">
        Migrating 40,000 Posts from Wordpress to Octopress
      </a>
    </h1>

    <span class="post-date">Sun, Aug 18, 2013</span>

    

<p>I just spent the weekend migrating several of my blogs, including this one (<a href="http://zhen.org">zhen.org</a>), <a href="http://cloudfeed.net">Cloudfeed</a>, <a href="http://mobileaware.net">Mobileaware</a>, <a href="http://dataaware.net">Dataaware</a>, and <a href="http://news.trustpath.com">TrustPath News</a>, from <a href="http://wordpress.org">wordpress</a> to <a href="http://octopress.org">octopress</a>.</p>

<p>I have one more, <a href="http://cloudaware.net">Cloudaware</a>, that I need to migrate but that one will take a bit of work. I&rsquo;ll explain why a bit later.</p>

<h3 id="toc_0">tl;dr</h3>

<h4 id="toc_1">The Bad</h4>

<ul>
<li>If you are not familiar with git, don&rsquo;t use Octopress. I almost want to say if you don&rsquo;t use git on other development projects on a fairly regular basis, don&rsquo;t use Octopress because you will run into issues that you may not be able to resolve.</li>
<li>If you have a very large site and need fast updates, don&rsquo;t use Octopress. Large sites with 1000&rsquo;s of posts will take a long time to generate. There&rsquo;s no incremental update currently. Everytime you update a post or create a new post you need to re-generate the whole site.</li>
<li>If you are not comfortable with the command line (terminal windows), don&rsquo;t use Octopress. Most of everything you do with Octopress is in the terminal so be prepare to type. :)</li>
<li>If your site uses a lot of wordpress plugins, you may not want to use Octopress. Octopress is still new so it may not have the plugins you are using. Do an inventory prior to determine feasibility.</li>
<li>If your site uses a special theme you may want to investigate the effort of migrating the theme. There&rsquo;s not a large selection like Wordpress. If you want your own theme then prepare to get your hands dirty or pay someone to do it. Good thing is octopress themes are much simpler than wordpress.</li>
</ul>

<h4 id="toc_2">The Good</h4>

<ul>
<li>If you are a developer and have a blog with mostly static content, Octopress is for you.</li>
<li>If you want a high performance static content site, Octopress is for you.</li>
<li>If you free blog hosting with a reputable service (e.g., github), Octopress is for you.</li>
<li>If you don&rsquo;t want to keep updating wordpress versions, Octopress is for you.</li>
</ul>

<h3 id="toc_3">The Reasons</h3>

<p>So what caused me to make the jump all of a sudden?</p>

<h4 id="toc_4">#1: I am lazy but I need better security</h4>

<p>Due to family, work, and side project reasons, I&rsquo;ve been not been paying much attention to these blogs. Some of them are automated but even the automation scripts are failing as I have not kept up with the APIs. This blog especially has been dormant since June 2009. Well, until now at least. This is my first blog post using Octopress.</p>

<p>Not only have I not kept the blogs fresh, I have also not kept Wordpress versions up to date. I&rsquo;ve upgraded wordpress a couple of times for these blogs over the years. The last update was April 6th, 2012 and the wordpress version was 3.3.1. The latest I checked is 3.6.</p>

<p>You are probably wondering why I haven&rsquo;t been hacked given the <a href="http://www.cvedetails.com/vulnerability-list/vendor_id-2337/product_id-4096/">long list of Wordpress vulnerabilities</a>. Well, the fact is I have been hacked once before due to a wordpress xmlrpc way back when. I lost a bunch of files that time. I vowed to keep wordpress updated but as you can tell I have not been successful. So I consider myself lucky.</p>

<p>So a static site hosted by someone else completely seems like the perfect approach.</p>

<h4 id="toc_5">#2 and #3: I am cheap but I want flexibility and performance</h4>

<p>For the past several years I&rsquo;ve been keeping a VPS just to run my blogs. I like to use my own domains and I don&rsquo;t feel like paying wordpress.com for mapping a custom domain and customizing themes. Not to mention you can&rsquo;t even customize the themes all that much. On top of that, I want to server my own ads, not pay wordpress.com NOT to display ads on my blog.</p>

<p>I pay $10/month for my VPS, and I can host any number of blogs I want on it. A single blog on wordpress.com can cost me more than $100 (custom domain, premium theme, custom design, no ads). For the 6 blogs I would be paying $700/year. And every blog I add will cost me another $100+.</p>

<p>Granted, the little VPS I have is barely sufficient to handle the load but it has been ok so far. However, I could really use a bit more horsepower.</p>

<p>So the static site will give me the performance I need and I want the ability to customize the heck out of the site (not that I do it all that much, but the perception of flexibility is still good.)</p>

<h3 id="toc_6">The Process</h3>

<p>Another name for this post could be: Migrate From Wordpress to Octopress in 9 Not-So-Simple Steps</p>

<ol>
<li>Clone Octopress</li>
<li>Export wordpress posts</li>
<li>Convert using Exitwp</li>
<li>Set up the Octopress Theme</li>
<li>Generate the Static Site</li>
<li>Preview the Site</li>
<li>Prepare for Publishing</li>
<li>Publish the Site</li>
<li>Commit your source</li>
</ol>

<p>I&rsquo;ve been seeing experimenting hosting static sites with Github gh-pages recently, see <a href="http://topclouds.org">Top Clouds</a>, so I thought that&rsquo;s a good way to host sites that don&rsquo;t change a whole lot. I&rsquo;ve also been seeing quite a few mentions of Octopress recently so I&rsquo;ve been tempted to check it out.</p>

<p>So with these reasons/excuses, I decided to take the plunge. I checked out many of the recent migrant blogs, including</p>

<ul>
<li><a href="http://jason.pureconcepts.net/2013/01/migrating-wordpress-octopress/">Jason McCreary</a></li>
<li><a href="http://joelhooks.com/blog/2012/07/25/fresh-start-migrating-wordpress-octopress/">Joel Hooks</a></li>
<li><a href="http://decodize.com/html/moving-from-wordpress-to-octopress/">Praveen Vijayan</a></li>
<li><a href="http://blog.baugues.com/moving-from-wordpress-to-octopress">Greg Baugues</a></li>
<li><a href="http://gotofritz.net/blog/weekly-challenge/migrating-wordpress-blogs-to-octopress/">Fabrizio (Fritz) Stelluto</a></li>
<li><a href="http://mattgemmell.com/2011/09/12/blogging-with-octopress/">Matt Gemmell</a></li>
</ul>

<p>These are just a few that I read. There&rsquo;s <a href="https://www.google.com/search?q=migrate+from+wordpress+to+octopress&amp;oq=migrate+from+wordpress+to+octo">a ton more</a> you can browse.</p>

<p>Many of these blogs explained how they migrated and were tremendously helpful. I can&rsquo;t thank these folks enough for sharing their experiences.</p>

<h3 id="toc_7">The Details</h3>

<p>For each of the blogs I migrated, I did the following.</p>

<h4 id="toc_8">1. Clone Octopress</h4>

<p>You need to do this once for each blog, so replace cloudfeed with the blog name.</p>

<p><code>git clone https://github.com/imathis/octopress cloudfeed</code></p>

<p>This will make a copy of octopress in a directory called &ldquo;cloudfeed&rdquo; for you. From there, you can run these git commands to save the copy in your own git repo.</p>

<pre><code>git config remote.origin.url https://git@github.com/zhenjl/cloudfeed
git push origin master
</code></pre>

<p>Github sets the first branch you push to in a new repo the default branch. So make sure you do this step before you publish to, say, the gh-pages branch.</p>

<p>I would highly recommend anyone starting with Octopress to do clone instead of fork on Github. If you fork it on Github, you will end up getting all the branches as well. You really don&rsquo;t need the extra branches. Plus, it will confuse the heck out of you if you didn&rsquo;t realize you got all the branches and you want to use the gh-pages branch to host your blog.</p>

<p>But if for whatever reason you did fork octopress, you can run these commands after you <code>git clone</code> your fork</p>

<pre><code>git push origin :2.5
git push origin :2.5-simplify-rakefile
git push origin :3.0
git push origin :commander
git push origin :gh-pages
git push origin :guard
git push origin :linklog
git push origin :migrator
git push origin :refactor_with_tests
git push origin :rubygemcli
git push origin :site
git push origin :site-2.1
</code></pre>

<p>These commands will remove all the extra branches from your fork.</p>

<h4 id="toc_9">2. Export wordpress posts</h4>

<p>Do this once for each blog you are converting.</p>

<p>This step is pretty simple. Just go to wordpress blog admin -&gt; Tools -&gt; Export, and you are golden.</p>

<p>However, make sure you run <code>xmllint</code> on the exported XML and fix any issue you see. However, watch out for any malformed HTML inside the CDATA section. Not exactly sure how to catch that but just remember exitwp requies parsing of the HTML to convert into markdown.</p>

<h4 id="toc_10">3. Convert using Exitwp</h4>

<p>You only need to do this step once.</p>

<p><code>git clone https://github.com/thomasf/exitwp</code></p>

<p><a href="https://github.com/thomasf/exitwp">Exitwp</a> is</p>

<blockquote>
<p>a tool for making migration from one or more wordpress blogs to the jekyll blog engine as easy as possible.</p>

<p>By default it will try to convert as much information as possible from wordpress but can also be told to filter the amount of data it converts.</p>
</blockquote>

<p>There are some dependencies, so make sure you read through the README.rst before moving forward.</p>

<p>Exitwp works by first parsing the <strong>WHOLE</strong> exported wordpress xml, then write out all the markdown files. The problem with that is if you have a very large site (<a href="http://dataaware.net">Dataaware</a> has over 12K entries, and <a href="http://cloudaware.net">Cloudaware</a> has over 25K posts), it will take a ton of memory. Not to mention you have no feedback on what&rsquo;s going on until the end.</p>

<p>I had one blog post that had some malformed HTML inside the CDATA section, and that somehow caused the parser to spin forever (still not sure what the deal is). I let exitwp run over night not knowing what&rsquo;s going on, but finally I updated exitwp to write out jekyll files one at a time instead of all at the end. I also outputed the blog title so I know which blog is getting stuck.</p>

<p>The following diff (sorry not patch format but that was too long) shows the differences.</p>

<pre><code>95,96d94
&lt;     header = parse_header()
&lt;
164,165d161
&lt;             write_jekyll({'header': header, 'items': export_items}, target_format)
&lt;             export_items = []
366a362
&gt;     write_jekyll(data, target_format)
</code></pre>

<p>In any case, exitwp is an awesome, and required, tool for migrating from wordpress to octopress. Learn to love it.</p>

<p>Put the final XML, from step #2, into wordpress-xml directory under exitwp, and you can run <code>python exitwp.py</code> to convert it.</p>

<p>One of the blogs above mentioned that you may need to fix up the image tags manually, but I didn&rsquo;t check the result as I didn&rsquo;t have many images in my blogs.</p>

<p>After exitwp finishes, you can copy the build/jekyll/<name>/_posts to the source directory under octopress.</p>

<p><code>cp -r build/jekyll/cloudfeed/_posts ../cloudfeed/source/_posts</code></p>

<h4 id="toc_11">4. Set up the Octopress Theme</h4>

<p>You need to do this for each blog you are migrating.</p>

<p>There are a list of themes on <a href="http://opthemes.com">Opthemes</a> you can download for Octopress. Most of them are pretty simple. The ones I used are <a href="https://github.com/elisehein/Pageturner">Pageturner</a> and <a href="https://github.com/shashankmehta/greyshade">Greyshade</a> with minor modifications.</p>

<p>One caveat to remember, after you <code>git clone</code> the theme into <code>.themes</code> directory, is you need to remove the .git directory for that theme, or else you won&rsquo;t be able to save a copy to your own repo. So <code>rm -rf .themes/&lt;theme_name&gt;/.git</code>.</p>

<p>Another caveat to remember is some of the themes have missing files, most likely they expect the Octopress classis theme to be installed. For example, there&rsquo;s a bunch of files missing from Greyshade if you haven&rsquo;t previously installed classic. So the best way to install the theme is</p>

<pre><code>rake install
rake &quot;install[greyshade]&quot;
</code></pre>

<p>Personally I don&rsquo;t really like it as it adds unncessary files to the source and sass directories, so I just copied the necessary files over to the new theme. You may not want to go through that level of pain.</p>

<h4 id="toc_12">5. Generate the Static Site</h4>

<p>You need to do this for each blog you are migrating, and each time you update the site.</p>

<p><code>rake generate</code></p>

<p>This command will generate the site for you. It is an easy step but could take forever. For <a href="http://dataaware.net">Dataaware</a> I have over 12K posts. It takes <strong>1 hour</strong> to generate the site on my Macbook Air (1.8 GHz i5, 4GB memory, SSD). For <a href="http://cloudfeed.net">Cloudfeed</a> with 5700+ posts, it takes 20-25 minutes. For <a href="http://cloudaware.net">Cloudaware</a> with over 25K posts, I would expect the generation process to take well over 2 hours.</p>

<p>Hopefully at some point jekyll can do incremental generation coz this really sucks (yes it&rsquo;s a technical word.)</p>

<h4 id="toc_13">6. Preview the Site</h4>

<p>To preview the site before publishing, you can run <code>rake preview</code>. It will start a web server on port 4000, so you can go to &ldquo;<a href="http://localhost:4000&quot;">http://localhost:4000&quot;</a> to see what your site looks like before publishing. It&rsquo;s great for reviewing new posts.</p>

<h4 id="toc_14">7. Prepare for Publishing</h4>

<p>If you are publishing to github, you need to run the following command to setup the github repo. Read the <a href="http://octopress.org/docs/deploying/github/">Deploying to Github Pages</a> page very carefully and follow the instructions there.</p>

<p>One thing to note is that octopress will do a git pull on the repo at the beginning of the deploy process. Most of the time this isn&rsquo;t a problem because the git repo in the _deploy directory aren&rsquo;t tracking any remotes. However if you had checked out the branch (probably because you are trying to fix some git push issues), AND you have a very large site, the git pull will take a long time to finish.</p>

<p>In any case, run the following command <code>rake setup_github_pages</code> to set the git repo to publish to.</p>

<h4 id="toc_15">8. Publish the Site</h4>

<p><code>rake deploy</code></p>

<p>Octopress will copy all the files to the _deploy directory and push the site to the repo you setup in step #7.</p>

<p>The problem is sometimes you will run into git merge problems and octopress will complain</p>

<pre><code>## Pushing generated \_deploy website
To ssh://git@github.com/zhenjl/trustpath.news.web
! [rejected]        gh-pages -&gt; gh-pages (non-fast-forward)
error: failed to push some refs to 'ssh://git@github.com/zhenjl/trustpath.news.web'
hint: Updates were rejected because the tip of your current branch is behind
hint: its remote counterpart. Merge the remote changes (e.g. 'git pull')
hint: before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.
</code></pre>

<p>To fix this, I had to</p>

<pre><code>cd _deploy
git pull origin gh-pages
git commit -a -m &quot;merging&quot;
</code></pre>

<p>This will at least get git to settle down. Then I do a <code>rake deploy</code> again which will overwite all the files and finally it will push successfully.</p>

<p>You can also combine this and step #5 (generate) using <code>rake gen_deploy</code>.</p>

<h4 id="toc_16">9. Commit your source</h4>

<p>By now you have reviewed your spanking new website and are probably pretty happy you have gotten this far. But don&rsquo;t forget the last thing which is to save your site&rsquo;s source.</p>

<p>In step #1 you had setup the remote origin for this octopress clone. You should now add all the files you have created and commit them to github.</p>

<pre><code>git add .
git commit -a -m &quot;first commit after conversion&quot;
git push origin master
</code></pre>

<p>Oh one last thing, if you use vim, be sure to add these to .gitignore.</p>

<pre><code>*.swp
*.un~
</code></pre>

<hr />

<p>So finally after all this we have a working site. I can tell immediately that pages are served up much faster (sorry no data/numbers). I am sure it has to do with both 1) the site is all static pages, and 2) it&rsquo;s served by github rather than the little VPS.</p>

<p>I am not quite ready to delete all the wordpress setups yet as the Octopress site generation takes hours. I will have to see if this painful step will become a deal breaker. Will keep you all posted on that.</p>

<hr />

<p><a href="https://news.ycombinator.com/item?id=6236080">Comments</a></p>

  </div>
  
</div>
</div>

  </body>
</html>
