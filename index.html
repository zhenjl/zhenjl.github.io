<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Zen 3.1 &middot; Zen 3.1 </title>

  
  <link rel="stylesheet" href="http://zhen.org/css/poole.css">
  <link rel="stylesheet" href="http://zhen.org/css/syntax.css">
  <link rel="stylesheet" href="http://zhen.org/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://zhen.org/index.xml" rel="alternate" type="application/rss+xml" title="Zen 3.1" />

  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-681691-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a href="http://zhen.org">Zen 3.1</a></h1>
      <p class="lead">
       Product. Data. Code 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="http://zhen.org/blog">Archive</a></li>
      <br/>
      <li>Projects</li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/strace/sequence">sequence</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surgemq/surgemq">surgemq</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surge">zhenjl/others</a></li>
      
    </ul>

    <a href="https://twitter.com/zhenjl"><i class="fa fa-twitter-square"></i></a>&nbsp;&nbsp;
    <a href="http://linkedin.com/in/zhenjl"><i class="fa fa-linkedin-square"></i></a>&nbsp;&nbsp;
    
    
    

    <p class="footnote">Powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    Theme originally made by <a href="http://twitter.com/mdo">@mdo</a> <br/>
    Theme modified by <a href="http://npf.io">Nate Finch</a> <br/>
    &copy; 2015 Jian Zhen. All Rights Reserved.</p>
    
  </div>
</div>

    <div class="content container">
<div class="posts">

  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/papers-i-read-2015-week-7/">
        Papers I Read: 2015 Week 7
      </a>
    </h1>

    <span class="post-date">Sun, Feb 15, 2015</span>

    

<h2 id="random-ramblings:10248ddcf524aa32198bc5cded1aa098">Random Ramblings</h2>

<p>Well, another week, <a href="http://www.nytimes.com/2015/02/05/business/hackers-breached-data-of-millions-insurer-says.html">another big data breach</a>. This time is Anthem, one of the nation’s largest health insurers. Ok, maybe it was last week that it happend. But this week they revealed that <a href="http://consumerist.com/2015/02/13/anthem-says-data-from-as-far-back-as-2004-exposed-during-hack-offering-free-identity-theft-protection/">hackers had access &hellip; going back as far as 2004</a>. WSJ blamed Anthem for <a href="http://www.wsj.com/articles/investigators-eye-china-in-anthem-hack-1423167560">not encrypting the data</a>. Though I have to agree with Rich Mogull over at Securosis that &ldquo;<a href="https://securosis.com/blog/even-if-anthem-encrypted-it-probably-wouldnt-have-mattered">even if Anthem had encrypted, it probably wouldn’t have helped</a>&rdquo;.</p>

<p>I feel bad for saying this but there&rsquo;s one positive side effect from all these data breaches. Security is now officially a boardroom topic. Anthem&rsquo;s CEO, Joseph Swedish, is now <a href="http://www.latimes.com/business/la-fi-anthem-hack-ceo-20150213-story.html#page=1">under the gun</a> because top level executives are no longer immune to major security breaches that affect the company&rsquo;s top line. Just ask <a href="http://www.forbes.com/sites/ericbasu/2014/06/15/target-ceo-fired-can-you-be-fired-if-your-company-is-hacked/">Target’s CEO Gregg Steinhafel</a>, or <a href="http://abcnews.go.com/Entertainment/wireStory/sony-chief-amy-pascal-acknowledges-fired-28918607">Sony&rsquo;s Co-Chairwoman Amy Pascal</a>.</p>

<p>Brian Krebs wrote a <a href="http://krebsonsecurity.com/2015/02/anthem-breach-may-have-started-in-april-2014/">detailed piece</a> analyzing the various pieces of information available relating to the Anthem hack. Quite an interesting read.</p>

<p>One chart in the artile that Brian referred to is the time difference between the “time to compromise” and the “time to discovery&rdquo;, taken from <a href="http://www.verizonenterprise.com/DBIR/2014/">Verizon’s 2014 Data Breach Investigations Report</a>. As Brian summaries, &ldquo;TL;DR: That gap is not improving, but instead is widening.&rdquo;</p>

<p>What this really says is that, <strong>you will get hacked</strong>. So how do you shorten the time between getting hacked, and finding out that you are hacked so you can quickly remediate the problem before worse things happen?</p>

<p><img src="http://krebsonsecurity.com/wp-content/uploads/2015/02/timetocompromise.png" alt="The time difference between the “time to compromise” and the “time to discovery.”" />
</p>

<p>With all these data breaches as backdrop, this week we also saw &ldquo;President Barack Obama signed an executive order on Friday designed to spur businesses and the Federal Government to share with each other information related to cybersecurity, hacking and data breaches for the purpose of safeguarding U.S. infrastructure, economics and citizens from cyber attacks.&rdquo; (<a href="https://gigaom.com/2015/02/13/obamas-executive-order-calls-for-sharing-of-security-data/">Gigaom</a>)</p>

<p>In general I don&rsquo;t really think government mandates like this will work. The industry has to feel the pain enough that they are willing to participate, otherwise it&rsquo;s just a waste of paper and ink. Facebook seems to be taking a lead in security information sharing and <a href="https://www.facebook.com/notes/protect-the-graph/threatexchange-sharing-for-a-safer-internet/1566584370248375">launched their ThreatExchange security framework</a> this week. along with Pinterest, Tumblr, Twitter, and Yahoo. Good for them! I hope this is not a temporary PR thing, and that they keep funding and supporting the framework.</p>

<h2 id="papers-i-read:10248ddcf524aa32198bc5cded1aa098">Papers I Read</h2>

<p>Another great resource of computer science papers is Adrian Coyler&rsquo;s <a href="http://blog.acolyer.org/">the morning paper</a>. He selects and summarizes &ldquo;an interesting/influential/important paper from the world of CS every weekday morning&rdquo;.</p>

<ul>
<li><a href="http://www.cs.put.poznan.pl/dweiss/site/publications/download/fsacomp.pdf">Smaller Representation of Finite State Automata</a></li>
</ul>

<p>I read this paper when I was trying to figure out how to make the FSAs smaller for the <a href="https://github.com/surge/xparse/tree/master/etld">Effective TLD matcher</a> I created. The FSM I generated is 212,294 lines long. That&rsquo;s just absolutely crazy. This paper seems to present an interesting way of compressing them.</p>

<p>I am not exactly sure if <a href="https://godoc.org/golang.org/x/net/publicsuffix">PublicSuffix</a> uses a similar representation but it basically represents a FSA as an array of bytes, and then walk the bytes like a binary search tree. It&rsquo;s interesting for sure.</p>

<blockquote>
<p>This paper is a follow-up to Jan Daciuk’s experiments on space-efficient finite state automata representation that can be used directly for traversals in main memory [4]. We investigate several techniques of reducing the memory footprint of minimal automata, mainly exploiting the fact that transition labels and transition pointer offset values are not evenly distributed and so are suitable for compression. We achieve a size gain of around 20–30% compared to the original representation given in [4]. This result is comparable to the state-of-the-art dictionary compression techniques like the LZ-trie [12] method, but remains
memory and CPU efficient during construction.</p>
</blockquote>

<ul>
<li><a href="http://arxiv.org/pdf/1409.5942v1.pdf">IP Tracing and Active Network Response</a></li>
</ul>

<blockquote>
<p>This work presents integrated model for active security response model. The proposed model introduces Active Response Mechanism (ARM) for tracing anonymous attacks in the network back to their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or “spoofed”, source addresses. This paper presents within the proposed model two tracing approaches based on:
• Sleepy Watermark Tracing (SWT) for unauthorized access attacks.
• Probabilistic Packet Marking (PPM) in the network for Denial of Service
(DoS) and Distributed Denial of Service (DDoS) attacks.</p>
</blockquote>

<ul>
<li><a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36356.pdf">Dapper, a Large-Scale Distributed Systems Tracing Infrastructure</a></li>
</ul>

<blockquote>
<p>Here we introduce the design of Dapper, Google’s production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design
choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries.</p>
</blockquote>

<ul>
<li><a href="http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/">STREAM PROCESSING, EVENT SOURCING, REACTIVE, CEP… AND MAKING SENSE OF IT ALL</a></li>
</ul>

<p>Not a paper, but a good write up nonetheless.</p>

<blockquote>
<p>Some people call it stream processing. Others call it Event Sourcing or CQRS. Some even call it Complex Event Processing. Sometimes, such self-important buzzwords are just smoke and mirrors, invented by companies who want to sell you stuff. But sometimes, they contain a kernel of wisdom which can really help us design better systems. In this talk, we will go in search of the wisdom behind the buzzwords. We will discuss how event streams can help make your application more scalable, more reliable and more maintainable.</p>
</blockquote>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">
        Sequence: Optimizing Go For the High Performance Log Scanner
      </a>
    </h1>

    <span class="post-date">Fri, Feb 13, 2015</span>

    

<p><a href="http://godoc.org/github.com/strace/sequence"><img src="http://godoc.org/github.com/strace/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="http://godoc.org/github.com/strace/sequence/cmd/sequence"><img src="http://godoc.org/github.com/strace/sequence/cmd/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="https://github.com/strace/sequence">github repo</a></p>

<p>This is part 3 of the <a href="http://strace.io/sequence">sequence</a> series.</p>

<ul>
<li><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Part 1</a> is about the high performance parser that can parse 100,000-200,000 MPs.</li>
<li><a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Part 2</a> is about automating the process of reducing 100 of 1000&rsquo;s of log messages down to dozens of unique patterns.</li>
<li><a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">Part 3</a> is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size and core count) for scanning and tokenizing log messages</li>
</ul>

<p>I would love to learn more about the state-of-the-art approaches that log vendors are using. These attempts are about scratching my own itch and trying to realize ideas I&rsquo;ve had in my mind. Given some of these ideas are 5 to 10 years old, they may already be outdated. Personally I just haven&rsquo;t heard of any groundbreaking approaches.</p>

<p>In any case, if you know of some of the more innovative ways people are approaching these problems, please please please comment below as I would love to hear from you.</p>

<h3 id="tl-dr:cb54f18a9944e1962d3fe8e3f09ea809">tl;dr</h3>

<ul>
<li>The <code>sequence</code> scanner is designed to tokenize free-form log messages.

<ul>
<li>It can scan between 200K to 500K log messages per second depending on message size and core count.</li>
<li>It recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.</li>
<li>The design is based mostly on finite-state machines.</li>
</ul></li>
<li>The performance was achieved by the following techniques:

<ol>
<li>Go Through the String Once and Only Once</li>
<li>Avoid Indexing into the String</li>
<li>Reduce Heap Allocation</li>
<li>Reduce Data Copying</li>
<li>Mind the Data Struture</li>
<li>Avoid Interfaces If Possible</li>
<li>Find Ways to Short Circuit Checks</li>
</ol></li>
</ul>

<h2 id="background:cb54f18a9944e1962d3fe8e3f09ea809">Background</h2>

<blockquote>
<p>In computer science, lexical analysis is the process of converting a sequence of characters into a sequence of tokens, i.e. meaningful character strings. A program or function that performs lexical analysis is called a lexical analyzer, lexer, tokenizer, or scanner. - <a href="http://en.wikipedia.org/wiki/Lexical_analysis">Wikipedia</a></p>
</blockquote>

<p>One of the most critical functions in the <code>sequence</code> parser is the message tokenization. At a very high level, message tokenization means taking a single log message and breaking it into a list of tokens.</p>

<h3 id="functional-requirements:cb54f18a9944e1962d3fe8e3f09ea809">Functional Requirements</h3>

<p>The challenge is knowing where the token break points are. Most log messages are free-form text, which means there&rsquo;s no common structure to them.</p>

<p>As an example, the following log message can be tokenized into the sequence of tokens below. As you can see, one cannot depend on white spaces to tokenize, as the timestamp would be broken into 3 parts; nor can one use punctuations like &ldquo;;&rdquo; or &ldquo;:&ldquo;, as they would break the log mesage into useless parts.</p>

<pre><code>jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream

  #   0: { Field=&quot;%funknown%&quot;, Type=&quot;%ts%&quot;, Value=&quot;jan 14 10:15:56&quot; }
  #   1: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;testserver&quot; }
  #   2: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;sudo&quot; }
  #   3: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   4: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;gonner&quot; }
  #   5: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   6: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;tty&quot; }
  #   7: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #   8: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;pts/3&quot; }
  #   9: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  10: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;pwd&quot; }
  #  11: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  12: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/home/gonner&quot; }
  #  13: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  14: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;user&quot; }
  #  15: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  16: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;root&quot; }
  #  17: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  18: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;command&quot; }
  #  19: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  20: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/bin/su&quot; }
  #  21: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;-&quot; }
  #  22: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;ustream&quot; }
</code></pre>

<p>So a log message <em>scanner</em> or <em>tokenizer</em> (we will use these terms interchangeably) must understand common components such as timestamp, URL, hex strings, IP addresses (v4 or v6), and mac addresses, so it can break the messages into <em>meaningful components</em>.</p>

<h3 id="performance-requirements:cb54f18a9944e1962d3fe8e3f09ea809">Performance Requirements</h3>

<p>From a performance requirements perspective, I really didn&rsquo;t start out with any expectations. However, after achieving 100-200K MPS for parsing (not just tokenizing), I have a strong desire to keep the performance at that level. So the more I can optimize the scanner to tokenize faster, the more head room I have for parsing.</p>

<p>One may ask, who can POSSIBLY use such performance? Many organizations that I know are generating between 50-100M messages per second (MPS), that&rsquo;s only 1,200 MPS. Some larger organizations I know are generating 60GB of Bluecoat logs per day, <strong>8 years ago</strong>!! That&rsquo;s a good 3,000 MPS assuming an average of 250 bytes per message. Even if log rate grows at 15%, that&rsquo;s still only 10K MPS today.</p>

<p>To run through an example, <a href="http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf">at EMC, 1.4 billion log messages are generated daily on average, at a rate of one terabyte a day</a>. That&rsquo;s 16,200 messages per second, and about 714 bytes per message. (Btw, what system can possibly generate messages that are 714 bytes long? That&rsquo;s crazy and completely inefficient!) These EMC numbers are from 2013, so they have likely increased by now.</p>

<p>The <code>sequence</code> parser, with a single CPU core, can process about 270,000 MPS for messages averaging 98 bytes. Assuming the performance is linear compare to the message size (which is pretty close to the truth), we can process 37,000 MPS for messages averaging 714 bytes. That&rsquo;s just enough to parse the 16,2000 MPS, with a little head room to do other types of analysis or future growth.</p>

<p>Obviously one can throw more hardware at solving the scale problem, but then again, why do that if you don&rsquo;t need to. Just because you have the hardware doesn&rsquo;t mean you should waste the money! Besides, there are much more interesting analytics problems your hardware can be used for than just tokenizing a message.</p>

<p>In any case, I want to squeeze every oz of performance out of the scanner so I can have more time in the back to parse and analyze. So let&rsquo;s set a goal of keeping at least 200,000 MPS for 100 bytes per message (BPM).</p>

<p>Yes, go ahead and tell me I shouldn&rsquo;t worry about micro-optimization, because this post is all about that. :)</p>

<h2 id="sequence-scanner:cb54f18a9944e1962d3fe8e3f09ea809">Sequence Scanner</h2>

<p>In the <code>sequence</code> package, we implemented a general log message scanner, called <a href="https://github.com/strace/sequence/blob/master/scanner.go">GeneralScanner</a>. GeneralScanner is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.</p>

<p>This implementation was able to achieve both the functional and performance requirements. The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro, although the tests were only using 1 or 2 cores. The first file is a bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message. Last is a mix of ASA, sshd and sudo logs, averaging 136 bytes per message.</p>

<pre><code>  $ ./sequence bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272869.35 msgs/sec

  $ ./sequence bench scan -i ../../data/allasa.log
  Scanned 234815 messages in 1.43 secs, ~ 163827.61 msgs/sec

  $ ./sequence bench scan -i ../../data/allasassh.log
  Scanned 447745 messages in 2.27 secs, ~ 197258.42 msgs/sec
</code></pre>

<p>Performance can be improved by adding more cores:</p>

<pre><code>  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/sshd.all -w 2
  Scanned 212897 messages in 0.43 secs, ~ 496961.52 msgs/sec

  $ GOMAXPROCS=2 ./sequenceo bench scan -i ../../data/allasa.log -w 2
  Scanned 234815 messages in 0.80 secs, ~ 292015.98 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/allasassh.log -w 2
  Scanned 447745 messages in 1.20 secs, ~ 373170.45 msgs/sec
</code></pre>

<h3 id="concepts:cb54f18a9944e1962d3fe8e3f09ea809">Concepts</h3>

<p>To understand the scanner, you have to understand the following concepts that are part of the package.</p>

<ul>
<li><p>A <em>Token</em> is a piece of information extracted from the original log message. It is a struct that contains fields for <em>TokenType</em>, <em>FieldType</em>, and <em>Value</em>.</p></li>

<li><p>A <em>TokenType</em> indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.</p></li>

<li><p>A <em>FieldType</em> indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).</p></li>

<li><p>A <em>Sequence</em> is a list of Tokens. It is the key data structure consumed and returned by the <em>Scanner</em>, <em>Analyzer</em>, and the <em>Parser</em>.</p></li>
</ul>

<p>Basically, the scanner takes a log message string, tokenizes it and returns a <em>Sequence</em> with the recognized <em>TokenType</em> marked. This <em>Sequence</em> is then fed into the analyzer or parser, and the analyzer or parser in turn returns another <em>Sequence</em> that has the recognized <em>FieldType</em> marked.</p>

<h3 id="design:cb54f18a9944e1962d3fe8e3f09ea809">Design</h3>

<p>Tokenizers or scanners are usually implemented using finite-state machines. Each FSM (or FSA, finite state automata) understands a specific sequences of characters that make up a type of token.</p>

<p>In the <code>sequence</code> scanner, there are three FSMs: Time, HexString and General.</p>

<ul>
<li>The Time FSM understands a list of <a href="https://github.com/strace/sequence/blob/master/time.go">time formats</a>. This list of time formats are commonly seen in log messages. It is also fairly easy to add to this list if needed.</li>
<li>The HexString FSM is designed to understand IPv6 addresses (dead:beef:1234:5678:223:32ff:feb1:2e50 or f0f0:f::1), MAC addresses (00:04:c1:8b:d8:82), fingerprints or signatures (de:ad:be:ef:74:a6:bb:45:45:52:71:de:b2:12:34:56).</li>
<li>The General FSM that recognizes URLs, IPv4 addresses, and any literal or strings.</li>
</ul>

<p>Each character in the log string are run through all three FSMs.</p>

<ol>
<li>If a time format is matched, that&rsquo;s what it will be returned.</li>
<li>Next if a hex string is matched, it is also returned.

<ul>
<li>We mark anything with 5 colon characters and no successive colons like &ldquo;::&rdquo; to be a MAC address.</li>
<li>Anything that has 7 colons and no successive colons are marked as IPv6 address.</li>
<li>Anything that has less than 7 colons but has only 1 set of successive colons like &ldquo;::&rdquo; are marked as IPv6 address.</li>
<li>Everything else is just a literal.</li>
</ul></li>
<li>Finally if neither of the above matched, we return what the general FSM has matched.

<ul>
<li>The general FSM recognizes these quote characters: &ldquo;, &lsquo; and &lt;. If these characters are encountered, then it will consider anything between the quotes to be a single token.</li>
<li>Anything that starts with http:// or https:// are considered URLs.</li>
<li>Anything that matches 4 integer octets are considered IP addresses.</li>
<li>Anything that matches two integers with a dot in between are considered floats.</li>
<li>Anything that matches just numbers are considered integers.</li>
<li>Everything else are literals.</li>
</ul></li>
</ol>

<h3 id="performance:cb54f18a9944e1962d3fe8e3f09ea809">Performance</h3>

<p>To achieve the performance requirements, the following rules and optimizations are followed. Some of these are Go specific, and some are general recommendations.</p>

<p><strong>1. Go Through the String Once and Only Once</strong></p>

<p>This is a hard requirement, otherwise we can&rsquo;t call this project a <em>sequential</em> parser. :)</p>

<p>This is probably a pretty obvious technique. The more times you loop through loop through a string, the lower the performance. If you used regular expressions to parse logs, you will likely go through parts of the log message multiple times due to back tracking or look forward, etc.</p>

<p>I took great pain to ensure that I don&rsquo;t need to look forward or look backward in the log string to determine the current token type, and I think the effort paid off.</p>

<p>In reality though, while I am only looping through the log string once, and only once, I do run each character through three different FSMs. However, it is still much less expensive than looping through three times, each time checking a single FSM. However, the more FSMs I run the characters through, the slower it gets.</p>

<p>This was apparently when I <a href="https://github.com/strace/sequence/commit/a5447814f43b4b9b7e804b14dde38e88fd53e6d0">updated the scanner to support IPv6 and hex strings</a>. I tried a couple of different approaches. First, I added an IPv6 specific FSM. So in addition to the original time, mac and general FSMs, there are now 4. That dropped performance by like 15%!!! That&rsquo;s just unacceptable.</p>

<p>The second approach, which is the one I checked in, combines the MAC, IPv6 and general hex strings into a single FSM. That helped somewhat. I was able to regain about 5% of the performance hit. However, because I can no longer short circuit the MAC address check (by string length and colon positions), I was still experiencing a 8-10% hit.</p>

<p>What this means is that for most tokens, instead of checking just 2 FSMs because I can short circuit the MAC check pretty early, I have to now check all 3 FSMs.</p>

<p>So the more FSMs, the more comlicated the FSMs, the more performance hits there will be.</p>

<p><strong>2. Avoid Indexing into the String</strong></p>

<p>This is really a Go-specific recommentation. Each time you index into a slice or string, Go will perform bounds checking for you, which means there&rsquo;s extra operations it&rsquo;s doing, and also means lower performance. As an example, here are results from two benchmark runs. The first is with bounds checking enabled, which is default Go behavior. The second disables bounds checking.</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.79 secs, ~ 268673.91 msgs/sec

  $ go run -gcflags=-B ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 277479.58 msgs/sec
</code></pre>

<p>The performance difference is approximately 3.5%! However, while it&rsquo;s fun to see the difference, I would never recommend disable bounds checking in production. So the next best thing is to remove as many operations that index into a string or slice as possible. Specifically:</p>

<ol>
<li>Use &ldquo;range&rdquo; in the loops, e.g. <code>for i, r := range str</code> instead of <code>for i := 0; i &lt; len(str); i++ { if str[i] == ... }</code></li>
<li>If you are checking a specific character in the string/slice multiple times, assign it to a variable and use the variable instead. This will avoid indexing into the slice/string multiple times.</li>
<li>If there are multiple conditions in an <code>if</code> statement, try to move (or add) the non-indexing checks to the front of the statement. This sometimes will help short circuit the checks and avoid the slice-indexing checks.</li>
</ol>

<p>One might question if this is worth optimizing, but like I said, I am trying to squeeze every oz of performance so 3.5% is still good for me. Unfornately I do know I won&rsquo;t get 3.5% since I can&rsquo;t remove every operation that index into slice/string.</p>

<p><strong>3. Reduce Heap Allocation</strong></p>

<p>This is true for all languages (where you can have some control of stack vs heap allocation), and it&rsquo;s even more true in Go. Mainly in Go, if you allocate a new slice, Go will &ldquo;zero&rdquo; out the allocated memory. This is great from a safety perspective, but it does add to the overhead.</p>

<p>As an example, in the scanner I originally allocated a new <em>Sequence</em> (slice of <em>Token</em>) for every new message. However, when i changed it to re-use the existing slice, the performance increased by over 10%!</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.87 secs, ~ 246027.12 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 275038.83 msgs/sec
</code></pre>

<p>The best thing to do is to run Go&rsquo;s builtin CPU profiler, and look at the numbers for Go internal functions such as <code>runtime.makeslice</code>, <code>runtime.markscan</code>, and <code>runtime.memclr</code>. Large percentages and numbers for these internal functions are dead giveaway that you are probably allocating too much stuff on the heap.</p>

<p>I religiously go through the SVGs generated from the Go profiler to help me identify hot spots where I can optimize.</p>

<p>Here&rsquo;s also a couple of tips I picked up from the <a href="https://groups.google.com/forum/#!topic/golang-nuts/baU4PZFyBQQ">go-nuts mailing list</a>:</p>

<ul>
<li>Maps are bad&ndash;even if they&rsquo;re storing integers or other non-pointer structs. The implementation appears to have lots of pointers inside which must be evaluated and followed during mark/sweep GC.  Using structures with pointers magnifies the expense.</li>
<li>Slices are surprisingly bad (including strings and substrings of existing strings). A slice is a pointer to the backing array with a length and capacity. It would appear that the internal pointer that causes the trouble because GC wants to inspect it.</li>
</ul>

<p><strong>4. Reduce Data Copying</strong></p>

<p>Data copying is expensive. It means the run time has to allocate new space and copy the data over. It&rsquo;s even more expensive when you can&rsquo;t have do <code>memcpy</code> of a slice in Go like you can in C. Again, direct memory copying is not Go&rsquo;s design goal. It is also much safer if you can prevent users from playing with memory directly too much. However, it is still a good idea to avoid any copying of data, whether it&rsquo;s string or slice.</p>

<p>As much as I can, I try to do in place processing of the data. Every <em>Sequence</em> is worked on locally and I try not to copy <em>Sequence</em> or string unless I absolutely have to.</p>

<p>Unfortunately I don&rsquo;t have any comparison numbers for this one, because I learned from <a href="http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/">previous projects</a> that I should avoid copying as much as possible.</p>

<p><strong>5. Mind the Data Struture</strong></p>

<p>If there&rsquo;s one thing I learned over the past year, is to use the right data structure for the right job. I&rsquo;ve written about other data structures such as <a href="http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/">ring buffer</a>, <a href="http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/">bloom filters</a>, and <a href="http://zhen.org/blog/go-skiplist/">skiplist</a> before.</p>

<p>However, <a href="http://en.wikipedia.org/wiki/Finite-state_machine">finite-state automata or machine</a> is my latest love and I&rsquo;ve been using it at various projects such as my <a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">porter2</a> and <a href="https://github.com/surge/xparse/tree/master/etld">effective TLD</a>. Ok, technical FSM itself is not a data structure and can be implemented in different ways. In the <code>sequence</code> project, I used both a tree representation as well as a bunch of switch-case statements. For the <a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">porter2</a> FSMs, I used switch-case to implement them.</p>

<p>Interestingly, swtich-case doesn&rsquo;t always win. I tested the time FSM using both tree and switch-case implementations, and the tree actually won out. (Below, 1 is tree, 2 is switch-case.) So guess which one is checked in?</p>

<pre><code>BenchmarkTimeStep1   2000000         696 ns/op
BenchmarkTimeStep2   2000000         772 ns/op
</code></pre>

<p>Writing this actually reminds me that in the parser, I am currently using a tree to parse the sequences. While parsing, there could be multiple paths that the sequence will match. Currently I walk all the matched paths fully, before choosing one that has the highest score. What I should do is to do a weighted walk, and always walk the highest score nodes first. If at the end I get a perfect score, I can just return that path and not have to walk the other paths. (Note to self, more parser optimization to do).</p>

<p><strong>6. Avoid Interfaces If Possible</strong></p>

<p>This is probably not a great advice to give to Go developers. Interface is probably one of the best Go features and everyone should learn to use it. However, if you want high performane, avoid interfaces as it provides additional layers of indirection. I don&rsquo;t have performance numbers for the <code>sequence</code> project since I tried to avoid interfaces in high performance areas from the start. However, previous in the <a href="http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/">ring buffer</a> project, the version that uses interface is 140% slower than the version that didn&rsquo;t.</p>

<p>I don&rsquo;t have the direct link but someone on the go-nuts mailing list also said:</p>

<blockquote>
<p>If you really want high performance, I would suggest avoiding interfaces and, in general, function calls like the plague, since they are quite expensive in Go (compared to C). We have implemented basically the same for our internal web framework (to be released some day) and we&rsquo;re almost 4x faster than encoding/json without doing too much optimization. I&rsquo;m sure we could make this even faster.</p>
</blockquote>

<p><strong>7. Find Ways to Short Circuit Checks</strong></p>

<p>Find ways to quickly eliminate the need to run a section of the code has been tremendously helpful to improve performance. For example, here are a couple of place where I tried to do that.</p>

<p>In <a href="https://github.com/strace/sequence/blob/master/scanner.go#L223">this first example</a>, I simply added <code>l == 1</code> before the actual equality check of the string values. The first output is before the add, the second is after. The difference is about 2% performance increase.</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272303.79 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 278433.34 msgs/sec
</code></pre>

<p>In <a href="https://github.com/strace/sequence/blob/master/scanner.go#L282">the second example</a>, I added a quick check to make sure the remaining string is at least as long as the shortest time format. If there&rsquo;s not enough characters, then don&rsquo;t run the time FSM. The performance difference is about 2.5%.</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272059.04 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 279388.47 msgs/sec
</code></pre>

<p>So by simply adding a couple of checks, I&rsquo;ve increased perfromance by close to 5%.</p>

<h2 id="conclusion:cb54f18a9944e1962d3fe8e3f09ea809">Conclusion</h2>

<p>At this point I think I have squeezed every bit of performance out of the scanner, to the extend of my knowledge. It&rsquo;s performing relatively well and it&rsquo;s given the parser plenty of head room to do other things. I hope some of these lessons are helpful to whatever you are doing.</p>

<p>Feel free to take a look at the <a href="https://github.com/strace/sequence">sequence</a> project and try it out if you. If you have any issues/comments, please don&rsquo;t hestiate to <a href="https://github.com/strace/sequence/issues">open a github issue</a>.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">
        Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns
      </a>
    </h1>

    <span class="post-date">Tue, Feb 10, 2015</span>

    

<p><a href="http://godoc.org/github.com/strace/sequence"><img src="http://godoc.org/github.com/strace/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="http://godoc.org/github.com/strace/sequence/cmd/sequence"><img src="http://godoc.org/github.com/strace/sequence/cmd/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="https://github.com/strace/sequence">github repo</a></p>

<p>This is part 2 of the <code>sequence</code> series. [</p>

<ul>
<li><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Part 1</a> is about the high performance parser that can parse 100,000-200,000 MPs.</li>
<li><a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Part 2</a> is about automating the process of reducing 100 of 1000&rsquo;s of log messages down to dozens of unique patterns.</li>
<li><a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">Part 3</a> is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size) for scanning and tokenizing log messages</li>
</ul>

<h2 id="background:61b1adc09fb9bc31a28d4faabfef3631">Background</h2>

<p>This post really takes me down the memory lane. Back in 2005, while I was at LogLogic, we envisioned an automated approach to tagging, or labeling, log messages. More specifically, we wanted to automatically tag specific components within the log messages with their semantic label, such as a source IP address, or a target user.</p>

<p>At the time, much like it is still today, the message parsing process is performed manually. This means someone has to manually look at the object and decided that the object should be labeled “user” or “targetUser.” An  analyst has to go through the log data, create a regular expression that extracts the useful strings out, and then finally assigning these to a specific label. This is extremely time consuming and error-prone.</p>

<p>At that time, the vision was to provide an automated approach to universally parse and analyze ANY log data. The key phrase being “automated approach.” This means the users should only need to provide minimum guidance to the system, if any, for the platforms to be able to analyze the log data. LogLogic never did much with this, unfortunately.</p>

<p>However, the tagging concept was later on adopted by (and I know how this got into CEE :) the <a href="http://cee.mitre.org/">Common Event Expression, or CEE</a> effort by Mitre. This idea of tags also inspired <a href="http://www.liblognorm.com/">liblognorm</a> to develop their <a href="http://www.libee.org/">libee</a> library and <a href="http://www.liblognorm.com/news/log-classification-with-liblognorm/">tagging system</a>. Rsyslog&rsquo;s <a href="http://www.rsyslog.com/doc/mmnormalize.html">mmnormalize</a> module is based on liblognorm.</p>

<p>And then there&rsquo;s Fedora&rsquo;s <a href="https://fedorahosted.org/lumberjack/">Project Lumberjack</a>, which &ldquo;is an open-source project to update and enhance the event log architecture&rdquo; and &ldquo;aims to improve the creation and standardize the content of event logs by implementing the concepts and specifications proposed by the ​Common Event Expression (CEE).&rdquo;</p>

<p>Then finally <a href="http://logstash.net/">logstash</a> has their <a href="http://logstash.net/docs/1.4.2/filters/grok">grok filter</a> that basically does similar extraction of unstructured data into a structured and queryable format. However, it seems like there might be some <a href="http://ghost.frodux.in/logstash-grok-speeds/">performance bottlenecks</a>.</p>

<p>However, none of these efforts attempted to solve the automated tagging/labeling problem. They mostly just try to provide a parser for log messages.</p>

<p>Also, it looks like many of these efforts have all been abandoned or put in hibernation, and haven&rsquo;t been updated since 2012 or 2013. liblognrom did put out <a href="http://www.liblognorm.com/news/">a couple of updates</a> in the past couple of years. Logstash&rsquo;s grok obviously is being maintained and developed with the <a href="http://www.elasticsearch.com/">Elasticsearch</a> backing.</p>

<p>It is understandable, unfortunately. Log parsing is <strong>BORING</strong>. I mean, who wants to sit there and stare at logs all day and try to come up with regular expressions or other types of parsing rules? LogLogic used to have a team of LogLabs analysts that did that, and I have to say I truly appreciated their effort and patience, because I cannot do that.</p>

<h3 id="the-end-result:61b1adc09fb9bc31a28d4faabfef3631">The End Result</h3>

<p>So instead of writing rules all day long, I decided to create an analyzer that can help us get at least 75% of the way there. The end result is the <code>Analyzer</code>, written in <a href="http://golang.org">Go</a>, in the <a href="https://github.com/strace/sequence">sequence</a> project I created. Here are some preliminary results. Below, we analyzed 2 files. The first is a file with over 200,000 sshd messages. The second is a file with a mixture of ASA, sshd, sudo and su log messages. It contains almost 450,000 messages.</p>

<p>By running the analyzer over these logs, the pure sshd log file returned 45 individual patterns, and the second returned 103 unique patterns.</p>

<pre><code>$ go run sequence.go analyze -i ../../data/sshd.all -o sshd.analyze
Analyzed 212897 messages, found 45 unique patterns, 45 are new.

$ go run sequence.go analyze -i ../../data/asasshsudo.log -o asasshsudo.analyze
Analyzed 447745 messages, found 103 unique patterns, 103 are new.
</code></pre>

<p>And the output file has entries such as:</p>

<pre><code>%msgtime% %apphost% %appname% [ %sessionid% ] : %status% %method% for %srcuser% from %srcipv4% port %srcport% ssh2
# Jan 15 19:39:26 irc sshd[7778]: Accepted password for jlz from 108.61.8.124 port 57630 ssh2

%msgtime% %appipv4% %appname% : %action% outbound %protocol% connection %sessionid% for %string% : %srcipv4% / %srcport% ( %ipv4% / %integer% ) to %string% : %dstipv4% / %dstport% ( %ipv4% / %integer% )
# 2012-04-05 18:46:18   172.23.0.1  %ASA-6-302013: Built outbound TCP connection 1424575 for outside:10.32.0.100/80 (10.32.0.100/80) to inside:172.23.73.72/2522 (10.32.0.1/54702)

%msgtime% %apphost% %appname% : %string% : tty = %string% ; pwd = %string% ; user = %srcuser% ; command = %command% - %string%
# Jan 15 14:09:11 irc sudo:    jlz : TTY=pts/1 ; PWD=/home/jlz ; USER=root ; COMMAND=/bin/su - irc
</code></pre>

<p>As you can see, the output is not 100%, but it gets us pretty close. Once the analyst goes through and updates the rules, he/she can re-run the analyzer anytime with any file to determine if there&rsquo;s new patterns. For example, below, we ran the sshd log file with an existing pattern file, and got 4 new log patterns.</p>

<pre><code>$ go run sequence.go analyze -i ../../data/sshd.all -p ../../patterns/sshd.txt -o sshd.analyze
Analyzed 212897 messages, found 39 unique patterns, 4 are new.
</code></pre>

<h3 id="parser-quick-review:61b1adc09fb9bc31a28d4faabfef3631">Parser - Quick Review</h3>

<p>I wrote about the <a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">sequence parser</a> a couple of weeks back. It is a <em>high performance sequential log parser</em>. It <em>sequentially</em> goes through a log message, <em>parses</em> out the meaningful parts, without the use regular expressions. It can achieve <em>high performance</em> parsing of <strong>100,000 - 200,000 messages per second (MPS)</strong> without the need to separate parsing rules by log source type. Underneath the hood, the <code>sequence</code> parser basically constructs a tree based on the sequential rules, walks the tree to identify all the possible paths, and returns the path that has the best match (highest weight) for the message.</p>

<p>While the analyzer is about reducing a large corupus of raw log messages down to a small set of unique patterns, the parser is all about matching log messages to an existing set of patters and determining whether a specific pattern has matched. Based on the pattern, it returns a sequence of tokens that basically extracts out the important pieces of information from the logs. The analysts can then take this sequence and perform other types of analysis.</p>

<p>The approach taken by the <code>sequence</code> parser is pretty much the same as liblognorm or other tree-based approaches.</p>

<h2 id="sequence-analyzer:61b1adc09fb9bc31a28d4faabfef3631">Sequence Analyzer</h2>

<p>In the following section I will go through additional details of how the <code>sequence</code> analyzer reduces 100 of 1000&rsquo;s of raw log messages down to just 10&rsquo;s of unique patterns, and then determining how to label the individual tokens.</p>

<h3 id="identifying-unique-patterns:61b1adc09fb9bc31a28d4faabfef3631">Identifying Unique Patterns</h3>

<p>Analyzer builds an analysis tree that represents all the Sequences from messages. It can be used to determine all of the unique patterns for a large body of messages.</p>

<p>It&rsquo;s based on a single basic concept, that for multiple log messages, if tokens in the same position shares one same parent and one same child, then the tokens in that position is likely variable string, which means it&rsquo;s something we can extract. For example, take a look at the following two messages:</p>

<pre><code>Jan 12 06:49:42 irc sshd[7034]: Accepted password for root from 218.161.81.238 port 4228 ssh2
Jan 12 14:44:48 jlz sshd[11084]: Accepted publickey for jlz from 76.21.0.16 port 36609 ssh2
</code></pre>

<p>The first token of each message is a timestamp, and the 3rd token of each message is the literal &ldquo;sshd&rdquo;. For the literals &ldquo;irc&rdquo; and &ldquo;jlz&rdquo;, they both share a common parent, which is a timestamp. They also both share a common child, which is &ldquo;sshd&rdquo;. This means token in between these, the 2nd token in each message, likely represents a variable token in this message type. In this case, &ldquo;irc&rdquo; and &ldquo;jlz&rdquo; happens to
represent the syslog host.</p>

<p>Looking further down the message, the literals &ldquo;password&rdquo; and &ldquo;publickey&rdquo; also share a common parent, &ldquo;Accepted&rdquo;, and a common child, &ldquo;for&rdquo;. So that means the token in this position is also a variable token (of type TokenString).</p>

<p>You can find several tokens that share common parent and child in these two messages, which means each of these tokens can be extracted. And finally, we can determine that the single pattern that will match both is:</p>

<pre><code>%time% %string% sshd [ %integer% ] : Accepted %string% for %string% from %ipv4% port %integer% ssh2
</code></pre>

<p>If later we add another message to this mix:</p>

<pre><code>Jan 12 06:49:42 irc sshd[7034]: Failed password for root from 218.161.81.238 port 4228 ssh2
</code></pre>

<p>The Analyzer will determine that the literals &ldquo;Accepted&rdquo; in the 1st message, and &ldquo;Failed&rdquo; in the 3rd message share a common parent &ldquo;:&rdquo; and a common child &ldquo;password&rdquo;, so it will determine that the token in this position is also a variable token. After all three messages are analyzed, the final pattern that will match all three
messages is:</p>

<pre><code>%time% %string% sshd [ %integer% ] : %string% %string% for %string% from %ipv4% port %integer% ssh2
</code></pre>

<p>By applying this concept, we can effectively identify all the unique patterns in a log file.</p>

<h3 id="determining-the-correct-labels:61b1adc09fb9bc31a28d4faabfef3631">Determining the Correct Labels</h3>

<p>Now that we have the unique patterns, we will scan the tokens to determine which labels we should apply to them.</p>

<p>System and network logs are mostly free form text. There&rsquo;s no specific patterns to any of them. So it&rsquo;s really difficult to determine how to label specific parts of the log message automatically. However, over the years, after looking at so many system and network log messages, some patterns will start to emerge.</p>

<p>There&rsquo;s no &ldquo;machine learning&rdquo; here. This section is all about codifying these human learnings. I&rsquo;ve created the following 6 rules to help label tokens in the log messages. By no means are these rules perfect. They are at best just guesses on how to label. But hopefully they can get us 75% of the way there and we human can just take it the rest of the way.</p>

<p><strong>0. Parsing Email and Hostname Formats</strong></p>

<p>This is technically not a labeling step. Before we actually start the labeling process, we wanted to first parse out a couple more formats like email and host names. The message tokenizer doesn&rsquo;t recognize these because they are difficult to parse and will slow down the tokenizer. These specific formats are also not needed by the parser. So because the analyzer doesn&rsquo;t care about performance as much, we can do this as post-processing step.</p>

<p>To recognize the hostname, we try to match the &ldquo;effective TLD&rdquo; using the <a href="https://github.com/surge/xparse/tree/master/etld">xparse/etld</a> package. It is an effective TLD matcher that returns the length of the effective domain name for the given string. It uses the data set from <a href="https://www.publicsuffix.org/list/effective_tld_names.dat">https://www.publicsuffix.org/list/effective_tld_names.dat</a>.</p>

<p><strong>1. Recognizing Syslog Headers</strong></p>

<p>First we will try to see if we can regonize the syslog headers. We try to recogize both RFC5424 and RFC3164 syslog headers:</p>

<pre><code>	// RFC5424
	// - &quot;1 2003-10-11T22:14:15.003Z mymachine.example.com evntslog - ID47 ...&quot;
	// - &quot;1 2003-08-24T05:14:15.000003-07:00 192.0.2.1 myproc 8710 - ...&quot;
	// - &quot;1 2003-10-11T22:14:15.003Z mymachine.example.com su - ID47 ...&quot;
	// RFC3164
	// - &quot;Oct 11 22:14:15 mymachine su: ...&quot;
	// - &quot;Aug 24 05:34:00 CST 1987 mymachine myproc[10]: ...&quot;
	// - &quot;jan 12 06:49:56 irc last message repeated 6 times&quot;
</code></pre>

<p>If the sequence pattern matches any of the above sequence, then we assume the first few tokens belong to the syslog header.</p>

<p><strong>2. Marking Key and Value Pairs</strong></p>

<p>The next step we perform is to mark known &ldquo;keys&rdquo;. There are two types of keys. First, we identify any token before the &ldquo;=&rdquo; as a key. For example, the message <code>fw=TOPSEC priv=6 recorder=kernel type=conn</code> contains 4 keys: <code>fw</code>, <code>priv</code>, <code>recorder</code> and <code>type</code>. These keys should be considered string literals, and should not be extracted. However, they can be used to determine how the value part should be labeled.</p>

<p>The second types of keys are determined by keywords that often appear in front of other tokens, I call these <strong>prekeys</strong>. For example, we know that the prekey <code>from</code> usually appears in front of any source host or IP address, and the prekey <code>to</code> usually appears in front of any destination host or IP address. Below are some examples of these prekeys.</p>

<pre><code>from 		= [ &quot;%srchost%&quot;, &quot;%srcipv4%&quot; ]
port 		= [ &quot;%srcport%&quot;, &quot;%dstport%&quot; ]
proto		= [ &quot;%protocol%&quot; ]
sport		= [ &quot;%srcport%&quot; ]
src 		= [ &quot;%srchost%&quot;, &quot;%srcipv4%&quot; ]
to 			= [ &quot;%dsthost%&quot;, &quot;%dstipv4%&quot;, &quot;%dstuser%&quot; ]
</code></pre>

<p>To help identify these prekeys, I wrote a quick program that goes through many of the logs I have to help identify what keywords appears before IP address, mac addresses, and other non-literal tokens. The result is put into the <a href="https://github.com/strace/sequence/blob/master/keymaps.go">keymaps.go</a> file. It&rsquo;s not comprehensive, but it&rsquo;s also not meant to be. We just need enough hints to help with labeling.</p>

<p><strong>3. Labeling &ldquo;Values&rdquo; by Their Keys</strong></p>

<p>Once the keys are labeled, we can label the values based on the mapping described above. For key/value pairs, we try to recognize both <code>key=value</code> or <code>key=&quot;value&quot;</code> formats (or other quote characters like &lsquo; or &lt;).</p>

<p>For the prekeys, we try to find the value token within 2 tokens of the key token. That means sequences such as <code>from 192.168.1.1</code> and <code>from ip 192.168.1.1</code> will identify <code>192.168.1.1</code> as the <code>%srcipv4%</code> based on the above mapping, but we will miss <code>from ip address 192.168.1.1</code>.</p>

<p><strong>4. Identifying Known Keywords</strong></p>

<p>Within most log messages, there are certain keywords that would indicate what actions were performed, what the state/status of the action was, and what objects the actions were performed on. CEE had a list that it identified, so I copied the list and added some of my own.</p>

<pre><code>action = [
	&quot;access&quot;,
	&quot;alert&quot;,
	&quot;allocate&quot;,
	&quot;allow&quot;,
	.
	.
	.
]

status = [
	&quot;accept&quot;,
	&quot;error&quot;,
	&quot;fail&quot;,
	&quot;failure&quot;,
	&quot;success&quot;
]

object = [
	&quot;account&quot;,
	&quot;app&quot;,
	&quot;bios&quot;,
	&quot;driver&quot;,
	.
	.
	.
]
</code></pre>

<p>In our labeling process, we basically goes through and identify all the string literals that are NOT marked as keys, and perform a <a href="https://github.com/surge/porter2">porter2 stemming operation</a> on the literal, then compare to the above list (which is also porter2 stemmed).</p>

<p>If a literal matches one of the above lists, then the corresponding label (<code>action</code>, <code>status</code>, <code>object</code>, <code>srcuser</code>, <code>method</code>, or <code>protocol</code>) is applied.</p>

<p><strong>5. Determining Positions of Specific Types</strong></p>

<p>In this next step, we are basically looking at the position of where some of the token types appear. Specifically, we are looking for <code>%time%</code>, <code>%url%</code>, <code>%mac%</code>, <code>%ipv4%</code>, <code>%host%</code>, and <code>%email%</code> tokens. Assuming the labels have not already been taken with the previous rules, the rules are as follows:</p>

<ul>
<li>The first %time% token is labeled as %msgtime%</li>
<li>The first %url% token is labeled as %object%</li>
<li>The first %mac% token is labeled as %srcmac% and the second is labeld as %dstmac%</li>
<li>The first %ipv4% token is labeled as %srcipv4% and the second is labeld as %dstipv4%</li>
<li>The first %host% token is labeled as %srchost% and the second is labeld as %dsthost%</li>
<li>The first %email% token is labeled as %srcemail% and the second is labeld as %dstemail%</li>
</ul>

<p><strong>6. Scanning for ip/port or ip:port Pairs</strong></p>

<p>Finally, after all that, we scan through the sequence again, and identify any numbers that follow an IP address, but separated by either a &ldquo;/&rdquo; or &ldquo;:&ldquo;. Then we label these numbers as either <code>%srcport%</code> or <code>%dstport%</code> based on how the previous IP address is labeled.</p>

<h3 id="summary:61b1adc09fb9bc31a28d4faabfef3631">Summary</h3>

<p>There are some limitations to the <code>sequence</code> parser and analyzer. For example, currently <code>sequence</code> does not handle multi-line logs. Each log message must appear as a single line. So if there&rsquo;s multi-line logs, they must be first be converted into a single line. Also, <code>sequence</code> has been only tested with a limited set of system (Linux, AIX, sudo, ssh, su, dhcp, etc etc), network (ASA, PIX, Neoteris, CheckPoint, Juniper Firewall) and infrastructure application (apache, bluecoat, etc) logs.</p>

<p>Documentation is available at godoc: <a href="http://godoc.org/github.com/strace/sequence">package</a>, <a href="http://godoc.org/github.com/strace/sequence/cmd/sequence">command</a>.</p>

<p>There are some pattern files developed for ASA, Sudo and SSH in the <code>patterns</code> directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages.</p>

<p>If you have a set of logs you would like me to test out, please feel free to <a href="https://github.com/strace/sequence/issues">open an issue</a> and we can arrange a way for me to download and test your logs.</p>

<p>Stay tuned for more log patterns&hellip;</p>

  </div>
  
</div>

<div class="posts">
  <h1 class="post-title">Archive</h1>
  <ul class="posts">
      
      <li><span><a href="http://zhen.org/blog/papers-i-read-2015-week-7/">Papers I Read: 2015 Week 7</a> - <time class="pull-right post-list">Sun, Feb 15, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">Sequence: Optimizing Go For the High Performance Log Scanner</a> - <time class="pull-right post-list">Fri, Feb 13, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns</a> - <time class="pull-right post-list">Tue, Feb 10, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/papers-i-read-2015-week-6/">Papers I Read: 2015 Week 6</a> - <time class="pull-right post-list">Sun, Feb 8, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS</a> - <time class="pull-right post-list">Sun, Feb 1, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">Generating Porter2 FSM For Fun and Performance in Go</a> - <time class="pull-right post-list">Wed, Jan 21, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/golang-from-a-non-programmers-perspective/">Go: From a Non-Programmer&#39;s Perspective</a> - <time class="pull-right post-list">Tue, Jan 13, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">PingMQ: A SurgeMQ-based ICMP Monitoring Tool</a> - <time class="pull-right post-list">Thu, Dec 25, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">SurgeMQ: High Performance MQTT Server and Client Libraries in Go</a> - <time class="pull-right post-list">Wed, Dec 24, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/">SurgeMQ: MQTT Message Queue @ 750,000 MPS</a> - <time class="pull-right post-list">Thu, Dec 4, 2014</h4></time></span></li>
      
  </ul>
</div>

</div>

  </body>

</html>
