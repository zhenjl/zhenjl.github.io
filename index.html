<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Zen 3.1 &middot; Zen 3.1 </title>

  
  <link rel="stylesheet" href="http://zhen.org//css/poole.css">
  <link rel="stylesheet" href="http://zhen.org//css/syntax.css">
  <link rel="stylesheet" href="http://zhen.org//css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://zhen.org/index.xml" rel="alternate" type="application/rss+xml" title="Zen 3.1" />

  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-681691-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a href="http://zhen.org/">Zen 3.1</a></h1>
      <p class="lead">
       Product. Data. Code 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="http://zhen.org//blog">Archive</a></li>
      <br/>
      <li>Projects</li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="http://sequence.trustpath.com">sequence</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surgemq/surgemq">surgemq</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surge">others</a></li>
      
    </ul>

    <a href="https://twitter.com/zhenjl"><i class="fa fa-twitter-square"></i></a>&nbsp;&nbsp;
    <a href="http://linkedin.com/in/zhenjl"><i class="fa fa-linkedin-square"></i></a>&nbsp;&nbsp;
    
    
    

    <p class="footnote">Powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    Theme originally made by <a href="http://twitter.com/mdo">@mdo</a> <br/>
    Theme modified by <a href="http://npf.io">Nate Finch</a> <br/>
    &copy; 2015 Jian Zhen. All Rights Reserved.</p>
    
  </div>
</div>

    <div class="content container">
<div class="posts">

  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/papers-i-read-2015-week-8/">
        Papers I Read: 2015 Week 8
      </a>
    </h1>

    <span class="post-date">Sun, Feb 22, 2015</span>

    

<h3 id="random-ramblings:29724b4ec2d333ad12e89fd98257dccd">Random Ramblings</h3>

<p>Another week, another report of hacks. This time, <a href="http://securelist.com/blog/research/68732/the-great-bank-robbery-the-carbanak-apt/">The Great Bank Robbery</a>, where up to 100 financial institutions have been hit.Total financial losses could be as a high as $1bn. You can download the <a href="http://25zbkz3k00wn2tp5092n6di7b5k.wpengine.netdna-cdn.com/files/2015/02/Carbanak_APT_eng.pdf">full report</a> and learn all about it.</p>

<p>Sony spent $15M to clean up and remediate their hack. I wonder how much these banks are going to spend on tracing the footsteps of their intruders and trying to figure out exactly where they have gone, what they have done and what they have taken.</p>

<p>I didn&rsquo;t make much progress this week on either <a href="https://github.com/strace/sequence">sequence</a> or <a href="https://github.com/surgemq/surgemq">surgemq</a> because of busy work schedule and my son getting sick AGAIN!! But I did merge the few surgemq <a href="https://github.com/surgemq/surgemq/pulls?q=is%3Apr+is%3Aclosed">pull requests</a> that the community has graciously contributed. One of them actually got it tested on Raspberry! That&rsquo;s pretty cool.</p>

<p>I also did manage to finish up the <a href="https://github.com/strace/sequence/commit/713979f70d6025308e434205973249aa3138e58e">experimental json scanner</a> that I&rsquo;ve been working on for the past couple of weeks. I will write more about it in the next <a href="http://strace.io/sequence">sequence</a> article.</p>

<p>Actually I am starting to feel a bit overwhelmed by having both projects. Both of them are very interesting and I can see both move forward in very positive ways. Lots of ideas in my head but not enough time to do them. Now that I am getting feature requests, issues and pull requests, I feel even worse because I haven&rsquo;t spent enough time on them. &lt;sigh&gt;</p>

<h3 id="papers-i-read:29724b4ec2d333ad12e89fd98257dccd">Papers I Read</h3>

<ul>
<li><a href="http://www.pdl.cmu.edu/PDL-FTP/HECStorage/git-cercs-12-08.pdf">Memory-Efficient GroupBy-Aggregate using Compressed Buffer Trees</a></li>
</ul>

<blockquote>
<p>Memory is rapidly becoming a precious resource in many data processing environments. This paper introduces
a new data structure called a Compressed Buffer Tree (CBT). Using a combination of buffering, compression,
and lazy aggregation, CBTs can improve the memoryefficiency of the GroupBy-Aggregate abstraction which
forms the basis of many data processing models like MapReduce and databases. We evaluate CBTs in the
context of MapReduce aggregation, and show that CBTs can provide significant advantages over existing hashbased
aggregation techniques: up to 2× less memory and 1.5× the throughput, at the cost of 2.5× CPU.</p>
</blockquote>

<ul>
<li><a href="https://www.usenix.org/system/files/conference/atc14/atc14-paper-hu.pdf">ELF: Efficient Lightweight Fast Stream Processing at Scale</a></li>
</ul>

<blockquote>
<p>Stream processing has become a key means for gaining rapid insights from webserver-captured data. Challenges
include how to scale to numerous, concurrently running streaming jobs, to coordinate across those jobs to share
insights, to make online changes to job functions to adapt to new requirements or data characteristics, and for each job, to efficiently operate over different time windows. The ELF stream processing system addresses these new challenges. Implemented over a set of agents enriching the web tier of datacenter systems, ELF obtains scalability by using a decentralized “many masters” architecture where for each job, live data is extracted directly from webservers, and placed into memory-efficient compressed buffer trees (CBTs) for local parsing and temporary storage, followed by subsequent aggregation using shared reducer trees (SRTs) mapped to sets of worker processes. Job masters at the roots of SRTs can dynamically customize worker actions, obtain aggregated results for end user delivery and/or coordinate with other jobs.</p>
</blockquote>

<ul>
<li><a href="https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html">Is Parallel Programming Hard, And, If So, What Can You Do About It?</a></li>
</ul>

<p>Not just a paper, it&rsquo;s a whole book w/ 800+ pages.</p>

<blockquote>
<p>The purpose of this book is to help you program shared-memory parallel machines without risking your sanity.1 We hope that this book’s design principles will help you avoid at least some parallel-programming pitfalls. That said, you should think of this book as a foundation on which to build, rather than as a completed cathedral. Your mission, if you choose to accept, is to help make further progress in the exciting field of parallel programming—progress that will in time render this book obsolete. Parallel programming is not as hard as some say, and we hope that this book makes your parallel-programming projects easier and more fun.</p>
</blockquote>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/papers-i-read-2015-week-7/">
        Papers I Read: 2015 Week 7
      </a>
    </h1>

    <span class="post-date">Sun, Feb 15, 2015</span>

    

<h2 id="random-ramblings:10248ddcf524aa32198bc5cded1aa098">Random Ramblings</h2>

<p>Well, another week, <a href="http://www.nytimes.com/2015/02/05/business/hackers-breached-data-of-millions-insurer-says.html">another big data breach</a>. This time is Anthem, one of the nation’s largest health insurers. Ok, maybe it was last week that it happend. But this week they revealed that <a href="http://consumerist.com/2015/02/13/anthem-says-data-from-as-far-back-as-2004-exposed-during-hack-offering-free-identity-theft-protection/">hackers had access &hellip; going back as far as 2004</a>. WSJ blamed Anthem for <a href="http://www.wsj.com/articles/investigators-eye-china-in-anthem-hack-1423167560">not encrypting the data</a>. Though I have to agree with Rich Mogull over at Securosis that &ldquo;<a href="https://securosis.com/blog/even-if-anthem-encrypted-it-probably-wouldnt-have-mattered">even if Anthem had encrypted, it probably wouldn’t have helped</a>&rdquo;.</p>

<p>I feel bad for saying this but there&rsquo;s one positive side effect from all these data breaches. Security is now officially a boardroom topic. Anthem&rsquo;s CEO, Joseph Swedish, is now <a href="http://www.latimes.com/business/la-fi-anthem-hack-ceo-20150213-story.html#page=1">under the gun</a> because top level executives are no longer immune to major security breaches that affect the company&rsquo;s top line. Just ask <a href="http://www.forbes.com/sites/ericbasu/2014/06/15/target-ceo-fired-can-you-be-fired-if-your-company-is-hacked/">Target’s CEO Gregg Steinhafel</a>, or <a href="http://abcnews.go.com/Entertainment/wireStory/sony-chief-amy-pascal-acknowledges-fired-28918607">Sony&rsquo;s Co-Chairwoman Amy Pascal</a>.</p>

<p>Brian Krebs wrote a <a href="http://krebsonsecurity.com/2015/02/anthem-breach-may-have-started-in-april-2014/">detailed piece</a> analyzing the various pieces of information available relating to the Anthem hack. Quite an interesting read.</p>

<p>One chart in the artile that Brian referred to is the time difference between the “time to compromise” and the “time to discovery&rdquo;, taken from <a href="http://www.verizonenterprise.com/DBIR/2014/">Verizon’s 2014 Data Breach Investigations Report</a>. As Brian summaries, &ldquo;TL;DR: That gap is not improving, but instead is widening.&rdquo;</p>

<p>What this really says is that, <strong>you will get hacked</strong>. So how do you shorten the time between getting hacked, and finding out that you are hacked so you can quickly remediate the problem before worse things happen?</p>

<p><img src="http://krebsonsecurity.com/wp-content/uploads/2015/02/timetocompromise.png" alt="The time difference between the “time to compromise” and the “time to discovery.”" />
</p>

<p>With all these data breaches as backdrop, this week we also saw &ldquo;President Barack Obama signed an executive order on Friday designed to spur businesses and the Federal Government to share with each other information related to cybersecurity, hacking and data breaches for the purpose of safeguarding U.S. infrastructure, economics and citizens from cyber attacks.&rdquo; (<a href="https://gigaom.com/2015/02/13/obamas-executive-order-calls-for-sharing-of-security-data/">Gigaom</a>)</p>

<p>In general I don&rsquo;t really think government mandates like this will work. The industry has to feel the pain enough that they are willing to participate, otherwise it&rsquo;s just a waste of paper and ink. Facebook seems to be taking a lead in security information sharing and <a href="https://www.facebook.com/notes/protect-the-graph/threatexchange-sharing-for-a-safer-internet/1566584370248375">launched their ThreatExchange security framework</a> this week. along with Pinterest, Tumblr, Twitter, and Yahoo. Good for them! I hope this is not a temporary PR thing, and that they keep funding and supporting the framework.</p>

<h2 id="papers-i-read:10248ddcf524aa32198bc5cded1aa098">Papers I Read</h2>

<p>Another great resource of computer science papers is Adrian Coyler&rsquo;s <a href="http://blog.acolyer.org/">the morning paper</a>. He selects and summarizes &ldquo;an interesting/influential/important paper from the world of CS every weekday morning&rdquo;.</p>

<ul>
<li><a href="http://www.cs.put.poznan.pl/dweiss/site/publications/download/fsacomp.pdf">Smaller Representation of Finite State Automata</a></li>
</ul>

<p>I read this paper when I was trying to figure out how to make the FSAs smaller for the <a href="https://github.com/surge/xparse/tree/master/etld">Effective TLD matcher</a> I created. The FSM I generated is 212,294 lines long. That&rsquo;s just absolutely crazy. This paper seems to present an interesting way of compressing them.</p>

<p>I am not exactly sure if <a href="https://godoc.org/golang.org/x/net/publicsuffix">PublicSuffix</a> uses a similar representation but it basically represents a FSA as an array of bytes, and then walk the bytes like a binary search tree. It&rsquo;s interesting for sure.</p>

<blockquote>
<p>This paper is a follow-up to Jan Daciuk’s experiments on space-efficient finite state automata representation that can be used directly for traversals in main memory [4]. We investigate several techniques of reducing the memory footprint of minimal automata, mainly exploiting the fact that transition labels and transition pointer offset values are not evenly distributed and so are suitable for compression. We achieve a size gain of around 20–30% compared to the original representation given in [4]. This result is comparable to the state-of-the-art dictionary compression techniques like the LZ-trie [12] method, but remains
memory and CPU efficient during construction.</p>
</blockquote>

<ul>
<li><a href="http://arxiv.org/pdf/1409.5942v1.pdf">IP Tracing and Active Network Response</a></li>
</ul>

<blockquote>
<p>This work presents integrated model for active security response model. The proposed model introduces Active Response Mechanism (ARM) for tracing anonymous attacks in the network back to their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or “spoofed”, source addresses. This paper presents within the proposed model two tracing approaches based on:
• Sleepy Watermark Tracing (SWT) for unauthorized access attacks.
• Probabilistic Packet Marking (PPM) in the network for Denial of Service
(DoS) and Distributed Denial of Service (DDoS) attacks.</p>
</blockquote>

<ul>
<li><a href="http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36356.pdf">Dapper, a Large-Scale Distributed Systems Tracing Infrastructure</a></li>
</ul>

<blockquote>
<p>Here we introduce the design of Dapper, Google’s production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design
choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries.</p>
</blockquote>

<ul>
<li><a href="http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/">STREAM PROCESSING, EVENT SOURCING, REACTIVE, CEP… AND MAKING SENSE OF IT ALL</a></li>
</ul>

<p>Not a paper, but a good write up nonetheless.</p>

<blockquote>
<p>Some people call it stream processing. Others call it Event Sourcing or CQRS. Some even call it Complex Event Processing. Sometimes, such self-important buzzwords are just smoke and mirrors, invented by companies who want to sell you stuff. But sometimes, they contain a kernel of wisdom which can really help us design better systems. In this talk, we will go in search of the wisdom behind the buzzwords. We will discuss how event streams can help make your application more scalable, more reliable and more maintainable.</p>
</blockquote>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">
        Sequence: Optimizing Go For the High Performance Log Scanner
      </a>
    </h1>

    <span class="post-date">Fri, Feb 13, 2015</span>

    

<p>Information here maybe outdated. Please visit <a href="http://sequence.trustpath.com">http://sequence.trustpath.com</a> for latest.</p>

<hr />

<p>This is part 3 of the <a href="http://sequence.trustpath.com">sequence</a> series.</p>

<ul>
<li><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Part 1</a> is about the high performance parser that can parse 100,000-200,000 MPs.</li>
<li><a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Part 2</a> is about automating the process of reducing 100 of 1000&rsquo;s of log messages down to dozens of unique patterns.</li>
<li><a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">Part 3</a> is about optimizing Go to achieve very high performance (200,000 - 500,000 MPS depending on message size and core count) for scanning and tokenizing log messages</li>
</ul>

<p>I would love to learn more about the state-of-the-art approaches that log vendors are using. These attempts are about scratching my own itch and trying to realize ideas I&rsquo;ve had in my mind. Given some of these ideas are 5 to 10 years old, they may already be outdated. Personally I just haven&rsquo;t heard of any groundbreaking approaches.</p>

<p>In any case, if you know of some of the more innovative ways people are approaching these problems, please please please comment below as I would love to hear from you.</p>

<h3 id="tl-dr:cb54f18a9944e1962d3fe8e3f09ea809">tl;dr</h3>

<ul>
<li>The <code>sequence</code> scanner is designed to tokenize free-form log messages.

<ul>
<li>It can scan between 200K to 500K log messages per second depending on message size and core count.</li>
<li>It recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.</li>
<li>The design is based mostly on finite-state machines.</li>
</ul></li>
<li>The performance was achieved by the following techniques:

<ol>
<li>Go Through the String Once and Only Once</li>
<li>Avoid Indexing into the String</li>
<li>Reduce Heap Allocation</li>
<li>Reduce Data Copying</li>
<li>Mind the Data Struture</li>
<li>Avoid Interfaces If Possible</li>
<li>Find Ways to Short Circuit Checks</li>
</ol></li>
</ul>

<h2 id="background:cb54f18a9944e1962d3fe8e3f09ea809">Background</h2>

<blockquote>
<p>In computer science, lexical analysis is the process of converting a sequence of characters into a sequence of tokens, i.e. meaningful character strings. A program or function that performs lexical analysis is called a lexical analyzer, lexer, tokenizer, or scanner. - <a href="http://en.wikipedia.org/wiki/Lexical_analysis">Wikipedia</a></p>
</blockquote>

<p>One of the most critical functions in the <code>sequence</code> parser is the message tokenization. At a very high level, message tokenization means taking a single log message and breaking it into a list of tokens.</p>

<h3 id="functional-requirements:cb54f18a9944e1962d3fe8e3f09ea809">Functional Requirements</h3>

<p>The challenge is knowing where the token break points are. Most log messages are free-form text, which means there&rsquo;s no common structure to them.</p>

<p>As an example, the following log message can be tokenized into the sequence of tokens below. As you can see, one cannot depend on white spaces to tokenize, as the timestamp would be broken into 3 parts; nor can one use punctuations like &ldquo;;&rdquo; or &ldquo;:&ldquo;, as they would break the log mesage into useless parts.</p>

<pre><code>jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream

  #   0: { Field=&quot;%funknown%&quot;, Type=&quot;%ts%&quot;, Value=&quot;jan 14 10:15:56&quot; }
  #   1: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;testserver&quot; }
  #   2: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;sudo&quot; }
  #   3: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   4: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;gonner&quot; }
  #   5: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   6: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;tty&quot; }
  #   7: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #   8: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;pts/3&quot; }
  #   9: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  10: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;pwd&quot; }
  #  11: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  12: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/home/gonner&quot; }
  #  13: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  14: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;user&quot; }
  #  15: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  16: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;root&quot; }
  #  17: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  18: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;command&quot; }
  #  19: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  20: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/bin/su&quot; }
  #  21: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;-&quot; }
  #  22: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;ustream&quot; }
</code></pre>

<p>So a log message <em>scanner</em> or <em>tokenizer</em> (we will use these terms interchangeably) must understand common components such as timestamp, URL, hex strings, IP addresses (v4 or v6), and mac addresses, so it can break the messages into <em>meaningful components</em>.</p>

<h3 id="performance-requirements:cb54f18a9944e1962d3fe8e3f09ea809">Performance Requirements</h3>

<p>From a performance requirements perspective, I really didn&rsquo;t start out with any expectations. However, after achieving 100-200K MPS for parsing (not just tokenizing), I have a strong desire to keep the performance at that level. So the more I can optimize the scanner to tokenize faster, the more head room I have for parsing.</p>

<p>One may ask, who can POSSIBLY use such performance? Many organizations that I know are generating between 50-100M messages per second (MPS), that&rsquo;s only 1,200 MPS. Some larger organizations I know are generating 60GB of Bluecoat logs per day, <strong>8 years ago</strong>!! That&rsquo;s a good 3,000 MPS assuming an average of 250 bytes per message. Even if log rate grows at 15%, that&rsquo;s still only 10K MPS today.</p>

<p>To run through an example, <a href="http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf">at EMC, 1.4 billion log messages are generated daily on average, at a rate of one terabyte a day</a>. That&rsquo;s 16,200 messages per second, and about 714 bytes per message. (Btw, what system can possibly generate messages that are 714 bytes long? That&rsquo;s crazy and completely inefficient!) These EMC numbers are from 2013, so they have likely increased by now.</p>

<p>The <code>sequence</code> parser, with a single CPU core, can process about 270,000 MPS for messages averaging 98 bytes. Assuming the performance is linear compare to the message size (which is pretty close to the truth), we can process 37,000 MPS for messages averaging 714 bytes. That&rsquo;s just enough to parse the 16,2000 MPS, with a little head room to do other types of analysis or future growth.</p>

<p>Obviously one can throw more hardware at solving the scale problem, but then again, why do that if you don&rsquo;t need to. Just because you have the hardware doesn&rsquo;t mean you should waste the money! Besides, there are much more interesting analytics problems your hardware can be used for than just tokenizing a message.</p>

<p>In any case, I want to squeeze every oz of performance out of the scanner so I can have more time in the back to parse and analyze. So let&rsquo;s set a goal of keeping at least 200,000 MPS for 100 bytes per message (BPM).</p>

<p>Yes, go ahead and tell me I shouldn&rsquo;t worry about micro-optimization, because this post is all about that. :)</p>

<h2 id="sequence-scanner:cb54f18a9944e1962d3fe8e3f09ea809">Sequence Scanner</h2>

<p>In the <code>sequence</code> package, we implemented a general log message scanner, called <a href="https://github.com/strace/sequence/blob/master/scanner.go">GeneralScanner</a>. GeneralScanner is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, hex strings, IP (v4, v6) addresses, URLs, MAC addresses, integers and floating point numbers.</p>

<p>This implementation was able to achieve both the functional and performance requirements. The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro, although the tests were only using 1 or 2 cores. The first file is a bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message. Last is a mix of ASA, sshd and sudo logs, averaging 136 bytes per message.</p>

<pre><code>  $ ./sequence bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272869.35 msgs/sec

  $ ./sequence bench scan -i ../../data/allasa.log
  Scanned 234815 messages in 1.43 secs, ~ 163827.61 msgs/sec

  $ ./sequence bench scan -i ../../data/allasassh.log
  Scanned 447745 messages in 2.27 secs, ~ 197258.42 msgs/sec
</code></pre>

<p>Performance can be improved by adding more cores:</p>

<pre><code>  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/sshd.all -w 2
  Scanned 212897 messages in 0.43 secs, ~ 496961.52 msgs/sec

  $ GOMAXPROCS=2 ./sequenceo bench scan -i ../../data/allasa.log -w 2
  Scanned 234815 messages in 0.80 secs, ~ 292015.98 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench scan -i ../../data/allasassh.log -w 2
  Scanned 447745 messages in 1.20 secs, ~ 373170.45 msgs/sec
</code></pre>

<h3 id="concepts:cb54f18a9944e1962d3fe8e3f09ea809">Concepts</h3>

<p>To understand the scanner, you have to understand the following concepts that are part of the package.</p>

<ul>
<li><p>A <em>Token</em> is a piece of information extracted from the original log message. It is a struct that contains fields for <em>TokenType</em>, <em>FieldType</em>, and <em>Value</em>.</p></li>

<li><p>A <em>TokenType</em> indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.</p></li>

<li><p>A <em>FieldType</em> indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).</p></li>

<li><p>A <em>Sequence</em> is a list of Tokens. It is the key data structure consumed and returned by the <em>Scanner</em>, <em>Analyzer</em>, and the <em>Parser</em>.</p></li>
</ul>

<p>Basically, the scanner takes a log message string, tokenizes it and returns a <em>Sequence</em> with the recognized <em>TokenType</em> marked. This <em>Sequence</em> is then fed into the analyzer or parser, and the analyzer or parser in turn returns another <em>Sequence</em> that has the recognized <em>FieldType</em> marked.</p>

<h3 id="design:cb54f18a9944e1962d3fe8e3f09ea809">Design</h3>

<p>Tokenizers or scanners are usually implemented using finite-state machines. Each FSM (or FSA, finite state automata) understands a specific sequences of characters that make up a type of token.</p>

<p>In the <code>sequence</code> scanner, there are three FSMs: Time, HexString and General.</p>

<ul>
<li>The Time FSM understands a list of <a href="https://github.com/strace/sequence/blob/master/time.go">time formats</a>. This list of time formats are commonly seen in log messages. It is also fairly easy to add to this list if needed.</li>
<li>The HexString FSM is designed to understand IPv6 addresses (dead:beef:1234:5678:223:32ff:feb1:2e50 or f0f0:f::1), MAC addresses (00:04:c1:8b:d8:82), fingerprints or signatures (de:ad:be:ef:74:a6:bb:45:45:52:71:de:b2:12:34:56).</li>
<li>The General FSM that recognizes URLs, IPv4 addresses, and any literal or strings.</li>
</ul>

<p>Each character in the log string are run through all three FSMs.</p>

<ol>
<li>If a time format is matched, that&rsquo;s what it will be returned.</li>
<li>Next if a hex string is matched, it is also returned.

<ul>
<li>We mark anything with 5 colon characters and no successive colons like &ldquo;::&rdquo; to be a MAC address.</li>
<li>Anything that has 7 colons and no successive colons are marked as IPv6 address.</li>
<li>Anything that has less than 7 colons but has only 1 set of successive colons like &ldquo;::&rdquo; are marked as IPv6 address.</li>
<li>Everything else is just a literal.</li>
</ul></li>
<li>Finally if neither of the above matched, we return what the general FSM has matched.

<ul>
<li>The general FSM recognizes these quote characters: &ldquo;, &lsquo; and &lt;. If these characters are encountered, then it will consider anything between the quotes to be a single token.</li>
<li>Anything that starts with http:// or https:// are considered URLs.</li>
<li>Anything that matches 4 integer octets are considered IP addresses.</li>
<li>Anything that matches two integers with a dot in between are considered floats.</li>
<li>Anything that matches just numbers are considered integers.</li>
<li>Everything else are literals.</li>
</ul></li>
</ol>

<h3 id="performance:cb54f18a9944e1962d3fe8e3f09ea809">Performance</h3>

<p>To achieve the performance requirements, the following rules and optimizations are followed. Some of these are Go specific, and some are general recommendations.</p>

<p><strong>1. Go Through the String Once and Only Once</strong></p>

<p>This is a hard requirement, otherwise we can&rsquo;t call this project a <em>sequential</em> parser. :)</p>

<p>This is probably a pretty obvious technique. The more times you loop through loop through a string, the lower the performance. If you used regular expressions to parse logs, you will likely go through parts of the log message multiple times due to back tracking or look forward, etc.</p>

<p>I took great pain to ensure that I don&rsquo;t need to look forward or look backward in the log string to determine the current token type, and I think the effort paid off.</p>

<p>In reality though, while I am only looping through the log string once, and only once, I do run each character through three different FSMs. However, it is still much less expensive than looping through three times, each time checking a single FSM. However, the more FSMs I run the characters through, the slower it gets.</p>

<p>This was apparently when I <a href="https://github.com/strace/sequence/commit/a5447814f43b4b9b7e804b14dde38e88fd53e6d0">updated the scanner to support IPv6 and hex strings</a>. I tried a couple of different approaches. First, I added an IPv6 specific FSM. So in addition to the original time, mac and general FSMs, there are now 4. That dropped performance by like 15%!!! That&rsquo;s just unacceptable.</p>

<p>The second approach, which is the one I checked in, combines the MAC, IPv6 and general hex strings into a single FSM. That helped somewhat. I was able to regain about 5% of the performance hit. However, because I can no longer short circuit the MAC address check (by string length and colon positions), I was still experiencing a 8-10% hit.</p>

<p>What this means is that for most tokens, instead of checking just 2 FSMs because I can short circuit the MAC check pretty early, I have to now check all 3 FSMs.</p>

<p>So the more FSMs, the more comlicated the FSMs, the more performance hits there will be.</p>

<p><strong>2. Avoid Indexing into the String</strong></p>

<p>This is really a Go-specific recommentation. Each time you index into a slice or string, Go will perform bounds checking for you, which means there&rsquo;s extra operations it&rsquo;s doing, and also means lower performance. As an example, here are results from two benchmark runs. The first is with bounds checking enabled, which is default Go behavior. The second disables bounds checking.</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.79 secs, ~ 268673.91 msgs/sec

  $ go run -gcflags=-B ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 277479.58 msgs/sec
</code></pre>

<p>The performance difference is approximately 3.5%! However, while it&rsquo;s fun to see the difference, I would never recommend disable bounds checking in production. So the next best thing is to remove as many operations that index into a string or slice as possible. Specifically:</p>

<ol>
<li>Use &ldquo;range&rdquo; in the loops, e.g. <code>for i, r := range str</code> instead of <code>for i := 0; i &lt; len(str); i++ { if str[i] == ... }</code></li>
<li>If you are checking a specific character in the string/slice multiple times, assign it to a variable and use the variable instead. This will avoid indexing into the slice/string multiple times.</li>
<li>If there are multiple conditions in an <code>if</code> statement, try to move (or add) the non-indexing checks to the front of the statement. This sometimes will help short circuit the checks and avoid the slice-indexing checks.</li>
</ol>

<p>One might question if this is worth optimizing, but like I said, I am trying to squeeze every oz of performance so 3.5% is still good for me. Unfornately I do know I won&rsquo;t get 3.5% since I can&rsquo;t remove every operation that index into slice/string.</p>

<p><strong>3. Reduce Heap Allocation</strong></p>

<p>This is true for all languages (where you can have some control of stack vs heap allocation), and it&rsquo;s even more true in Go. Mainly in Go, if you allocate a new slice, Go will &ldquo;zero&rdquo; out the allocated memory. This is great from a safety perspective, but it does add to the overhead.</p>

<p>As an example, in the scanner I originally allocated a new <em>Sequence</em> (slice of <em>Token</em>) for every new message. However, when i changed it to re-use the existing slice, the performance increased by over 10%!</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.87 secs, ~ 246027.12 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.77 secs, ~ 275038.83 msgs/sec
</code></pre>

<p>The best thing to do is to run Go&rsquo;s builtin CPU profiler, and look at the numbers for Go internal functions such as <code>runtime.makeslice</code>, <code>runtime.markscan</code>, and <code>runtime.memclr</code>. Large percentages and numbers for these internal functions are dead giveaway that you are probably allocating too much stuff on the heap.</p>

<p>I religiously go through the SVGs generated from the Go profiler to help me identify hot spots where I can optimize.</p>

<p>Here&rsquo;s also a couple of tips I picked up from the <a href="https://groups.google.com/forum/#!topic/golang-nuts/baU4PZFyBQQ">go-nuts mailing list</a>:</p>

<ul>
<li>Maps are bad&ndash;even if they&rsquo;re storing integers or other non-pointer structs. The implementation appears to have lots of pointers inside which must be evaluated and followed during mark/sweep GC.  Using structures with pointers magnifies the expense.</li>
<li>Slices are surprisingly bad (including strings and substrings of existing strings). A slice is a pointer to the backing array with a length and capacity. It would appear that the internal pointer that causes the trouble because GC wants to inspect it.</li>
</ul>

<p><strong>4. Reduce Data Copying</strong></p>

<p>Data copying is expensive. It means the run time has to allocate new space and copy the data over. It&rsquo;s even more expensive when you can&rsquo;t have do <code>memcpy</code> of a slice in Go like you can in C. Again, direct memory copying is not Go&rsquo;s design goal. It is also much safer if you can prevent users from playing with memory directly too much. However, it is still a good idea to avoid any copying of data, whether it&rsquo;s string or slice.</p>

<p>As much as I can, I try to do in place processing of the data. Every <em>Sequence</em> is worked on locally and I try not to copy <em>Sequence</em> or string unless I absolutely have to.</p>

<p>Unfortunately I don&rsquo;t have any comparison numbers for this one, because I learned from <a href="http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/">previous projects</a> that I should avoid copying as much as possible.</p>

<p><strong>5. Mind the Data Struture</strong></p>

<p>If there&rsquo;s one thing I learned over the past year, is to use the right data structure for the right job. I&rsquo;ve written about other data structures such as <a href="http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/">ring buffer</a>, <a href="http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/">bloom filters</a>, and <a href="http://zhen.org/blog/go-skiplist/">skiplist</a> before.</p>

<p>However, <a href="http://en.wikipedia.org/wiki/Finite-state_machine">finite-state automata or machine</a> is my latest love and I&rsquo;ve been using it at various projects such as my <a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">porter2</a> and <a href="https://github.com/surge/xparse/tree/master/etld">effective TLD</a>. Ok, technical FSM itself is not a data structure and can be implemented in different ways. In the <code>sequence</code> project, I used both a tree representation as well as a bunch of switch-case statements. For the <a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">porter2</a> FSMs, I used switch-case to implement them.</p>

<p>Interestingly, swtich-case doesn&rsquo;t always win. I tested the time FSM using both tree and switch-case implementations, and the tree actually won out. (Below, 1 is tree, 2 is switch-case.) So guess which one is checked in?</p>

<pre><code>BenchmarkTimeStep1   2000000         696 ns/op
BenchmarkTimeStep2   2000000         772 ns/op
</code></pre>

<p>Writing this actually reminds me that in the parser, I am currently using a tree to parse the sequences. While parsing, there could be multiple paths that the sequence will match. Currently I walk all the matched paths fully, before choosing one that has the highest score. What I should do is to do a weighted walk, and always walk the highest score nodes first. If at the end I get a perfect score, I can just return that path and not have to walk the other paths. (Note to self, more parser optimization to do).</p>

<p><strong>6. Avoid Interfaces If Possible</strong></p>

<p>This is probably not a great advice to give to Go developers. Interface is probably one of the best Go features and everyone should learn to use it. However, if you want high performane, avoid interfaces as it provides additional layers of indirection. I don&rsquo;t have performance numbers for the <code>sequence</code> project since I tried to avoid interfaces in high performance areas from the start. However, previous in the <a href="http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/">ring buffer</a> project, the version that uses interface is 140% slower than the version that didn&rsquo;t.</p>

<p>I don&rsquo;t have the direct link but someone on the go-nuts mailing list also said:</p>

<blockquote>
<p>If you really want high performance, I would suggest avoiding interfaces and, in general, function calls like the plague, since they are quite expensive in Go (compared to C). We have implemented basically the same for our internal web framework (to be released some day) and we&rsquo;re almost 4x faster than encoding/json without doing too much optimization. I&rsquo;m sure we could make this even faster.</p>
</blockquote>

<p><strong>7. Find Ways to Short Circuit Checks</strong></p>

<p>Find ways to quickly eliminate the need to run a section of the code has been tremendously helpful to improve performance. For example, here are a couple of place where I tried to do that.</p>

<p>In <a href="https://github.com/strace/sequence/blob/master/scanner.go#L223">this first example</a>, I simply added <code>l == 1</code> before the actual equality check of the string values. The first output is before the add, the second is after. The difference is about 2% performance increase.</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272303.79 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 278433.34 msgs/sec
</code></pre>

<p>In <a href="https://github.com/strace/sequence/blob/master/scanner.go#L282">the second example</a>, I added a quick check to make sure the remaining string is at least as long as the shortest time format. If there&rsquo;s not enough characters, then don&rsquo;t run the time FSM. The performance difference is about 2.5%.</p>

<pre><code>  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.78 secs, ~ 272059.04 msgs/sec

  $ go run ./sequence.go bench scan -i ../../data/sshd.all
  Scanned 212897 messages in 0.76 secs, ~ 279388.47 msgs/sec
</code></pre>

<p>So by simply adding a couple of checks, I&rsquo;ve increased perfromance by close to 5%.</p>

<h2 id="conclusion:cb54f18a9944e1962d3fe8e3f09ea809">Conclusion</h2>

<p>At this point I think I have squeezed every bit of performance out of the scanner, to the extend of my knowledge. It&rsquo;s performing relatively well and it&rsquo;s given the parser plenty of head room to do other things. I hope some of these lessons are helpful to whatever you are doing.</p>

<p>Feel free to take a look at the <a href="https://github.com/strace/sequence">sequence</a> project and try it out if you. If you have any issues/comments, please don&rsquo;t hestiate to <a href="https://github.com/strace/sequence/issues">open a github issue</a>.</p>

  </div>
  
</div>

<div class="posts">
  <h1 class="post-title">Archive</h1>
  <ul class="posts">
      
      <li><span><a href="http://zhen.org/blog/papers-i-read-2015-week-8/">Papers I Read: 2015 Week 8</a> - <time class="pull-right post-list">Sun, Feb 22, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/papers-i-read-2015-week-7/">Papers I Read: 2015 Week 7</a> - <time class="pull-right post-list">Sun, Feb 15, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-optimizing-go-for-high-performance-log-scanner/">Sequence: Optimizing Go For the High Performance Log Scanner</a> - <time class="pull-right post-list">Fri, Feb 13, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns</a> - <time class="pull-right post-list">Tue, Feb 10, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/papers-i-read-2015-week-6/">Papers I Read: 2015 Week 6</a> - <time class="pull-right post-list">Sun, Feb 8, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS</a> - <time class="pull-right post-list">Sun, Feb 1, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/generating-porter2-fsm-for-fun-and-performance/">Generating Porter2 FSM For Fun and Performance in Go</a> - <time class="pull-right post-list">Wed, Jan 21, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/golang-from-a-non-programmers-perspective/">Go: From a Non-Programmer&#39;s Perspective</a> - <time class="pull-right post-list">Tue, Jan 13, 2015</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">PingMQ: A SurgeMQ-based ICMP Monitoring Tool</a> - <time class="pull-right post-list">Thu, Dec 25, 2014</h4></time></span></li>
      
      <li><span><a href="http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">SurgeMQ: High Performance MQTT Server and Client Libraries in Go</a> - <time class="pull-right post-list">Wed, Dec 24, 2014</h4></time></span></li>
      
  </ul>
</div>

</div>

  </body>

</html>
