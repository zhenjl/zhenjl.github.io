<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Zen 3.1 &middot; Zen 3.1 </title>

  
  <link rel="stylesheet" href="http://localhost:1313/css/poole.css">
  <link rel="stylesheet" href="http://localhost:1313/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:1313/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://localhost:1313/index.xml" rel="alternate" type="application/rss+xml" title="Zen 3.1" />

  <link href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-681691-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a href="http://localhost:1313">Zen 3.1</a></h1>
      <p class="lead">
       Product. Data. Code 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="http://localhost:1313/blog">Archive</a></li>
      <br/>
      <li>Projects</li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/strace/sequence">sequence</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surgemq/surgemq">surgemq</a></li>
      
        <li><i class='fa fa-github-square'></i>&nbsp;&nbsp;<a href="https://github.com/surge">zhenjl/others</a></li>
      
    </ul>

    <a href="https://twitter.com/zhenjl"><i class="fa fa-twitter-square"></i></a>&nbsp;&nbsp;
    <a href="http://linkedin.com/in/zhenjl"><i class="fa fa-linkedin-square"></i></a>&nbsp;&nbsp;
    
    
    

    <p class="footnote">Powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    Theme originally made by <a href="http://twitter.com/mdo">@mdo</a> <br/>
    Theme modified by <a href="http://npf.io">Nate Finch</a> <br/>
    &copy; 2015 Jian Zhen. All Rights Reserved.</p>
    
  </div>
</div>

    <div class="content container">
<div class="posts">

  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">
        Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns
      </a>
    </h1>

    <span class="post-date">Tue, Feb 10, 2015</span>

    

<p><a href="http://godoc.org/github.com/strace/sequence"><img src="http://godoc.org/github.com/strace/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="http://godoc.org/github.com/strace/sequence/cmd/sequence"><img src="http://godoc.org/github.com/strace/sequence/cmd/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="https://github.com/strace/sequence">github repo</a></p>

<p>This is part 2 of the <code>sequence</code> series. <a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">Part 1</a> is about the high performance parser that can parse 100,000-200,000 MPs.</p>

<h2 id="background:61b1adc09fb9bc31a28d4faabfef3631">Background</h2>

<p>This post really takes me down the memory lane. Back in 2005, while I was at LogLogic, we envisioned an automated approach to tagging, or labeling, log messages. More specifically, we wanted to automatically tag specific components within the log messages with their semantic label, such as a source IP address, or a target user.</p>

<p>At the time, much like it is still today, the message parsing process is performed manually. This means someone has to manually look at the object and decided that the object should be labeled “user” or “targetUser.” An  analyst has to go through the log data, create a regular expression that extracts the useful strings out, and then finally assigning these to a specific label. This is extremely time consuming and error-prone.</p>

<p>At that time, the vision was to provide an automated approach to universally parse and analyze ANY log data. The key phrase being “automated approach.” This means the users should only need to provide minimum guidance to the system, if any, for the platforms to be able to analyze the log data. LogLogic never did much with this, unfortunately.</p>

<p>However, the tagging concept was later on adopted by (and I know how this got into CEE :) the <a href="http://cee.mitre.org/">Common Event Expression, or CEE</a> effort by Mitre. This idea of tags also inspired <a href="http://www.liblognorm.com/">liblognorm</a> to develop their <a href="http://www.libee.org/">libee</a> library and <a href="http://www.liblognorm.com/news/log-classification-with-liblognorm/">tagging system</a>. Rsyslog&rsquo;s <a href="http://www.rsyslog.com/doc/mmnormalize.html">mmnormalize</a> module is based on liblognorm.</p>

<p>And then there&rsquo;s Fedora&rsquo;s <a href="https://fedorahosted.org/lumberjack/">Project Lumberjack</a>, which &ldquo;is an open-source project to update and enhance the event log architecture&rdquo; and &ldquo;aims to improve the creation and standardize the content of event logs by implementing the concepts and specifications proposed by the ​Common Event Expression (CEE).&rdquo;</p>

<p>Then finally <a href="http://logstash.net/">logstash</a> has their <a href="http://logstash.net/docs/1.4.2/filters/grok">grok filter</a> that basically does similar extraction of unstructured data into a structured and queryable format. However, it seems like there might be some <a href="http://ghost.frodux.in/logstash-grok-speeds/">performance bottlenecks</a>.</p>

<p>However, none of these efforts attempted to solve the automated tagging/labeling problem. They mostly just try to provide a parser for log messages.</p>

<p>Also, it looks like many of these efforts have all been abandoned or put in hibernation, and haven&rsquo;t been updated since 2012 or 2013. liblognrom did put out <a href="http://www.liblognorm.com/news/">a couple of updates</a> in the past couple of years. Logstash&rsquo;s grok obviously is being maintained and developed with the <a href="http://www.elasticsearch.com/">Elasticsearch</a> backing.</p>

<p>It is understandable, unfortunately. Log parsing is <strong>BORING</strong>. I mean, who wants to sit there and stare at logs all day and try to come up with regular expressions or other types of parsing rules? LogLogic used to have a team of LogLabs analysts that did that, and I have to say I truly appreciated their effort and patience, because I cannot do that.</p>

<h3 id="the-end-result:61b1adc09fb9bc31a28d4faabfef3631">The End Result</h3>

<p>So instead of writing rules all day long, I decided to create an analyzer that can help us get at least 75% of the way there. The end result is the <code>Analyzer</code>, written in <a href="http://golang.org">Go</a>, in the <a href="https://github.com/strace/sequence">sequence</a> project I created. Here are some preliminary results. Below, we analyzed 2 files. The first is a file with over 200,000 sshd messages. The second is a file with a mixture of ASA, sshd, sudo and su log messages. It contains almost 450,000 messages.</p>

<p>By running the analyzer over these logs, the pure sshd log file returned 45 individual patterns, and the second returned 103 unique patterns.</p>

<pre><code>$ go run sequence.go analyze -i ../../data/sshd.all -o sshd.analyze
Analyzed 212897 messages, found 45 unique patterns, 45 are new.

$ go run sequence.go analyze -i ../../data/asasshsudo.log -o asasshsudo.analyze
Analyzed 447745 messages, found 103 unique patterns, 103 are new.
</code></pre>

<p>And the output file has entries such as:</p>

<pre><code>%msgtime% %apphost% %appname% [ %sessionid% ] : %status% %method% for %srcuser% from %srcipv4% port %srcport% ssh2
# Jan 15 19:39:26 irc sshd[7778]: Accepted password for jlz from 108.61.8.124 port 57630 ssh2

%msgtime% %appipv4% %appname% : %action% outbound %protocol% connection %sessionid% for %string% : %srcipv4% / %srcport% ( %ipv4% / %integer% ) to %string% : %dstipv4% / %dstport% ( %ipv4% / %integer% )
# 2012-04-05 18:46:18   172.23.0.1  %ASA-6-302013: Built outbound TCP connection 1424575 for outside:10.32.0.100/80 (10.32.0.100/80) to inside:172.23.73.72/2522 (10.32.0.1/54702)

%msgtime% %apphost% %appname% : %string% : tty = %string% ; pwd = %string% ; user = %srcuser% ; command = %command% - %string%
# Jan 15 14:09:11 irc sudo:    jlz : TTY=pts/1 ; PWD=/home/jlz ; USER=root ; COMMAND=/bin/su - irc
</code></pre>

<p>As you can see, the output is not 100%, but it gets us pretty close. Once the analyst goes through and updates the rules, he/she can re-run the analyzer anytime with any file to determine if there&rsquo;s new patterns. For example, below, we ran the sshd log file with an existing pattern file, and got 4 new log patterns.</p>

<pre><code>$ go run sequence.go analyze -i ../../data/sshd.all -p ../../patterns/sshd.txt -o sshd.analyze
Analyzed 212897 messages, found 39 unique patterns, 4 are new.
</code></pre>

<h3 id="parser-quick-review:61b1adc09fb9bc31a28d4faabfef3631">Parser - Quick Review</h3>

<p>I wrote about the <a href="http://zhen.org/blog/sequence-high-performance-sequential-semantic-log--parser/">sequence parser</a> a couple of weeks back. It is a <em>high performance sequential log parser</em>. It <em>sequentially</em> goes through a log message, <em>parses</em> out the meaningful parts, without the use regular expressions. It can achieve <em>high performance</em> parsing of <strong>100,000 - 200,000 messages per second (MPS)</strong> without the need to separate parsing rules by log source type. Underneath the hood, the <code>sequence</code> parser basically constructs a tree based on the sequential rules, walks the tree to identify all the possible paths, and returns the path that has the best match (highest weight) for the message.</p>

<p>While the analyzer is about reducing a large corupus of raw log messages down to a small set of unique patterns, the parser is all about matching log messages to an existing set of patters and determining whether a specific pattern has matched. Based on the pattern, it returns a sequence of tokens that basically extracts out the important pieces of information from the logs. The analysts can then take this sequence and perform other types of analysis.</p>

<p>The approach taken by the <code>sequence</code> parser is pretty much the same as liblognorm or other tree-based approaches.</p>

<h2 id="sequence-analyzer:61b1adc09fb9bc31a28d4faabfef3631">Sequence Analyzer</h2>

<p>In the following section I will go through additional details of how the <code>sequence</code> analyzer reduces 100 of 1000&rsquo;s of raw log messages down to just 10&rsquo;s of unique patterns, and then determining how to label the individual tokens.</p>

<h3 id="identifying-unique-patterns:61b1adc09fb9bc31a28d4faabfef3631">Identifying Unique Patterns</h3>

<p>Analyzer builds an analysis tree that represents all the Sequences from messages. It can be used to determine all of the unique patterns for a large body of messages.</p>

<p>It&rsquo;s based on a single basic concept, that for multiple log messages, if tokens in the same position shares one same parent and one same child, then the tokens in that position is likely variable string, which means it&rsquo;s something we can extract. For example, take a look at the following two messages:</p>

<pre><code>Jan 12 06:49:42 irc sshd[7034]: Accepted password for root from 218.161.81.238 port 4228 ssh2
Jan 12 14:44:48 jlz sshd[11084]: Accepted publickey for jlz from 76.21.0.16 port 36609 ssh2
</code></pre>

<p>The first token of each message is a timestamp, and the 3rd token of each message is the literal &ldquo;sshd&rdquo;. For the literals &ldquo;irc&rdquo; and &ldquo;jlz&rdquo;, they both share a common parent, which is a timestamp. They also both share a common child, which is &ldquo;sshd&rdquo;. This means token in between these, the 2nd token in each message, likely represents a variable token in this message type. In this case, &ldquo;irc&rdquo; and &ldquo;jlz&rdquo; happens to
represent the syslog host.</p>

<p>Looking further down the message, the literals &ldquo;password&rdquo; and &ldquo;publickey&rdquo; also share a common parent, &ldquo;Accepted&rdquo;, and a common child, &ldquo;for&rdquo;. So that means the token in this position is also a variable token (of type TokenString).</p>

<p>You can find several tokens that share common parent and child in these two messages, which means each of these tokens can be extracted. And finally, we can determine that the single pattern that will match both is:</p>

<pre><code>%time% %string% sshd [ %integer% ] : Accepted %string% for %string% from %ipv4% port %integer% ssh2
</code></pre>

<p>If later we add another message to this mix:</p>

<pre><code>Jan 12 06:49:42 irc sshd[7034]: Failed password for root from 218.161.81.238 port 4228 ssh2
</code></pre>

<p>The Analyzer will determine that the literals &ldquo;Accepted&rdquo; in the 1st message, and &ldquo;Failed&rdquo; in the 3rd message share a common parent &ldquo;:&rdquo; and a common child &ldquo;password&rdquo;, so it will determine that the token in this position is also a variable token. After all three messages are analyzed, the final pattern that will match all three
messages is:</p>

<pre><code>%time% %string% sshd [ %integer% ] : %string% %string% for %string% from %ipv4% port %integer% ssh2
</code></pre>

<p>By applying this concept, we can effectively identify all the unique patterns in a log file.</p>

<h3 id="determining-the-correct-labels:61b1adc09fb9bc31a28d4faabfef3631">Determining the Correct Labels</h3>

<p>Now that we have the unique patterns, we will scan the tokens to determine which labels we should apply to them.</p>

<p>System and network logs are mostly free form text. There&rsquo;s no specific patterns to any of them. So it&rsquo;s really difficult to determine how to label specific parts of the log message automatically. However, over the years, after looking at so many system and network log messages, some patterns will start to emerge.</p>

<p>There&rsquo;s no &ldquo;machine learning&rdquo; here. This section is all about codifying these human learnings. I&rsquo;ve created the following 6 rules to help label tokens in the log messages. By no means are these rules perfect. They are at best just guesses on how to label. But hopefully they can get us 75% of the way there and we human can just take it the rest of the way.</p>

<p><strong>0. Parsing Email and Hostname Formats</strong></p>

<p>This is technically not a labeling step. Before we actually start the labeling process, we wanted to first parse out a couple more formats like email and host names. The message tokenizer doesn&rsquo;t recognize these because they are difficult to parse and will slow down the tokenizer. These specific formats are also not needed by the parser. So because the analyzer doesn&rsquo;t care about performance as much, we can do this as post-processing step.</p>

<p>To recognize the hostname, we try to match the &ldquo;effective TLD&rdquo; using the <a href="https://github.com/surge/xparse/tree/master/etld">xparse/etld</a> package. It is an effective TLD matcher that returns the length of the effective domain name for the given string. It uses the data set from <a href="https://www.publicsuffix.org/list/effective_tld_names.dat">https://www.publicsuffix.org/list/effective_tld_names.dat</a>.</p>

<p><strong>1. Recognizing Syslog Headers</strong></p>

<p>First we will try to see if we can regonize the syslog headers. We try to recogize both RFC5424 and RFC3164 syslog headers:</p>

<pre><code>	// RFC5424
	// - &quot;1 2003-10-11T22:14:15.003Z mymachine.example.com evntslog - ID47 ...&quot;
	// - &quot;1 2003-08-24T05:14:15.000003-07:00 192.0.2.1 myproc 8710 - ...&quot;
	// - &quot;1 2003-10-11T22:14:15.003Z mymachine.example.com su - ID47 ...&quot;
	// RFC3164
	// - &quot;Oct 11 22:14:15 mymachine su: ...&quot;
	// - &quot;Aug 24 05:34:00 CST 1987 mymachine myproc[10]: ...&quot;
	// - &quot;jan 12 06:49:56 irc last message repeated 6 times&quot;
</code></pre>

<p>If the sequence pattern matches any of the above sequence, then we assume the first few tokens belong to the syslog header.</p>

<p><strong>2. Marking Key and Value Pairs</strong></p>

<p>The next step we perform is to mark known &ldquo;keys&rdquo;. There are two types of keys. First, we identify any token before the &ldquo;=&rdquo; as a key. For example, the message <code>fw=TOPSEC priv=6 recorder=kernel type=conn</code> contains 4 keys: <code>fw</code>, <code>priv</code>, <code>recorder</code> and <code>type</code>. These keys should be considered string literals, and should not be extracted. However, they can be used to determine how the value part should be labeled.</p>

<p>The second types of keys are determined by keywords that often appear in front of other tokens, I call these <strong>prekeys</strong>. For example, we know that the prekey <code>from</code> usually appears in front of any source host or IP address, and the prekey <code>to</code> usually appears in front of any destination host or IP address. Below are some examples of these prekeys.</p>

<pre><code>from 		= [ &quot;%srchost%&quot;, &quot;%srcipv4%&quot; ]
port 		= [ &quot;%srcport%&quot;, &quot;%dstport%&quot; ]
proto		= [ &quot;%protocol%&quot; ]
sport		= [ &quot;%srcport%&quot; ]
src 		= [ &quot;%srchost%&quot;, &quot;%srcipv4%&quot; ]
to 			= [ &quot;%dsthost%&quot;, &quot;%dstipv4%&quot;, &quot;%dstuser%&quot; ]
</code></pre>

<p>To help identify these prekeys, I wrote a quick program that goes through many of the logs I have to help identify what keywords appears before IP address, mac addresses, and other non-literal tokens. The result is put into the <a href="https://github.com/strace/sequence/blob/master/keymaps.go">keymaps.go</a> file. It&rsquo;s not comprehensive, but it&rsquo;s also not meant to be. We just need enough hints to help with labeling.</p>

<p><strong>3. Labeling &ldquo;Values&rdquo; by Their Keys</strong></p>

<p>Once the keys are labeled, we can label the values based on the mapping described above. For key/value pairs, we try to recognize both <code>key=value</code> or <code>key=&quot;value&quot;</code> formats (or other quote characters like &lsquo; or &lt;).</p>

<p>For the prekeys, we try to find the value token within 2 tokens of the key token. That means sequences such as <code>from 192.168.1.1</code> and <code>from ip 192.168.1.1</code> will identify <code>192.168.1.1</code> as the <code>%srcipv4%</code> based on the above mapping, but we will miss <code>from ip address 192.168.1.1</code>.</p>

<p><strong>4. Identifying Known Keywords</strong></p>

<p>Within most log messages, there are certain keywords that would indicate what actions were performed, what the state/status of the action was, and what objects the actions were performed on. CEE had a list that it identified, so I copied the list and added some of my own.</p>

<pre><code>action = [
	&quot;access&quot;,
	&quot;alert&quot;,
	&quot;allocate&quot;,
	&quot;allow&quot;,
	.
	.
	.
]

status = [
	&quot;accept&quot;,
	&quot;error&quot;,
	&quot;fail&quot;,
	&quot;failure&quot;,
	&quot;success&quot;
]

object = [
	&quot;account&quot;,
	&quot;app&quot;,
	&quot;bios&quot;,
	&quot;driver&quot;,
	.
	.
	.
]
</code></pre>

<p>In our labeling process, we basically goes through and identify all the string literals that are NOT marked as keys, and perform a <a href="https://github.com/surge/porter2">porter2 stemming operation</a> on the literal, then compare to the above list (which is also porter2 stemmed).</p>

<p>If a literal matches one of the above lists, then the corresponding label (<code>action</code>, <code>status</code>, <code>object</code>, <code>srcuser</code>, <code>method</code>, or <code>protocol</code>) is applied.</p>

<p><strong>5. Determining Positions of Specific Types</strong></p>

<p>In this next step, we are basically looking at the position of where some of the token types appear. Specifically, we are looking for <code>%time%</code>, <code>%url%</code>, <code>%mac%</code>, <code>%ipv4%</code>, <code>%host%</code>, and <code>%email%</code> tokens. Assuming the labels have not already been taken with the previous rules, the rules are as follows:</p>

<ul>
<li>The first %time% token is labeled as %msgtime%</li>
<li>The first %url% token is labeled as %object%</li>
<li>The first %mac% token is labeled as %srcmac% and the second is labeld as %dstmac%</li>
<li>The first %ipv4% token is labeled as %srcipv4% and the second is labeld as %dstipv4%</li>
<li>The first %host% token is labeled as %srchost% and the second is labeld as %dsthost%</li>
<li>The first %email% token is labeled as %srcemail% and the second is labeld as %dstemail%</li>
</ul>

<p><strong>6. Scanning for ip/port or ip:port Pairs</strong></p>

<p>Finally, after all that, we scan through the sequence again, and identify any numbers that follow an IP address, but separated by either a &ldquo;/&rdquo; or &ldquo;:&ldquo;. Then we label these numbers as either <code>%srcport%</code> or <code>%dstport%</code> based on how the previous IP address is labeled.</p>

<h3 id="summary:61b1adc09fb9bc31a28d4faabfef3631">Summary</h3>

<p>There are some limitations to the <code>sequence</code> parser and analyzer. For example, currently <code>sequence</code> does not handle multi-line logs. Each log message must appear as a single line. So if there&rsquo;s multi-line logs, they must be first be converted into a single line. Also, <code>sequence</code> has been only tested with a limited set of system (Linux, AIX, sudo, ssh, su, dhcp, etc etc), network (ASA, PIX, Neoteris, CheckPoint, Juniper Firewall) and infrastructure application (apache, bluecoat, etc) logs.</p>

<p>Documentation is available at godoc: <a href="http://godoc.org/github.com/strace/sequence">package</a>, <a href="http://godoc.org/github.com/strace/sequence/cmd/sequence">command</a>.</p>

<p>There are some pattern files developed for ASA, Sudo and SSH in the <code>patterns</code> directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages.</p>

<p>If you have a set of logs you would like me to test out, please feel free to <a href="https://github.com/strace/sequence/issues">open an issue</a> and we can arrange a way for me to download and test your logs.</p>

<p>Stay tuned for more log patterns&hellip;</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/blog/papers-i-read-2015-week-6/">
        Papers I Read: 2015 Week 6
      </a>
    </h1>

    <span class="post-date">Sun, Feb 8, 2015</span>

    

<p><a href="http://paperswelove.org/">Papers We Love</a> has been making rounds lately and a lot of people are excited about it. I also think it&rsquo;s kind of cool since I&rsquo;ve been reading a lot of research papers over the past year or so. I have been killing some trees because of that.</p>

<p>My interests have been mostly around data analytics, but the specific focus areas have changed a few times. I have read papers on data structures (bloom filters, skiplist, bitmap compression, etc), security analytics, consumer behavioral analysis, loyalty analytics, and now back to security analytics. In fact, recently I started reading a few security research papers that I found on <a href="http://www.covert.io/">covert.io</a>, put together by Jason Trost.</p>

<p>In any case, I thought it might be an interesting idea to share some of the papers I read/scan/skim on weekly basis. This way I can also track what I read over time.</p>

<h3 id="random-ramblings:e2137c343047358d9912399632750231">Random Ramblings</h3>

<p>This week has been a disaster. I was the last one in the family to catch the cold, but probably lasted the longest. In fact I am still only about 50%. This whole week I have been having headaches, body aches, and haven&rsquo;t been able to concentrate. My body must be trying to catch up on sleep or something. For a couple days I actually slept for almost 12 hours a night!</p>

<p>I&rsquo;ve been meaning to work on <a href="https://github.com/strace/sequence">sequence</a> and finish updating the analyzer, but really had a hard time concentrating. Any non-working hours are basically spent in bed if I could.</p>

<p>So this is probably the worst week to start the &ldquo;Papers I Read&rdquo; series since I only technically read 1 paper. But I am going to cheat a little, and list the papers I read over the past couple of weeks, pretty much all in my spare time.</p>

<p>This week we also saw Sony&rsquo;s accouncement that last year&rsquo;s hack cost them <a href="http://www.sony.net/SonyInfo/IR/financial/fr/150204_sony.pdf">$15 million</a> to investigate and remediate. It&rsquo;s pretty crazy if you think about it.</p>

<p>Let&rsquo;s assume that they hired a bunch of high-priced consultants, say $250/hour, to help comb through the logs and clean the systems. And let&rsquo;s say <sup>2</sup>&frasl;<sub>3</sub> of the $15m is spent on these consultants. That&rsquo;s <code>$10m / $250 = 40,000 hours</code>.</p>

<p>Let&rsquo;s say these consultants worked full time, non-stop, no weekends, no breaks, for 2 months since the announcement on Nov 24, 2014, that would be a team of 56 people (<code>40,000 hours / 60 days / 12 hours/day = 56</code>) working 12 hour days!</p>

<p>I&rsquo;ll tell ya, these security guys are raking it in. They make money upfront by selling products/services to protect the company, then they make money in the back by selling forensic services to clean up after the hack.</p>

<p>[Disclaimer: any mistake in my calculations/assumptions I blame on my drugged brain cells.]</p>

<h3 id="papers-i-read:e2137c343047358d9912399632750231">Papers I Read</h3>

<ul>
<li><a href="http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf">Beehive: Large-Scale Log Analysis for Detecting Suspicious Activity in Enterprise Networks</a></li>
</ul>

<blockquote>
<p>We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents.</p>
</blockquote>

<ul>
<li><a href="http://minds.cs.umn.edu/publications/chapter.pdf">Data Mining for Cyber Security</a></li>
</ul>

<blockquote>
<p>This chapter provides an overview of the Minnesota Intrusion Detection System (MINDS), which uses a suite of data mining based algorithms to address different aspects of cyber security. The various components of MINDS such as the scan detector, anomaly detector and the profiling module detect different types of attacks
and intrusions on a computer network.</p>
</blockquote>

<ul>
<li><a href="http://www.covert.io/research-papers/security/VAST-%20Network%20Visibility%20Across%20Space%20and%20Time.pdf">VAST: Network Visibility Across Space and Time</a></li>
</ul>

<blockquote>
<p>Key operational networking tasks, such as troubleshooting and defending against attacks, greatly benefit from attaining views of network activity that are unified across space and time. This means that data from heterogeneous devices and systems is treated in a uniformfashion, and that analyzing past activity and detecting future instances follow the same procedures. Based on previous ideas that formulated principles for comprehensive
network visibility [AKP+08], we present the design and architecture of Visibility Across Space and Time (VAST), an intelligent database that serves as a single vantage point into the network. The system is based on a generic event model to handle network data from disparate sources and provides a query architecture that allows operators or remote applications to extract events matching a given condition. We implemented a proof-of-principle prototype that can archive and index events from a wide range of sources. Moreover, we conducted a preliminary performance evaluation to verify that our implementation works efficient and as expected.</p>
</blockquote>

<ul>
<li><a href="http://www.covert.io/research-papers/security/Finding%20The%20Needle-%20Suppression%20of%20False%20Alarms%20in%20Large%20Intrusion%20Detection%20Data%20Sets.pdf">Finding The Needle: Suppression of False Alarms in Large Intrusion Detection Data Sets</a></li>
</ul>

<blockquote>
<p>Managed security service providers (MSSPs) must manage and monitor thousands of intrusion detection sensors.
The sensors often vary by manufacturer and software version, making the problem of creating generalized tools to separate true attacks from false positives particularly difficult. Often times it is useful from an operations perspective to know if a particular sensor is acting out of character. We propose a solution to this problem using anomaly detection techniques over the set of alarms produced by the sensors. Similar to the manner in which an anomaly based sensor detects deviations from normal user or system behavior, we establish the baseline
behavior of a sensor and detect deviations from this baseline. We show that departures from this profile by a sensor have a high probability of being artifacts of genuine attacks. We evaluate a set of time-based Markovian heuristics against a simple compression algorithm and show that we are able to detect the existence of all attacks which were manually identified by security personnel, drastically reduce the number of false positives, and identify attacks which were overlooked during manual evaluation.</p>
</blockquote>

<ul>
<li><a href="http://user.informatik.uni-goettingen.de/~krieck/docs/2013a-aisec.pdf">A Close Look on n-Grams in Intrusion Detection: Anomaly Detection vs. Classification</a></li>
</ul>

<blockquote>
<p>Detection methods based on n-gram models have been widely studied for the identification of attacks and malicious software. These methods usually build on one of two learning schemes: anomaly detection, where a model of normality is constructed from n-grams, or classification, where a discrimination between benign and malicious n-grams is learned. Although successful in many security domains, previous work falls short of explaining why a particular scheme is used and more importantly what renders one favorable over the other for a given type of data. In this paper we provide a close look on n-gram models for intrusion detection. We specifically study anomaly detection and classification using n-grams and develop criteria for data being used in one or the other
scheme. Furthermore, we apply these criteria in the scope of web intrusion detection and empirically validate their effectiveness with different learning-based detection methods for client-side and service-side attacks.</p>
</blockquote>

<ul>
<li><a href="http://blog.scalyr.com/2014/05/searching-20-gbsec-systems-engineering-before-algorithms/">Searching 20 GB/sec: Systems Engineering Before Algorithms</a></li>
</ul>

<p>Ok, this is a blog post, not a research paper, but it&rsquo;s somewhat interesting nonetheless.</p>

<blockquote>
<p>This article describes how we met that challenge using an “old school”, brute-force approach, by eliminating layers and avoiding complex data structures. There are lessons here that you can apply to your own engineering challenges.</p>
</blockquote>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/blog/sequence-high-performance-sequential-semantic-log--parser/">
        Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS
      </a>
    </h1>

    <span class="post-date">Sun, Feb 1, 2015</span>

    

<p><a href="http://godoc.org/github.com/strace/sequence"><img src="http://godoc.org/github.com/strace/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="http://godoc.org/github.com/strace/sequence/sequence"><img src="http://godoc.org/github.com/strace/sequence/sequence?status.svg" alt="GoDoc" />
</a></p>

<p><a href="https://github.com/strace/sequence">github repo</a></p>

<p>This is part 1 of the <code>sequence</code> series. <a href="http://zhen.org/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Part </a> an automated analyzer that reduce 100,000&rsquo;s of Log Messages to 10&rsquo;s of Patterns.</p>

<hr />

<p><code>sequence</code> is a <em>high performance sequential log parser</em>. It <em>sequentially</em> goes through a log message, <em>parses</em> out the meaningful parts, without the use regular expressions. It can achieve <em>high performance</em> parsing of <strong>100,000 - 200,000 messages per second (MPS)</strong> without the need to separate parsing rules by log source type.</p>

<p><strong><code>sequence</code> is currently under active development and should be considered unstable until further notice.</strong></p>

<p><strong>If you have a set of logs you would like me to test out, please feel free to <a href="https://github.com/strace/sequence/issues">open an issue</a> and we can arrange a way for me to download and test your logs.</strong></p>

<h3 id="motivation:d4419a1968098b45153921045f06326c">Motivation</h3>

<p>Log messages are notoriusly difficult to parse because they all have different formats. Industries (see Splunk, ArcSight, Tibco LogLogic, Sumo Logic, Logentries, Loggly, LogRhythm, etc etc etc) have been built to solve the problems of parsing, understanding and analyzing log messages.</p>

<p>Let&rsquo;s say you have a bunch of log files you like to parse. The first problem you will typically run into is you have no way of telling how many DIFFERENT types of messages there are, so you have no idea how much work there will be to develop rules to parse all the messages. Not only that, you have hundreds of thousands, if not  millions of messages, in front of you, and you have no idea what messages are worth parsing, and what&rsquo;s not.</p>

<p>The typical workflow is develop a set of regular expressions and keeps testing against the logs until some magical moment where all the logs you want parsed are parsed. Ask anyone who does this for a living and they will tell you this process is long, frustrating and error-prone.</p>

<p>Even after you have developed a set of regular expressions that match the original set of messages, if new messages come in, you will have to determine which of the new messages need to be parsed. And if you develop a new set of regular expressions to parse those new messages, you still have no idea if the regular expressions will conflict with the ones you wrote before. If you write your regex parsers too liberally, it can easily parse the wrong messages.</p>

<p>After all that, you will end up finding out the regex parsers are quite slow. It can typically parse several thousands messages per second. Given enough CPU resources on a large enough machine, regex parsers can probably parse tens of thousands of messages per second. Even to achieve this type of performance, you will likely need to limit the number of regular expressions the parser has. The more regex rules, the slower the parser will go.</p>

<p>To work around this performance issue, companies have tried to separate the regex rules for different log message types into different parsers. For example, they will have a parser for Cisco ASA logs, a parser for sshd logs, a parser for Apache logs, etc etc. And then they will require the users to tell them which parser to use (usually by indicating the log source type of the originating IP address or host.)</p>

<p>Sequence is developed to make analyzing and parsing log messages a lot easier and faster.</p>

<h3 id="performance:d4419a1968098b45153921045f06326c">Performance</h3>

<p>The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro. The first file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message.</p>

<pre><code>  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 1.69 secs, ~ 126319.27 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 2.89 secs, ~ 81323.41 msgs/sec

  $ ./sequence bench -d ../patterns -i ../data/asasshsudo.log
  Parsed 447745 messages in 4.47 secs, ~ 100159.65 msgs/sec
</code></pre>

<p>Performance can be improved by adding more cores:</p>

<pre><code>  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.00 secs, ~ 212711.83 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 1.56 secs, ~ 150769.68 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -d ../patterns -i ../data/asasshsudo.log -w 2
  Parsed 447745 messages in 2.52 secs, ~ 177875.94 msgs/sec
</code></pre>

<h3 id="documentation:d4419a1968098b45153921045f06326c">Documentation</h3>

<p>Documentation is available at godoc: <a href="http://godoc.org/github.com/strace/sequence">package</a>, <a href="http://godoc.org/github.com/strace/sequence/sequence">command</a>.</p>

<h3 id="license:d4419a1968098b45153921045f06326c">License</h3>

<p>Copyright &copy; 2014 Dataence, LLC. All rights reserved.</p>

<p>Licensed under the Apache License, Version 2.0 (the &ldquo;License&rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>

<p><a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>

<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &ldquo;AS IS&rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>

<h3 id="roadmap-futures:d4419a1968098b45153921045f06326c">Roadmap / Futures</h3>

<p>There are some pattern files developed for ASA, Sudo and SSH in the <code>patterns</code> directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages. So currently there&rsquo;s not a set roadmap.</p>

<h2 id="concepts:d4419a1968098b45153921045f06326c">Concepts</h2>

<p>The following concepts are part of the package:</p>

<ul>
<li><p>A <em>Token</em> is a piece of information extracted from the original log message. It is a struct that contains fields for <em>TokenType</em>, <em>FieldType</em>, <em>Value</em>, and indicators of whether it&rsquo;s a key or value in the key=value pair.</p></li>

<li><p>A <em>TokenType</em> indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.</p></li>

<li><p>A <em>FieldType</em> indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).</p></li>

<li><p>A <em>Sequence</em> is a list of Tokens. It is returned by the <em>Tokenizer</em>, and the <em>Parser</em>.</p></li>

<li><p>A <em>Scanner</em> is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, IPv4 addresses, URLs, MAC addresses,
integers and floating point numbers. It also recgonizes key=value or key=&ldquo;value&rdquo; or key=&lsquo;value&rsquo; or key=<value> pairs.</p></li>

<li><p>A <em>Parser</em> is a tree-based parsing engine for log messages. It builds a parsing tree based on pattern sequence supplied, and for each message sequence, returns the matching pattern sequence. Each of the message tokens will be marked with the semantic field types.</p></li>
</ul>

<h2 id="sequence-command:d4419a1968098b45153921045f06326c">Sequence Command</h2>

<p>The <code>sequence</code> command is developed to demonstrate the use of this package. You can find it in the <code>sequence</code> directory. The <code>sequence</code> command implements the <em>sequential semantic log parser</em>.</p>

<pre><code>   Usage:
     sequence [command]

   Available Commands:
     scan                      scan will tokenize a log file or message and output a list of tokens
     parse                     parse will parse a log file and output a list of parsed tokens for each of the log messages
     bench                     benchmark the parsing of a log file, no output is provided
     help [command]            Help about any command
</code></pre>

<h3 id="scan:d4419a1968098b45153921045f06326c">Scan</h3>

<pre><code>  Usage:
    sequence scan [flags]

   Available Flags:
    -h, --help=false: help for scan
    -m, --msg=&quot;&quot;: message to tokenize
</code></pre>

<p>Example</p>

<pre><code>  $ ./sequence scan -m &quot;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream&quot;
  #   0: { Field=&quot;%funknown%&quot;, Type=&quot;%ts%&quot;, Value=&quot;jan 14 10:15:56&quot; }
  #   1: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;testserver&quot; }
  #   2: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;sudo&quot; }
  #   3: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   4: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;gonner&quot; }
  #   5: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   6: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;tty&quot; }
  #   7: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #   8: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;pts/3&quot; }
  #   9: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  10: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;pwd&quot; }
  #  11: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  12: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/home/gonner&quot; }
  #  13: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  14: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;user&quot; }
  #  15: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  16: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;root&quot; }
  #  17: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;;&quot; }
  #  18: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;command&quot; }
  #  19: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  20: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;/bin/su&quot; }
  #  21: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;-&quot; }
  #  22: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;ustream&quot; }
</code></pre>

<h3 id="parse:d4419a1968098b45153921045f06326c">Parse</h3>

<pre><code>  Usage:
    sequence parse [flags]

   Available Flags:
    -h, --help=false: help for parse
    -i, --infile=&quot;&quot;: input file, required
    -o, --outfile=&quot;&quot;: output file, if empty, to stdout
    -d, --patdir=&quot;&quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&quot;&quot;: initial pattern file, required
</code></pre>

<p>The following command parses a file based on existing rules. Note that the
performance number (9570.20 msgs/sec) is mostly due to reading/writing to disk.
To get a more realistic performance number, see the benchmark section below.</p>

<pre><code>  $ ./sequence parse -d ../../patterns -i ../../data/sshd.all  -o parsed.sshd
  Parsed 212897 messages in 22.25 secs, ~ 9570.20 msgs/sec
</code></pre>

<p>This is an entry from the output file:</p>

<pre><code>  Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
  #   0: { Field=&quot;%createtime%&quot;, Type=&quot;%ts%&quot;, Value=&quot;jan 15 19:39:26&quot; }
  #   1: { Field=&quot;%apphost%&quot;, Type=&quot;%string%&quot;, Value=&quot;jlz&quot; }
  #   2: { Field=&quot;%appname%&quot;, Type=&quot;%string%&quot;, Value=&quot;sshd&quot; }
  #   3: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;[&quot; }
  #   4: { Field=&quot;%sessionid%&quot;, Type=&quot;%integer%&quot;, Value=&quot;7778&quot; }
  #   5: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;]&quot; }
  #   6: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #   7: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;pam_unix&quot; }
  #   8: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;(&quot; }
  #   9: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;sshd&quot; }
  #  10: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #  11: { Field=&quot;%funknown%&quot;, Type=&quot;%string%&quot;, Value=&quot;session&quot; }
  #  12: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;)&quot; }
  #  13: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;:&quot; }
  #  14: { Field=&quot;%object%&quot;, Type=&quot;%string%&quot;, Value=&quot;session&quot; }
  #  15: { Field=&quot;%action%&quot;, Type=&quot;%string%&quot;, Value=&quot;opened&quot; }
  #  16: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;for&quot; }
  #  17: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;user&quot; }
  #  18: { Field=&quot;%dstuser%&quot;, Type=&quot;%string%&quot;, Value=&quot;jlz&quot; }
  #  19: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;by&quot; }
  #  20: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;(&quot; }
  #  21: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;uid&quot; }
  #  22: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;=&quot; }
  #  23: { Field=&quot;%funknown%&quot;, Type=&quot;%integer%&quot;, Value=&quot;0&quot; }
  #  24: { Field=&quot;%funknown%&quot;, Type=&quot;%literal%&quot;, Value=&quot;)&quot; }
</code></pre>

<h3 id="benchmark:d4419a1968098b45153921045f06326c">Benchmark</h3>

<pre><code>  Usage:
    sequence bench [flags]

   Available Flags:
    -c, --cpuprofile=&quot;&quot;: CPU profile filename
    -h, --help=false: help for bench
    -i, --infile=&quot;&quot;: input file, required
    -d, --patdir=&quot;&quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&quot;&quot;: pattern file, required
    -w, --workers=1: number of parsing workers
</code></pre>

<p>The following command will benchmark the parsing of two files. First file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA
log file, averaging 180 bytes per message.</p>

<pre><code>  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 1.69 secs, ~ 126319.27 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 2.89 secs, ~ 81323.41 msgs/sec
</code></pre>

<p>Performance can be improved by adding more cores:</p>

<pre><code>  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.00 secs, ~ 212711.83 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 1.56 secs, ~ 150769.68 msgs/sec
</code></pre>

  </div>
  
</div>

<div class="posts">
  <h1 class="post-title">Archive</h1>
  <ul class="posts">
      
      <li><span><a href="http://localhost:1313/blog/sequence-automated-analyzer-for-reducing-100k-messages-to-10s-of-patterns/">Sequence: Automated Analyzer for Reducing 100,000&#39;s of Log Messages to 10&#39;s of Patterns</a> - <time class="pull-right post-list">Tue, Feb 10, 2015</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/papers-i-read-2015-week-6/">Papers I Read: 2015 Week 6</a> - <time class="pull-right post-list">Sun, Feb 8, 2015</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/sequence-high-performance-sequential-semantic-log--parser/">Sequence: A High Performance Sequential Semantic Log Parser at 175,000 MPS</a> - <time class="pull-right post-list">Sun, Feb 1, 2015</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/generating-porter2-fsm-for-fun-and-performance/">Generating Porter2 FSM For Fun and Performance in Go</a> - <time class="pull-right post-list">Wed, Jan 21, 2015</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/golang-from-a-non-programmers-perspective/">Go: From a Non-Programmer&#39;s Perspective</a> - <time class="pull-right post-list">Tue, Jan 13, 2015</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">PingMQ: A SurgeMQ-based ICMP Monitoring Tool</a> - <time class="pull-right post-list">Thu, Dec 25, 2014</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">SurgeMQ: High Performance MQTT Server and Client Libraries in Go</a> - <time class="pull-right post-list">Wed, Dec 24, 2014</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/surgemq-mqtt-message-queue-750k-mps/">SurgeMQ: MQTT Message Queue @ 750,000 MPS</a> - <time class="pull-right post-list">Thu, Dec 4, 2014</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/graceful-shutdown-of-go-net-dot-listeners/">Graceful Shutdown of Go net.Listeners</a> - <time class="pull-right post-list">Thu, Dec 12, 2013</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/ring-buffer-variable-length-low-latency-disruptor-style/">Ring Buffer - Variable-Length, Low-Latency, Lock-Free, Disruptor-Style</a> - <time class="pull-right post-list">Sat, Nov 30, 2013</h4></time></span></li>
      
  </ul>
</div>

</div>

  </body>

</html>
