<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title> Zen 3.1 &middot; Zen 3.1 </title>

  
  <link rel="stylesheet" href="http://localhost:1313/css/poole.css">
  <link rel="stylesheet" href="http://localhost:1313/css/syntax.css">
  <link rel="stylesheet" href="http://localhost:1313/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.ico">

  
  <link href="http://localhost:1313/index.xml/" rel="alternate" type="application/rss+xml" title="Zen 3.1" />

  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-681691-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1 class="brand"><a href="http://localhost:1313">Zen 3.1</a></h1>
      <p class="lead">
       Product. Data. Code 
      </p>
    </div>



    <ul class="sidebar-nav">
      <li><a href="http://localhost:1313/blog">Archive</a></li>
      
    </ul>
      <a href="https://twitter.com/zhenjl"><i class="fa fa-twitter-square"></i></a>&nbsp;&nbsp;
      
      
      <a href="https://github.com/zhenjl"><i class="fa fa-github-square"></i></a>&nbsp;&nbsp;
      

    <p class="footnote">Powered by <a href="http://hugo.spf13.com">Hugo</a> <br/>
    Theme originally made by <a href="http://twitter.com/mdo">@mdo</a> <br/>
    Theme modified by <a href="http://npf.io">Nate Finch</a> <br/>
    &copy; 2015 Jian Zhen. All rights reserved.</p>
    
  </div>
</div>

    <div class="content container">
<div class="posts">

  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">
        PingMQ: A SurgeMQ-based ICMP Monitoring Tool
      </a>
    </h1>

    <span class="post-date">Thu, Dec 25, 2014</span>

    

<p><a href="https://github.com/surge/surgemq/tree/master/cmd/pingmq">pingmq</a> is developed to demonstrate the different use cases one can use <a href="//surgemq.com">SurgeMQ</a>, a high performance MQTT server and client library. In this simplified use case, a network administrator can setup server uptime monitoring system by periodically sending ICMP ECHO_REQUEST to all the IPs in their network, and send the results to SurgeMQ.</p>

<p>Then multiple clients can subscribe to results based on their different needs. For example, a client maybe only interested in any failed ping attempts, as that would indicate a host might be down. After a certain number of failures the client may then raise some type of flag to indicate host down.</p>

<p>There are three benefits of using SurgeMQ for this use case.</p>

<ul>
<li>First, with all the different monitoring tools out there that wants to know if hosts are up or down, they can all now subscribe to a single source of information. They no longer need to write their own uptime tools.</li>
<li>Second, assuming there are 5 monitoring tools on the network that wants to ping each and every host, the small packets are going to congest the network. The company can save 80% on their uptime monitoring bandwidth by having a single tool that pings the hosts, and have the rest subscribe to the results.</li>
<li>Third/last, the company can enhance their security posture by placing tighter restrictions on their firewalls if there&rsquo;s only a single host that can send ICMP ECHO_REQUESTS to all other hosts.</li>
</ul>

<p>The following commands will run pingmq as a server, pinging the 8.8.8.0/28 CIDR block, and publishing the results to /ping/success/{ip} and /ping/failure/{ip} topics every 30 seconds. <code>sudo</code> is needed because we are using RAW sockets and that requires root privilege.</p>

<pre><code>$ go build
$ sudo ./pingmq server -p 8.8.8.0/28 -i 30
</code></pre>

<p>The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.</p>

<pre><code>$ ./pingmq client -t /ping/failure/+
8.8.8.6: Request timed out for seq 1
</code></pre>

<p>The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.</p>

<pre><code>$ ./pingmq client -t /ping/success/+
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
</code></pre>

<p>One can also subscribe to a specific IP by using the following command.</p>

<pre><code>$ ./pingmq client -t /ping/+/8.8.8.8
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
</code></pre>

<h3 id="toc_0">Commands</h3>

<p>There are two builtin commands for <code>pingmq</code>.</p>

<p><strong><code>pingmq server</code></strong></p>

<pre><code>Usage:
  pingmq server [flags]

 Available Flags:
  -h, --help=false: help for server
  -i, --interval=60: ping interval in seconds
  -p, --ping=[]: Comma separated list of IPv4 addresses to ping
  -q, --quiet=false: print out ping results
  -u, --uri=&quot;tcp://:5836&quot;: URI to run the server on
</code></pre>

<p><strong><code>pingmq client</code></strong></p>

<pre><code>Usage:
  pingmq client [flags]

 Available Flags:
  -h, --help=false: help for client
  -s, --server=&quot;tcp://127.0.0.1:5836&quot;: PingMQ server to connect to
  -t, --topic=[]: Comma separated list of topics to subscribe to
</code></pre>

<h3 id="toc_1">IP Addresses</h3>

<p>To list IPs you like to use with <code>pingmq</code>, you can use the following formats:</p>

<pre><code>10.1.1.1      -&gt; 10.1.1.1
10.1.1.1,2    -&gt; 10.1.1.1, 10.1.1.2
10.1.1,2.1    -&gt; 10.1.1.1, 10.1.2.1
10.1.1,2.1,2  -&gt; 10.1.1.1, 10.1.1.2 10.1.2.1, 10.1.2.2
10.1.1.1-2    -&gt; 10.1.1.1, 10.1.1.2
10.1.1.-2     -&gt; 10.1.1.0, 10.1.1.1, 10.1.1.2
10.1.1.1-10   -&gt; 10.1.1.1, 10.1.1.2 ... 10.1.1.10
10.1.1.1-     -&gt; 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-3.1    -&gt; 10.1.1.1, 10.1.2.1, 10.1.3.1
10.1-3.1-3.1  -&gt; 10.1.1.1, 10.1.2.1, 10.1.3.1, 10.2.1.1, 10.2.2.1, 10.2.3.1, 10.3.1.1, 10.3.2.1, 10.3.3.1
10.1.1        -&gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-2      -&gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1-2        -&gt; 10.1.0.0, 10.1.0,1 ... 10.2.255.254, 10..2.255.255
10            -&gt; 10.0.0.0 ... 10.255.255.255
10.1.1.2,3,4  -&gt; 10.1.1.1, 10.1.1.2, 10.1.1.3, 10.1.1.4
10.1.1,2      -&gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1.1/28     -&gt; 10.1.1.0 ... 10.1.1.255
10.1.1.0/28   -&gt; 10.1.1.0 ... 10.1.1.15
10.1.1.0/30   -&gt; 10.1.1.0, 10.1.1.1, 10.1.1.2, 10.1.1.3
10.1.1.128/25 -&gt; 10.1.1.128 ... 10.1.1.255
</code></pre>

<h3 id="toc_2">Topic Format</h3>

<p>TO subscribe to the <code>pingmq</code> results, you can use the following formats:</p>

<ul>
<li><code>/ping/#</code> will subscribe to both success and failed pings for all IP addresses</li>
<li><code>ping/success/+</code> will subscribe to success pings for all IP addresses</li>
<li><code>ping/failure/+</code> will subscribe to failed pings for all IP addresses</li>
<li><code>ping/+/8.8.8.8</code> will subscribe to both success and failed pings for all IP 8.8.8.8</li>
</ul>

<h3 id="toc_3">Building</h3>

<p>To build <code>pingmq</code>, you need to have installed <a href="http://golang.org">Go 1.3.3 or 1.4</a>. Then run the following:</p>

<pre><code># go get github.com/surge/surgemq
# cd surgemq/examples/pingmq
# go build
</code></pre>

<p>After that, you should see the <code>pingmq</code> command in the pingmq directory.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">
        SurgeMQ: High Performance MQTT Server and Client Libraries in Go
      </a>
    </h1>

    <span class="post-date">Wed, Dec 24, 2014</span>

    

<p><strong>Happy Holidays!</strong></p>

<p>This is more of an announcement post as SurgeMQ is now compatibility-tested with some of the popular MQTT clients out there, and it&rsquo;s reaching <em>playable</em> state.</p>

<p>For completeness sake, please bear with some of the duplicate content in this post. The <a href="//blog/surgemq-mqtt-message-queue-750k-mps/">last post</a> made front page of <a href="https://news.ycombinator.com/item?id=8708921">Hacker News</a> and generated some great comments and discussions.</p>

<hr />

<p>SurgeMQ is a high performance MQTT broker and client library that aims to be fully compliant with MQTT 3.1 and 3.1.1 specs. The primary package that&rsquo;s of interest is package <a href="http://godoc.org/github.com/surge/surgemq/service">service</a>. It provides the MQTT Server and Client services in a library form.</p>

<p><strong>SurgeMQ is currently under active development and should be considered unstable until further notice.</strong></p>

<h3 id="toc_0">MQTT</h3>

<p>According to the <a href="http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html">MQTT spec</a>:</p>

<blockquote>
<p>MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.</p>

<p>The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include:</p>

<ul>
<li>Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications.</li>
<li>A messaging transport that is agnostic to the content of the payload.</li>
<li>Three qualities of service for message delivery:

<ul>
<li>&ldquo;At most once&rdquo;, where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after.</li>
<li>&ldquo;At least once&rdquo;, where messages are assured to arrive but duplicates can occur.</li>
<li>&ldquo;Exactly once&rdquo;, where message are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied.</li>
</ul></li>
<li>A small transport overhead and protocol exchanges minimized to reduce network traffic.</li>
<li>A mechanism to notify interested parties when an abnormal disconnection occurs.</li>
</ul>
</blockquote>

<p>There&rsquo;s some very large implementation of MQTT such as <a href="https://www.facebook.com/notes/facebook-engineering/building-facebook-messenger/10150259350998920">Facebook Messenger</a>. There&rsquo;s also an active Eclipse project, <a href="https://eclipse.org/paho/">Paho</a>, that provides scalable open-source client implementations for many different languages, including C/C++, Java, Python, JavaScript, C# .Net and Go.</p>

<h3 id="toc_1">Features, Limitations, and Future</h3>

<p><strong>Features</strong></p>

<ul>
<li>Supports QOS 0, 1 and 2 messages</li>
<li>Supports will messages</li>
<li>Supports retained messages (add/remove)</li>
<li>Pretty much everything in the spec except for the list below</li>
</ul>

<p><strong>Limitations</strong></p>

<ul>
<li>All features supported are in memory only. Once the server restarts everything is cleared.

<ul>
<li>However, all the components are written to be pluggable so one can write plugins based on the Go interfaces defined.</li>
</ul></li>
<li>Message redelivery on reconnect is not currently supported.</li>
<li>Message offline queueing on disconnect is not supported. Though this is also not a specific requirement for MQTT.</li>
</ul>

<p><strong>Future</strong></p>

<ul>
<li>Message re-delivery (DUP)</li>
<li>$SYS topics</li>
<li>Server bridge</li>
<li>Ack timeout/retry</li>
<li>Session persistence</li>
<li>Better authentication modules</li>
</ul>

<h3 id="toc_2">Performance</h3>

<p>Current performance benchmark of SurgeMQ, running all publishers, subscribers and broker on a single 4-core (2.8Ghz i7) MacBook Pro, is able to achieve:</p>

<ul>
<li>over <strong>400,000 MPS</strong> in a 1:1 single publisher and single producer configuration</li>
<li>over <strong>450,000 MPS</strong> in a 20:1 fan-in configuration</li>
<li>over <strong>750,000 MPS</strong> in a 1:20 fan-out configuration</li>
<li>over <strong>700,000 MPS</strong> in a full mesh configuration with 20 clients</li>
</ul>

<h3 id="toc_3">Compatibility</h3>

<p>In addition, SurgeMQ has been tested with the following client libraries and it <em>seems</em> to work:</p>

<ul>
<li><em>libmosquitto 1.3.5 (in C).</em> Tested with the bundled test programs msgsps_pub and msgsps_sub</li>
<li><em>Paho MQTT Conformance/Interoperability Testing Suite (in Python).</em> Tested with all 10 test cases, 3 did not pass. They are

<ol>
<li>&ldquo;offline messages queueing test&rdquo; which is not supported by SurgeMQ</li>
<li>&ldquo;redelivery on reconnect test&rdquo; which is not yet implemented by SurgeMQ</li>
<li>&ldquo;run subscribe failure test&rdquo; which is not a valid test</li>
</ol></li>
<li><em>Paho Go Client Library (in Go).</em> Tested with one of the tests in the library, in fact, that tests is now part of the tests for SurgeMQ.</li>
<li><em>Paho C Client library (in C).</em> Tested with most of the test cases and failed the same ones as the conformance test because the features are not yet implemented. Actually I think there&rsquo;s a bug in the test suite as it calls the PUBLISH handler function for non-PUBLISH messages.</li>
</ul>

<h3 id="toc_4">Documentation</h3>

<p>Documentation is available at <a href="http://godoc.org/github.com/surge/surgemq">godoc</a>.</p>

<p>More information regarding the design of the SurgeMQ is available at <a href="http://surgemq.com">zen 3.1</a>.</p>

<h3 id="toc_5">License</h3>

<p>Copyright &copy; 2014 Dataence, LLC. All rights reserved.</p>

<p>Licensed under the Apache License, Version 2.0 (the &ldquo;License&rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>

<p><a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>

<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &ldquo;AS IS&rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>

<h3 id="toc_6">Examples</h3>

<h4 id="toc_7">Server Example</h4>

<pre><code>// Create a new server
svr := &amp;service.Server{
    KeepAlive:        300,               // seconds
    ConnectTimeout:   2,                 // seconds
    SessionsProvider: &quot;mem&quot;,             // keeps sessions in memory
    Authenticator:    &quot;mockSuccess&quot;,     // always succeed
    TopicsProvider:   &quot;mem&quot;,             // keeps topic subscriptions in memory
}

// Listen and serve connections at localhost:1883
svr.ListenAndServe(&quot;tcp://:1883&quot;)
</code></pre>

<h4 id="toc_8">Client Example</h4>

<pre><code>// Instantiates a new Client
c := &amp;Client{}

// Creates a new MQTT CONNECT message and sets the proper parameters
msg := message.NewConnectMessage()
msg.SetWillQos(1)
msg.SetVersion(4)
msg.SetCleanSession(true)
msg.SetClientId([]byte(&quot;surgemq&quot;))
msg.SetKeepAlive(10)
msg.SetWillTopic([]byte(&quot;will&quot;))
msg.SetWillMessage([]byte(&quot;send me home&quot;))
msg.SetUsername([]byte(&quot;surgemq&quot;))
msg.SetPassword([]byte(&quot;verysecret&quot;))

// Connects to the remote server at 127.0.0.1 port 1883
c.Connect(&quot;tcp://127.0.0.1:1883&quot;, msg)

// Creates a new SUBSCRIBE message to subscribe to topic &quot;abc&quot;
submsg := message.NewSubscribeMessage()
submsg.AddTopic([]byte(&quot;abc&quot;), 0)

// Subscribes to the topic by sending the message. The first nil in the function
// call is a OnCompleteFunc that should handle the SUBACK message from the server.
// Nil means we are ignoring the SUBACK messages. The second nil should be a
// OnPublishFunc that handles any messages send to the client because of this
// subscription. Nil means we are ignoring any PUBLISH messages for this topic.
c.Subscribe(submsg, nil, nil)

// Creates a new PUBLISH message with the appropriate contents for publishing
pubmsg := message.NewPublishMessage()
pubmsg.SetPacketId(pktid)
pubmsg.SetTopic([]byte(&quot;abc&quot;))
pubmsg.SetPayload(make([]byte, 1024))
pubmsg.SetQoS(qos)

// Publishes to the server by sending the message
c.Publish(pubmsg, nil)

// Disconnects from the server
c.Disconnect()
</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="http://localhost:1313/blog/surgemq-mqtt-message-queue-750k-mps/">
        SurgeMQ: MQTT Message Queue @ 750,000 MPS
      </a>
    </h1>

    <span class="post-date">Thu, Dec 4, 2014</span>

    

<ul>
<li>Wow, this made front page of <a href="https://news.ycombinator.com/item?id=8708921">Hacker News</a>! First for me!</li>
<li>jacques_chester on HN has an <a href="https://news.ycombinator.com/item?id=8709146">EXCELLENT comment</a> that&rsquo;s definitely worth reading. <a href="https://news.ycombinator.com/item?id=8709557">My response</a>.</li>
</ul>

<h3 id="toc_0">tl;dr</h3>

<ul>
<li><a href="https://github.com/surge/surgemq">SurgeMQ</a> aims to provide a MQTT broker and client library that&rsquo;s fully compliant with <a href="http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html">MQTT spec 3.1.1</a>. In addition, it tries to be backward compatible with 3.1.</li>
<li>SurgeMQ is under active development and should be considered unstable. Some of the key MQTT requirements, such as retained messages, still need to be added. The eventual goal is to pass the <a href="https://eclipse.org/paho/clients/testing/">MQTT Conformance/Interoperability Testing</a>.</li>
<li>Having said that, running all publishers, subscribers and broker on a single 4-core (2.8Ghz i7) MacBook Pro, SurgeMQ is able to achieve

<ul>
<li><strong>over 400,000</strong> MPS in a 1:1 single publisher and single producer configuration</li>
<li><strong>over 450,000</strong> MPS in a 20:1 fan-in configuration</li>
<li><strong>over 750,000</strong> MPS in a 1:20 fan-out configuration</li>
<li><strong>over 700,000</strong> MPS in a full mesh configuration with 20 clients</li>
</ul></li>
<li>In developing SurgeMQ, I improved the performance 15-20X by keeping it simple and serial (KISS), reducing garbage collector pressure, reducing memory copy, and eliminating anything that could potentially introduce latency.</li>
<li>There are still many areas that can be improved and I look forward to hearing any suggestions you may have.</li>
<li>I cannot say this enough: <strong>benchmark, profile, optimize, rinse, repeat</strong>. Go has made testing, benchmarking, and profiling extremely simple. You owe it to yourself to optimize your code using these tools.</li>
</ul>

<blockquote>
<p>Lesson 1: Don&rsquo;t be clever. Keep It Simple and Serial (KISS).</p>

<p>Lesson 2: Reduce or remove memory copying.</p>

<p>Lesson 3: Race conditions can happen even if you think you followed all the right steps.</p>

<p>Lesson 4: Use the race detector!</p>
</blockquote>

<h3 id="toc_1">Go Learn Project #8 - Message Queue</h3>

<p>It&rsquo;s now been over a year since my last post! Family and work have occupied pretty much all of my time so spare time to learn Go was hard to come by.</p>

<p>However, I was able to squeeze in an implementation of a <a href="https://github.com/surge/mqtt">MQTT encoder/decoder</a> library in July. The implementation is now outdated and is no longer maintained, but it allowed me to learn about the <a href="http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html">MQTT protocol</a> and got me thinking about potentially implmenting a broker.</p>

<p>Now months later, I am finally able spend a few weekends and nights developing <a href="https://github.com/surge/surgemq">SurgeMQ</a>, a (soon to be) full MQTT 3.1.1 compliant message broker.</p>

<h4 id="toc_2">Message Queues</h4>

<p>According to <a href="http://en.wikipedia.org/wiki/Message_queue">Wikipedia</a>:</p>

<blockquote>
<p>Message queues provide an asynchronous communications protocol, meaning that the sender and receiver of the message do not need to interact with the message queue at the same time. Messages placed onto the queue are stored until the recipient retrieves them. Message queues have implicit or explicit limits on the size of data that may be transmitted in a single message and the number of messages that may remain outstanding on the queue.</p>
</blockquote>

<p>Tyler Treat of <a href="http://www.bravenewgeek.com">Brave New Geek</a> also wrote a <a href="http://www.bravenewgeek.com/tag/message-queues/">good series on message queues</a> that went over several of the key MQ implementations. One specific post, <a href="http://www.bravenewgeek.com/dissecting-message-queues/">Dissecting Message Queues</a>, is especially interesting because it benchmarks some of the major message queue implmentations out there, both brokered and brokerless.</p>

<p>In that post, Tyler found that borkerless queues had the highest throughput, achieving millions of MPS sent and received. Brokered message queue performances ranged from 12,000 MPS (<a href="nsq.io">NSQ</a>) to 195,000 MPS (<a href="nats.io">Gnatsd</a>). While the post showed that the Gnatsd latency to be around 300+ microseconds, in reality it&rsquo;s probably more like the NSQ in terms of latency due to the sender sleeping whenever Gnatsd is 10+ messages behind. Regardless, hats off to Tyler. Great job!</p>

<h4 id="toc_3">MQTT</h4>

<p>I got interested in MQTT because &ldquo;<a href="http://mqtt.org">MQTT</a> is a machine-to-machine (M2M)/&ldquo;Internet of Things&rdquo; connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport.&rdquo;</p>

<p>According to the <a href="http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html">MQTT spec</a>:</p>

<blockquote>
<p>MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.</p>

<p>The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include:</p>

<ul>
<li>Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications.</li>
<li>A messaging transport that is agnostic to the content of the payload.</li>
<li>Three qualities of service for message delivery:

<ul>
<li>&ldquo;At most once&rdquo;, where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after.</li>
<li>&ldquo;At least once&rdquo;, where messages are assured to arrive but duplicates can occur.</li>
<li>&ldquo;Exactly once&rdquo;, where message are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied.</li>
</ul></li>
<li>A small transport overhead and protocol exchanges minimized to reduce network traffic.</li>
<li>A mechanism to notify interested parties when an abnormal disconnection occurs.</li>
</ul>
</blockquote>

<p>There&rsquo;s some very large implementation of MQTT such as <a href="https://www.facebook.com/notes/facebook-engineering/building-facebook-messenger/10150259350998920">Facebook Messenger</a>. There&rsquo;s also an active Eclipse project, <a href="https://eclipse.org/paho/">Paho</a>, that provides scalable open-source client implementations for many different languages, including C/C++, Java, Python, JavaScript, C# .Net and Go.</p>

<p>Given the popularity, I decided to implement a MQTT broker in order to learn about message queues.</p>

<h3 id="toc_4">Architecture</h3>

<p><img src="/images/surgemq-mqtt-message-queue-750k-mps/smqfailedarch.png"></p>

<p>The above image showed a couple of the architecture approaches I attempted. In them, R is the receiver, which reads from net.Conn, P is the processor, which processes the messages and determines what to do or where to send them, and S is the sender, which sends any messages out to net.Conn. Each R, P, and S are their own goroutines.</p>

<p>I started the project wanting to be clever, and wanted to dynamically scale up/down a shared pool of processors as the number of messages increase/decrease. As I thought through it, it just got more and more complicated with the logic and coordination. At the end, before I even wrote much of the code, I scraped the idea.</p>

<blockquote>
<p>Lesson 1: Don&rsquo;t be clever. Keep It Simple and Serial (KISS).</p>
</blockquote>

<p>The second architecture approach I took is much simpler and probably much more idiomatic Go.</p>

<ul>
<li>Each connection has their own complete set of R, P and S, instead of sharing P across multiple connections.</li>
<li>Each R, P and S are their own goroutines.</li>
<li>Between R and P, and P and S are channels that carry MQTT messages.</li>
<li>R was using bufio.Reader to read from net.Conn, and S was using bufio.Writer to write to net.Conn.</li>
<li>sync.Pool was used to help reduce the amount of memory allocation required, thus reducing GC pressure</li>
</ul>

<p>This approach worked and I was able to write <a href="https://github.com/surge/mqtt/commit/1eeba02bb5b7f624fc82a0ca975444944c1ec662">enough code</a> to test it. However, the performance was hideoous. In a 1:1 (single publisher and single subscriber) configuration, it was doing about 22,000-25,000 MPS.</p>

<pre><code>$ go test -vv=3 -logtostderr -run=LotsOf -cpu=2 -v
=== RUN TestServiceLotsOfPublish0Messages-2
1000000 messages, 44297366818 ns, 44297.366818 ns/msg, 22574 msgs/sec
</code></pre>

<p>After profiling and looking at the <a href="/images/surgemq-mqtt-message-queue-750k-mps/2ndfailcpuprof.svg">CPU profile</a>, I realized there are a lot of memory copying (io.Copy and io.CopyN), as well as there are still quite a bit GC activities (scanblock). On the memory copying front, there&rsquo;s copying from net.Conn into bufio, then more copying from bufio to the MQTT messages internal buffer, then more copying from MQTT message internal buffers to the outgoing bufio, then to the net.Conn. So lots and lots of memory copying, not a good thing.</p>

<h4 id="toc_5">Buffered Network IO</h4>

<p>The buffered network IO is a good approach, however, there are two things I wished I had:</p>

<ol>
<li>bufio shifts bytes around by copying. For example, whenever it needs to fill the buffer, it copies all the remaining bytes to the front of the buffer, then fill the rest. That&rsquo;s a lot of copying!</li>
<li>I needed something I can access the bytes directly so I can remove majority of the memory copying.</li>
</ol>

<p>At this point I decided to try a new technique I learned while doing Go Learn Project #7 - <a href="/blog/ring-buffer-variable-length-low-latency-disruptor-style/">Ring Buffer</a>. The basic idea is that instead of using bufio to read and write to net.Conn, I will implement my own version of that.</p>

<p>The ring buffer will implement the interfaces ReadFrom(), WriteTo(), Read() and Write().</p>

<ul>
<li>The receiver will essentially copy data directly from net.Conn into the ring buffer (technially the ring buffer will ReadFrom() net.Conn and put the read bytes into the internal buffer).</li>
<li>The processor can &ldquo;peek&rdquo; a byte slice (no copying) from the ring buffer, process it, and then commit the bytes once processing is done.</li>
<li>If the message needs to be send to other subscribers, the bytes will then be copied into the subscriber&rsquo;s outgoing buffer.</li>
</ul>

<p>While this is not &ldquo;zero-copy&rdquo;, it seems good enough.</p>

<p>I started by implementing a lock-less ring buffer, and it worked quite well. But as mentioned in the ring buffer article, you really shouldn&rsquo;t use it unless there&rsquo;s plenty of CPU cores lying around. And also calling runtime.Gosched() thousands of times is really not healthy for the Go scheduler.</p>

<p>So keeping Lesson 1 in mind, I modified the ring buffer to use two sync.Cond (reader sync.Cond and writer sync.Cond) to block (cond.Wait()) when there&rsquo;s not enough bytes to read or when there&rsquo;s not enough space to write. And then unblock (cond.Broadcast()) when bytes are either read from it, or written to it.</p>

<p>This is a single producer/single consumer ring buffer and is not designed for multiples of anything. The original thought was that since each connection has their own set of R, P and S, there shouldn&rsquo;t really be a need for multiple writers or readers. It turns out I was wrong, at least on the writer front. We will explain this a bit later.</p>

<p>At the end, this turned out to be the winning combination. I was able to achieve 20X performance increase with this approach after some additional tweaking. Specifically, I tested several buffer block size (the amount of data to read from and write to net.Conn) including 1024, 2048, 4096 and 8192 bytes. The highest performing one is 8192 bytes.</p>

<p>I also experiemented with different buffer sizes, including 256KB, 512KB and 1024KB. 256KB turned out to be sufficient in that it&rsquo;s the smallest buffer size that doesn&rsquo;t reduce performance by alot, nor higher numbers will help inprove performance.</p>

<blockquote>
<p>Lesson 2: Reduce or remove memory copying.</p>
</blockquote>

<h4 id="toc_6">Final Architecture</h4>

<p>This the final architecture I ended up with and it&rsquo;s working very well. The cost of each client connection are:</p>

<ul>
<li>3 goroutines: R (receiver), P (processor) and S (sender)</li>
<li>2 ring buffers of 256K each</li>
</ul>

<p>There&rsquo;s very few memory copy operations going on, nor is there much memory allocation. So a good outcome overall.</p>

<p><img src="/images/surgemq-mqtt-message-queue-750k-mps/finalarch.png"></p>

<h3 id="toc_7">Race Conditions</h3>

<p>With the ring bufer implementation, I was able to achieve 400,000 MPS with a 1:1 configuration. This worked well until I started doing multiple publishers and subscribers. The first problem I ran into was the Processor hanging. <code>go test -race</code> also didn&rsquo;t show anything that could help me.</p>

<p>After running tests over and over again, with more and more glog.Debugf() statements, I tracked the problem to the Processor. It was waiting for space in the ring buffer to write the outgoing messages. I know that&rsquo;s not possible as I am blasting messages out to net.Conn as fast as I can, so there&rsquo;s no way that write space is not available.</p>

<p>After running even more tests, and with even more glog.Debugf() statements, I finally determined the problem to be the way I was using sync.Cond. (I wish I saved the debug output..sigh) In the following code block, I was waiting for the consumer position (cpos) to pass the point in which there will be enough data for writing (wrap).</p>

<pre><code>		this.pcond.L.Lock()
		for cpos = this.cseq.get(); wrap &gt; cpos; cpos = this.cseq.get() {
			this.pcond.Wait()
		}
		this.pcond.L.Unlock()
</code></pre>

<p>The steps are really quite simple:</p>

<ol>
<li>I lock the producer sync.Conn</li>
<li>I get the consumer position, compare it to wrap (position that I need cpos to pass to indicate there&rsquo;s enough write space)</li>
<li>If there&rsquo;s not enough space, I wait, otherwise I move on</li>
<li>I unlock the producer sync.Conn</li>
</ol>

<p>Then in the Sender goroutine, I read data from the ring buffer, write to net.Conn, update the consumer position, and call <code>pcond.Broadcast()</code> to unblock the above <code>pcond.Wait()</code>.</p>

<pre><code>		this.cseq.set(cpos + int64(n))
		this.pcond.Broadcast()
</code></pre>

<p>According to the <a href="http://golang.org/pkg/sync/#Cond.Broadcast">Go doc</a>,</p>

<blockquote>
<p>Broadcast wakes all goroutines waiting on c.</p>

<p>It is allowed but not required for the caller to hold c.L during the call.</p>
</blockquote>

<p>So what I have above should work perfectly fine. Except it doesn&rsquo;t. What happens is that I ran into a situation where <code>pcond.Broadcast()</code> was called after the <code>wrap &gt; cpos</code> check, but before <code>pcond.Wait()</code>. In these cases, the <code>wrap &gt; cpos</code> returned true, which means we need to go wait. But before <code>pcond.Wait()</code> was called, the Sender goroutine has updated cpos, and called <code>pcond.Broadcast()</code>. So when <code>pcond.Wait()</code> is called, there&rsquo;s nothing to wake it up, and thus it hangs forever.</p>

<p>On the Sender side, because there&rsquo;s no more data to read, it is also just waiting for more data. So both the Sender and Processor are now hung.</p>

<p>After I finally figured out the root cause, I realized that, unlike what the go doc suggested, the caller should really hold c.L during the call to Broadcast(). So I modified the code to the following:</p>

<pre><code>		this.cseq.set(cpos + int64(n))
		this.pcond.L.Lock()
		this.pcond.Broadcast()
		this.pcond.L.Unlock()
</code></pre>

<p>What this does is that it ensures I can never call <code>pcond.Broadcast()</code> after <code>pcond.L.Lock()</code> (in the Processor goroutine) is called but <code>pcond.Wait()</code> is not called. When <code>pcond.Wait()</code> is called, it actually calls <code>pcond.L.Unlock()</code> internally so it will allow <code>pcond.L.Lock()</code> in the Sender goroutine to be called.</p>

<p>In any case, we are finally on our way to working with multiple clients.</p>

<blockquote>
<p>Lesson 3: Race conditions can happen even if you think you followed all the right steps.</p>
</blockquote>

<h4 id="toc_8">But Wait, There&rsquo;s More (Race Conditions)</h4>

<p>As I increase the number of publishers and subscribers, all the sudden I was getting errors about receiving RESERVED messages, and this happens intermittenly, and only when I blast enough messages. Sometimes I have to run the tests many times to catch this from happening.</p>

<p>It turns out that while I was thinking I only had 1 Publisher per client connection that&rsquo;s writing to the outgoing buffer, I, in fact, had many. This happens when a client is sent a message to a topic that it subscribes to. In this case, the Processor of the publishing client calls the subscriber client&rsquo;s Publish() method, and writes the message to the outgoing ring buffer. At the same time, other publishing clients can be publishing other messages to the subscriber client. When this happens, they could overwrite eachother&rsquo;s message because the ring buffer is NOT designed for multiple writers.</p>

<p><code>go test -race</code> should technically find this race condition (I think). But given that this condition only happens intermittenly and sometimes it only happens when there&rsquo;s a large volume of messages, the race detector was taking too long and I was too impatient.</p>

<p>Regardless, after identifying the root cause, I added a Mutex to serialize the writes. At some point I may come back and rewrite it without the lock. But for now it&rsquo;s good enough.</p>

<blockquote>
<p>Lesson 4: Use the race detector!</p>
</blockquote>

<h3 id="toc_9">Performance Benchmarks</h3>

<p>These performance numbers are calculated as follows:</p>

<ul>
<li>sent messages MPS = total messages sent / total elapsed time between 1st and last message sent for all senders</li>
<li>received messages MPS = total messages received / total elapsed time between 1st and last message received for all receivers</li>
</ul>

<h4 id="toc_10">Environment</h4>

<pre><code>$ go version
go version go1.3.3 darwin/amd64

---

Macbook Pro Late 2013
2.8 GHz Intel Core i7
16 GB 1600 MHz DDR3
</code></pre>

<h4 id="toc_11">Server</h4>

<p>To start the server,</p>

<pre><code>$ cd benchmark
$ GOMAXPROCS=2 go test -run=TestServer -vv=2 -logtostderr
server/ListenAndServe: server is ready...
</code></pre>

<h4 id="toc_12">1:1</h4>

<p>To run the single publisher and single subscriber test case:</p>

<pre><code>$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 1 -receivers 1
Total Sent 1000000 messages in 2434523153 ns, 2434 ns/msg, 410758 msgs/sec
Total Received 1000000 messages in 2434523153 ns, 2434 ns/msg, 410758 msgs/sec
</code></pre>

<h4 id="toc_13">Fan-In</h4>

<p>To run the Fan-In test with 20 senders and 1 receiver:</p>

<pre><code>$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 20 -receivers 1
Total Sent 1035436 messages in 2212609304 ns, 2136 ns/msg, 467970 msgs/sec
Total Received 1000022 messages in 2212609304 ns, 2212 ns/msg, 451965 msgs/sec
</code></pre>

<h4 id="toc_14">Fan-Out</h4>

<p>To run the Fan-Out test with 1 sender and 20 receivers:</p>

<pre><code>$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 1 -receivers 20
Total Sent 1000000 messages in 10715317340 ns, 10715 ns/msg, 93324 msgs/sec
Total Received 8180723 messages in 10715317340 ns, 1309 ns/msg, 763460 msgs/sec
</code></pre>

<h4 id="toc_15">Mesh</h4>

<p>To run a full mesh test where every client is subscribed to the same topic, thus every message sent w/ the right topic will go to ALL of the other clients:</p>

<pre><code>$ GOMAXPROCS=2 go test -run=TestMesh -vv=2 -logtostderr -senders 20 -messages 100000
Total Sent 2000000 messages in 51385336097 ns, 25692 ns/msg, 38921 msgs/sec
Total Received 40000000 messages in 51420975243 ns, 1285 ns/msg, 777892 msgs/sec
</code></pre>

<h3 id="toc_16">Next Steps</h3>

<p>There&rsquo;s a lot more to do with SurgeMQ. Given the limited time I have, I expect it will take me a while to get to full compliant with the MQTT spec. But that will be my focus, now that performance is out of the way, as I get time.</p>

  </div>
  
</div>

<div class="posts">
  <h1 class="post-title">Archive</h1>
  <ul class="posts">
      
      <li><span><a href="http://localhost:1313/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/">PingMQ: A SurgeMQ-based ICMP Monitoring Tool</a> - <time class="pull-right post-list">Thu, Dec 25, 2014</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/">SurgeMQ: High Performance MQTT Server and Client Libraries in Go</a> - <time class="pull-right post-list">Wed, Dec 24, 2014</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/surgemq-mqtt-message-queue-750k-mps/">SurgeMQ: MQTT Message Queue @ 750,000 MPS</a> - <time class="pull-right post-list">Thu, Dec 4, 2014</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/graceful-shutdown-of-go-net-dot-listeners/">Graceful Shutdown of Go net.Listeners</a> - <time class="pull-right post-list">Thu, Dec 12, 2013</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/ring-buffer-variable-length-low-latency-disruptor-style/">Ring Buffer - Variable-Length, Low-Latency, Lock-Free, Disruptor-Style</a> - <time class="pull-right post-list">Sat, Nov 30, 2013</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/go-vs-java-decoding-billions-of-integers-per-second/">Go vs Java: Decoding Billions of Integers Per Second</a> - <time class="pull-right post-list">Thu, Nov 14, 2013</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/improving-cityhash-performance-by-go-profiling/">Improving Cityhash Performance by Go Profiling</a> - <time class="pull-right post-list">Sun, Nov 10, 2013</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/benchmarking-integer-compression-in-go/">Benchmarking Integer Compression in Go</a> - <time class="pull-right post-list">Fri, Oct 11, 2013</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/bitmap-compression-using-ewah-in-go/">Bitmap Compression using EWAH in Go</a> - <time class="pull-right post-list">Sun, Sep 15, 2013</h4></time></span></li>
      
      <li><span><a href="http://localhost:1313/blog/benchmarking-bloom-filters-and-hash-functions-in-go/">Benchmarking Bloom Filters and Hash Functions in Go</a> - <time class="pull-right post-list">Wed, Sep 4, 2013</h4></time></span></li>
      
  </ul>
</div>

</div>

  </body>

</html>
