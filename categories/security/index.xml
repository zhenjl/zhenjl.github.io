<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Security on Zen 3.1 </title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://zhen.org/categories/security/index.xml/</link>
    <language>en-us</language>
    <author>Jian Zhen</author>
    <copyright>Jian Zhen</copyright>
    <updated>Sun, 08 Feb 2015 19:57:56 PST</updated>
    
    <item>
      <title>Papers I Read: 2015 Week 6</title>
      <link>http://zhen.org/blog/papers-i-read-2015-week-6/</link>
      <pubDate>Sun, 08 Feb 2015 19:57:56 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/papers-i-read-2015-week-6/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://paperswelove.org/&#34;&gt;Papers We Love&lt;/a&gt; has been making rounds lately and a lot of people are excited about it. I also think it&amp;rsquo;s kind of cool since I&amp;rsquo;ve been reading a lot of research papers over the past year or so. I have been killing some trees because of that.&lt;/p&gt;

&lt;p&gt;My interests have been mostly around data analytics, but the specific focus areas have changed a few times. I have read papers on data structures (bloom filters, skiplist, bitmap compression, etc), security analytics, consumer behavioral analysis, loyalty analytics, and now back to security analytics. In fact, recently I started reading a few security research papers that I found on &lt;a href=&#34;http://www.covert.io/&#34;&gt;covert.io&lt;/a&gt;, put together by Jason Trost.&lt;/p&gt;

&lt;p&gt;In any case, I thought it might be an interesting idea to share some of the papers I read/scan/skim on weekly basis. This way I can also track what I read over time.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Random Ramblings&lt;/h3&gt;

&lt;p&gt;This week has been a disaster. I was the last one in the family to catch the cold, but probably lasted the longest. In fact I am still only about 50%. This whole week I have been having headaches, body aches, and haven&amp;rsquo;t been able to concentrate. My body must be trying to catch up on sleep or something. For a couple days I actually slept for almost 12 hours a night!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been meaning to work on &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; and finish updating the analyzer, but really had a hard time concentrating. Any non-working hours are basically spent in bed if I could.&lt;/p&gt;

&lt;p&gt;So this is probably the worst week to start the &amp;ldquo;Papers I Read&amp;rdquo; series since I only technically read 1 paper. But I am going to cheat a little, and list the papers I read over the past couple of weeks, pretty much all in my spare time.&lt;/p&gt;

&lt;p&gt;This week we also saw Sony&amp;rsquo;s accouncement that last year&amp;rsquo;s hack cost them &lt;a href=&#34;http://www.sony.net/SonyInfo/IR/financial/fr/150204_sony.pdf&#34;&gt;$15 million&lt;/a&gt; to investigate and remediate. It&amp;rsquo;s pretty crazy if you think about it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s assume that they hired a bunch of high-priced consultants, say $250/hour, to help comb through the logs and clean the systems. And let&amp;rsquo;s say &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; of the $15m is spent on these consultants. That&amp;rsquo;s &lt;code&gt;$10m / $250 = 40,000 hours&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say these consultants worked full time, non-stop, no weekends, no breaks, for 2 months since the announcement on Nov 24, 2014, that would be a team of 56 people (&lt;code&gt;40,000 hours / 60 days / 12 hours/day = 56&lt;/code&gt;) working 12 hour days!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll tell ya, these security guys are raking it in. They make money upfront by selling products/services to protect the company, then they make money in the back by selling forensic services to clean up after the hack.&lt;/p&gt;

&lt;p&gt;[Disclaimer: any mistake in my calculations/assumptions I blame on my drugged brain cells.]&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Papers I Read&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf&#34;&gt;Beehive: Large-Scale Log Analysis for Detecting Suspicious Activity in Enterprise Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://minds.cs.umn.edu/publications/chapter.pdf&#34;&gt;Data Mining for Cyber Security&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;This chapter provides an overview of the Minnesota Intrusion Detection System (MINDS), which uses a suite of data mining based algorithms to address different aspects of cyber security. The various components of MINDS such as the scan detector, anomaly detector and the profiling module detect different types of attacks
and intrusions on a computer network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/VAST-%20Network%20Visibility%20Across%20Space%20and%20Time.pdf&#34;&gt;VAST: Network Visibility Across Space and Time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Key operational networking tasks, such as troubleshooting and defending against attacks, greatly benefit from attaining views of network activity that are unified across space and time. This means that data from heterogeneous devices and systems is treated in a uniformfashion, and that analyzing past activity and detecting future instances follow the same procedures. Based on previous ideas that formulated principles for comprehensive
network visibility [AKP+08], we present the design and architecture of Visibility Across Space and Time (VAST), an intelligent database that serves as a single vantage point into the network. The system is based on a generic event model to handle network data from disparate sources and provides a query architecture that allows operators or remote applications to extract events matching a given condition. We implemented a proof-of-principle prototype that can archive and index events from a wide range of sources. Moreover, we conducted a preliminary performance evaluation to verify that our implementation works efficient and as expected.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/Finding%20The%20Needle-%20Suppression%20of%20False%20Alarms%20in%20Large%20Intrusion%20Detection%20Data%20Sets.pdf&#34;&gt;Finding The Needle: Suppression of False Alarms in Large Intrusion Detection Data Sets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Managed security service providers (MSSPs) must manage and monitor thousands of intrusion detection sensors.
The sensors often vary by manufacturer and software version, making the problem of creating generalized tools to separate true attacks from false positives particularly difficult. Often times it is useful from an operations perspective to know if a particular sensor is acting out of character. We propose a solution to this problem using anomaly detection techniques over the set of alarms produced by the sensors. Similar to the manner in which an anomaly based sensor detects deviations from normal user or system behavior, we establish the baseline
behavior of a sensor and detect deviations from this baseline. We show that departures from this profile by a sensor have a high probability of being artifacts of genuine attacks. We evaluate a set of time-based Markovian heuristics against a simple compression algorithm and show that we are able to detect the existence of all attacks which were manually identified by security personnel, drastically reduce the number of false positives, and identify attacks which were overlooked during manual evaluation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://user.informatik.uni-goettingen.de/~krieck/docs/2013a-aisec.pdf&#34;&gt;A Close Look on n-Grams in Intrusion Detection: Anomaly Detection vs. Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Detection methods based on n-gram models have been widely studied for the identification of attacks and malicious software. These methods usually build on one of two learning schemes: anomaly detection, where a model of normality is constructed from n-grams, or classification, where a discrimination between benign and malicious n-grams is learned. Although successful in many security domains, previous work falls short of explaining why a particular scheme is used and more importantly what renders one favorable over the other for a given type of data. In this paper we provide a close look on n-gram models for intrusion detection. We specifically study anomaly detection and classification using n-grams and develop criteria for data being used in one or the other
scheme. Furthermore, we apply these criteria in the scope of web intrusion detection and empirically validate their effectiveness with different learning-based detection methods for client-side and service-side attacks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.scalyr.com/2014/05/searching-20-gbsec-systems-engineering-before-algorithms/&#34;&gt;Searching 20 GB/sec: Systems Engineering Before Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ok, this is a blog post, not a research paper, but it&amp;rsquo;s somewhat interesting nonetheless.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This article describes how we met that challenge using an “old school”, brute-force approach, by eliminating layers and avoiding complex data structures. There are lessons here that you can apply to your own engineering challenges.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Papers I Read: 2015 Week 6</title>
      <link>http://zhen.org/blog/papers-i-read-2015-week-7/</link>
      <pubDate>Sun, 08 Feb 2015 19:57:56 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/papers-i-read-2015-week-7/</guid>
      <description>

&lt;h3 id=&#34;toc_0&#34;&gt;Random Ramblings&lt;/h3&gt;

&lt;p&gt;Athem hack&lt;/p&gt;

&lt;p&gt;Another great resource of computer science papers is Adrian Coyler&amp;rsquo;s &lt;a href=&#34;http://blog.acolyer.org/&#34;&gt;the morning paper&lt;/a&gt;. He selects and summarizes &amp;ldquo;an interesting/influential/important paper from the world of CS every weekday morning&amp;rdquo;.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Papers I Read&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.put.poznan.pl/dweiss/site/publications/download/fsacomp.pdf&#34;&gt;Smaller Representation of Finite State Automata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I read this paper when I was trying to figure out how to make the FSAs smaller for the &lt;a href=&#34;https://github.com/surge/xparse/tree/master/etld&#34;&gt;Effective TLD matcher&lt;/a&gt; I created. The FSM I generated is 212,294 lines long. That&amp;rsquo;s just absolutely crazy. This paper seems to present an interesting way of compressing them.&lt;/p&gt;

&lt;p&gt;I am not exactly sure if &lt;a href=&#34;https://godoc.org/golang.org/x/net/publicsuffix&#34;&gt;PublicSuffix&lt;/a&gt; uses a similar representation but it basically represents a FSA as an array of bytes, and then walk the bytes like a binary search tree. It&amp;rsquo;s interesting for sure.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This paper is a follow-up to Jan Daciuk’s experiments on space-efficient finite state automata representation that can be used directly for traversals in main memory [4]. We investigate several techniques of reducing the memory footprint of minimal automata, mainly exploiting the fact that transition labels and transition pointer offset values are not evenly distributed and so are suitable for compression. We achieve a size gain of around 20–30% compared to the original representation given in [4]. This result is comparable to the state-of-the-art dictionary compression techniques like the LZ-trie [12] method, but remains
memory and CPU efficient during construction.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Response to &#34;Assessing the Security Benefits of Cloud Computing&#34;</title>
      <link>http://zhen.org/blog/response-to-assessing-the-security-benefits-of-cloud-computing/</link>
      <pubDate>Thu, 28 Aug 2008 01:58:22 &#43;0000</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/response-to-assessing-the-security-benefits-of-cloud-computing/</guid>
      <description>

&lt;p&gt;Craig Balding from &lt;a href=&#34;http://cloudsecurity.org&#34;&gt;Cloud Security&lt;/a&gt; wrote an interesting piece on the &lt;a href=&#34;http://cloudsecurity.org/2008/07/21/assessing-the-security-benefits-of-cloud-computing/&#34;&gt;security benefits of cloud computing&lt;/a&gt; back in July (that I just now got to read.) Craig qualified the post as &lt;strong&gt;potential security benefits&lt;/strong&gt; of Cloud Computing.&lt;/p&gt;

&lt;p&gt;After reading through it, I felt compelled to respond, even though it&amp;rsquo;s a been over a month since the post is up. Craig mentioned he won&amp;rsquo;t talk about the &amp;ldquo;flip&amp;rdquo; side of these benefits in this post, so I figure I will do that. :)
I have only quoted the headers from Craig&amp;rsquo;s article so please refer to the &lt;a href=&#34;http://cloudsecurity.org/2008/07/21/assessing-the-security-benefits-of-cloud-computing/&#34;&gt;original article&lt;/a&gt; for all the details.&lt;/p&gt;

&lt;p&gt;Overall, Craig has made a good list of potential benefits. However, we really need to distinguish the benefits of virtualization vs cloud computing. Many of the benefits listed here are really benefits of virtualization and not cloud computing. When I read the title, I was hoping to read about how the cloud could be more secure than enterprise environments. I think this list has a mix of that, and how enterprise could use the cloud for some security use cases. That&amp;rsquo;s fine but mixing them together can be misleading.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;1. Centralised Data&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Reduced Data Leakage&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As Craig said, &amp;ldquo;this is the benefit I hear most from Cloud providers&amp;rdquo;. Unfortunately I have to disagree with Craig here. In my view, the cloud providers are dead wrong about this one. Many of the cloud providers talk about how laptops or backup tapes being stolen as the biggest threat to data leakage, and they are right about that. However, having enterprise data stored in the cloud doesn&amp;rsquo;t reduce these risks one bit. Travelers will continue to copy data to their laptops as they need to access them while on the road. Old habits die hard. Enterprises will continue to backup data to tapes because they can&amp;rsquo;t simply reply on cloud providers to backup their data. These will still happen no matter where the data is stored.&lt;/p&gt;

&lt;p&gt;In fact, there likely will be an increased chance of data leakage by using cloud computing because now the cloud providers will have to somehow backup their data (maybe on tape!!)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monitoring benefits&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Most enterprises, probably including the one Craig works for, have centralized file servers, content management systems, etc etc. However, we continue to see problems with data leakage. Having data stored in clouds is not all that different than storing on centralized corporate file servers. Centralized storage and monitoring is not an advantage for clouds. Enterprises had centralized storage/archiving solutions for years.&lt;/p&gt;

&lt;p&gt;In my opinion, cloud storage makes it even tougher to monitor data leakage. Think about the tools available to monitor enterprise file servers. Many of them monitors all types of access: read, write, via CIFS/NFS/etc, via local system. How do you do all of that in the cloud? Think S3, the only thing S3 provide you are http access logs. You have no way of knowing who else viewed your files if it&amp;rsquo;s done locally, for example.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;2. Incident Response / Forensics&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Forensic readiness&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To a certain extent this benefits is real. However, it&amp;rsquo;s not a cloud-only benefit. You get the same benefit by simply doing virtualization on your infrastructure. VMware allows you to easily clone an image so that you can perform whatever analysis is needed on the image instead of the original virtual machine. Same as Xen.&lt;/p&gt;

&lt;p&gt;However, think about the cases where forensics require physical hard disk scan in case the attacker has &amp;ldquo;rm&amp;rdquo; the &amp;ldquo;bad stuff&amp;rdquo; such as audit trails or root kit. You now have NO WAY of getting to that in a virtualized environment. Granted, this is probably an issue with any network/san attached storage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decrease evidence acquisition time&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Same as above, it&amp;rsquo;s not a cloud-exclusive benefit. It&amp;rsquo;s simply a benefit of virtualization. The only real benefit of the cloud, as mentioned by Craig, is not having to &amp;ldquo;find&amp;rdquo; storage. Though I would say that&amp;rsquo;s the least of your worries if there&amp;rsquo;s a real attack that happened.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eliminate or reduce service downtime&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First, if the server/VM is truly &amp;ldquo;0wn3d&amp;rdquo;, I am not sure you want to keep that system up and running. You may want to bring a good copy of the VM up and run that instead. (or just go back to a previous good snapshot.)&lt;/p&gt;

&lt;p&gt;Second, with the cloud, you don&amp;rsquo;t even have a CHOICE of using physical acquisition toolkit. So I am not so sure that&amp;rsquo;s a benefit. :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decrease evidence transfer time&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Again, not a real benefit of the cloud. First, bit-by-bit copies of the VM in the cloud still takes time just like if you would in the real world. Second, this benefit can also be realized as part of the internal VM infrastructure, not cloud-exclusive.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eliminate forensic image verification time&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ok, so this is a minor benefit, but not a security benefit of the cloud. It&amp;rsquo;s more about the performance and scalability of the cloud.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Decrease time to access protected documents&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Both this and the next benefit are really about the elasticity and scalability of the clouds and not security.&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;3. Password assurance testing (aka cracking)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Decrease password cracking time&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Same as above, this is about the benefits of elasticity and scalability, not security.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Keep cracking activities to dedicated machines&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Same as above, this is about the benefits of elasticity and scalability, not security.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;4. Logging&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;‘Unlimited’, pay per drink storage&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Improve log indexing and search&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Getting compliant with Extended logging&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ok, this is about the utility and scalability of the cloud. Not a cloud security benefit. It&amp;rsquo;s about using the cloud for security tasks.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;5. Improve the state of security software (performance)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Drive vendors to create more efficient security software&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I believe this is true for even software on dedicated machines. Not cloud-exclusive.&lt;/p&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;6. Secure builds&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Pre-hardened, change control builds&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This I agree with. Having pre-built images that are secure from the start is a HUGE benefit. Though it&amp;rsquo;s a benefit of virtualization and virtual machines, not cloud-exclusive.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reduce exposure through patching offline&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t understand this one. Once the VM is running in production, I can imagine taking that down to do patching. You would have to manage the patching process like any other machine, no?&lt;/p&gt;

&lt;p&gt;Now image templates can be updated with patches so if new machines are started, they are pre-patched.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Easier to test impact of security changes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Again I agree. However, it&amp;rsquo;s still the benefit of virtualization, not necessarily cloud-exclusive.&lt;/p&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;7. Security Testing&lt;/h3&gt;

&lt;p&gt;**Reduce cost of testing security: **&lt;/p&gt;

&lt;p&gt;Agreed. It&amp;rsquo;s a side benefit of economies of scale.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>