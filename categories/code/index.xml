<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Code on Zen 3.1 </title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://zhen.org/categories/code/index.xml/</link>
    <language>en-us</language>
    <author>Jian Zhen</author>
    <copyright>Jian Zhen</copyright>
    <updated>Mon, 05 Jan 2015 22:40:20 PST</updated>
    
    <item>
      <title>Sequence: A High Performance Sequential Semantic Log Analyzer and Parser</title>
      <link>http://zhen.org/blog/sequence-high-performance-sequential-semantic-log-analyzer-parser/</link>
      <pubDate>Mon, 05 Jan 2015 22:40:20 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/sequence-high-performance-sequential-semantic-log-analyzer-parser/</guid>
      <description>

&lt;p&gt;&lt;code&gt;sequence&lt;/code&gt; is a &lt;em&gt;high performance sequential semantic log message analyzer and parser&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It is &lt;em&gt;sequential&lt;/em&gt; because it goes through a log message sequentially and does not use regular expressions.&lt;/li&gt;
&lt;li&gt;It is &lt;em&gt;semantic&lt;/em&gt; because it tries to extract meaningful information out of the log messages and give them semantic indicators, e.g., src IPv4 or dst IPv4.&lt;/li&gt;
&lt;li&gt;It is an &lt;em&gt;analyzer&lt;/em&gt; because analyzes a large corpus of text-based log messages and try to determine the unique patterns that would represent all of them.&lt;/li&gt;
&lt;li&gt;It is a &lt;em&gt;parser&lt;/em&gt; because it will take a message and parses out the meaningful parts.&lt;/li&gt;
&lt;li&gt;It is &lt;em&gt;high performance&lt;/em&gt; because it can parse 100K+ messages per second without the need to separate parsing rules by log source type.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;sequence&lt;/code&gt; is currently under active development and should be considered unstable until further notice.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Motivation&lt;/h3&gt;

&lt;p&gt;Log messages are notoriusly difficult to parse because they all have different formats. Industries (see Splunk, ArcSight, Tibco LogLogic, Sumo Logic, Logentries, Loggly, LogRhythm, etc etc etc) have been built to solve the problems of parsing, understanding and analyzing log messages.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you have a bunch of log files you like to parse. The first problem you will typically run into is you have no way of telling how many DIFFERENT types of messages there are, so you have no idea how much work there will be to develop rules to parse all the messages. Not only that, you have hundreds of thousands, if not  millions of messages, in front of you, and you have no idea what messages are worth parsing, and what&amp;rsquo;s not.&lt;/p&gt;

&lt;p&gt;The typical workflow is develop a set of regular expressions and keeps testing against the logs until some magical moment where all the logs you want parsed are parsed. Ask anyone who does this for a living and they will tell you this process is long, frustrating and error-prone.&lt;/p&gt;

&lt;p&gt;Even after you have developed a set of regular expressions that match the original set of messages, if new messages come in, you will have to determine which of the new messages need to be parsed. And if you develop a new set of regular expressions to parse those new messages, you still have no idea if the regular expressions will conflict with the ones you wrote before. If you write your regex parsers too liberally, it can easily parse the wrong messages.&lt;/p&gt;

&lt;p&gt;After all that, you will end up finding out the regex parsers are quite slow. It can typically parse several thousands messages per second. Given enough CPU resources on a large enough machine, regex parsers can probably parse tens of thousands of messages per second. Even to achieve this type of performance, you will likely need to limit the number of regular expressions the parser has. The more regex rules, the slower the parser will go.&lt;/p&gt;

&lt;p&gt;To work around this performance issue, companies have tried to separate the regex rules for different log message types into different parsers. For example, they will have a parser for Cisco ASA logs, a parser for sshd logs, a parser for Apache logs, etc etc. And then they will require the users to tell them which parser to use (usually by indicating the log source type of the originating IP address or host.)&lt;/p&gt;

&lt;p&gt;Sequence is developed to make analyzing and parsing log messages a lot easier and faster.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro. The first file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go version
  go version go1.4 darwin/amd64

  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 2.65 secs, ~ 80449.93 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 4.42 secs, ~ 53081.36 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.52 secs, ~ 140139.27 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 2.51 secs, ~ 93614.09 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;License&lt;/h3&gt;

&lt;p&gt;Copyright &amp;copy; 2014 Dataence, LLC. All rights reserved.&lt;/p&gt;

&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &amp;ldquo;License&amp;rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;ldquo;AS IS&amp;rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Roadmap / Futures&lt;/h3&gt;

&lt;p&gt;There are some pattern files developed for ASA, Sudo and SSH in the &lt;code&gt;patterns&lt;/code&gt; directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages. So currently there&amp;rsquo;s not a set roadmap.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Concepts&lt;/h2&gt;

&lt;p&gt;The following concepts are part of the package:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;Token&lt;/em&gt; is a piece of information extracted from the original log message. It is a struct that contains fields for &lt;em&gt;TokenType&lt;/em&gt;, &lt;em&gt;FieldType&lt;/em&gt;, &lt;em&gt;Value&lt;/em&gt;, and indicators of whether it&amp;rsquo;s a key or value in the key=value pair.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;TokenType&lt;/em&gt; indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;FieldType&lt;/em&gt; indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Sequence&lt;/em&gt; is a list of Tokens. It is returned by the &lt;em&gt;Scanner&lt;/em&gt;, the &lt;em&gt;Analyzer&lt;/em&gt;, and the &lt;em&gt;Parser&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Scanner&lt;/em&gt; is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, IPv4 addresses, URLs, MAC addresses,
integers and floating point numbers. It also recgonizes key=value or key=&amp;ldquo;value&amp;rdquo; or key=&amp;lsquo;value&amp;rsquo; or key=&lt;value&gt; pairs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Analyzer&lt;/em&gt; builds an analysis tree that represents all the Sequences from messages. It can be used to determine all of the unique patterns for a large body of messages.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Parser&lt;/em&gt; is a tree-based parsing engine for log messages. It builds a parsing tree based on pattern sequence supplied, and for each message sequence, returns the matching pattern sequence. Each of the message tokens will be marked with the semantic field types.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Sequence Command&lt;/h2&gt;

&lt;p&gt;The typical workflow of using sequence is to first analyze all of the log messages to determine the unique patterns. This could easily reduce millions of log messages down to maybe 30-50 formats.&lt;/p&gt;

&lt;p&gt;Then the analyst can look through these formats and annotate the patterns with the semantic meanings. Once that&amp;rsquo;s done, the analyst can run the parser with these annotated rules and outcome the parsed tokens.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; command is developed to demonstrate the use of this package. You can find it in the cmd/sequence directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   Usage:
     sequence [command]

   Available Commands:
     scan                      scan will tokenize a log file or message and output a list of tokens
     analyze                   analyze will analyze a log file and output a list of patterns that will match all the log messages
     parse                     parse will parse a log file and output a list of parsed tokens for each of the log messages
     bench                     benchmark the parsing of a log file, no output is provided
     help [command]            Help about any command
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;Scan&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence scan [flags]

   Available Flags:
    -h, --help=false: help for scan
    -m, --msg=&amp;quot;&amp;quot;: message to tokenize
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence scan -m &amp;quot;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream&amp;quot;
  #   0: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 14 10:15:56&amp;quot; }
  #   1: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;testserver&amp;quot; }
  #   2: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sudo&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   4: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;gonner&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;tty&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pts/3&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;pwd&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/home/gonner&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  14: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  15: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;root&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  18: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;command&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/bin/su&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;-&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;ustream&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;Analyze&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence analyze [flags]

   Available Flags:
    -h, --help=false: help for analyze
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -o, --outfile=&amp;quot;&amp;quot;: output file, if empty, to stdout
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used, optional
    -p, --patfile=&amp;quot;&amp;quot;: initial pattern file, optional
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command analyzes a set of sshd log messages, and output the
patterns to the sshd.pat file. In this example, &lt;code&gt;sequence&lt;/code&gt; analyzed over 200K
messages and found 45 unique patterns. Notice we are not supplying an existing
pattern file, so it treats all the patters as new.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence analyze -i ../../data/sshd.all  -o sshd.pat
  Analyzed 212897 messages, found 45 unique patterns, 45 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the output file has entries such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  %ts% %string% sshd [ %integer% ] : %string% ( sshd : %string% ) : session %string% for user %string% by ( uid = %integer% )
  # Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the following command, we added an existing pattern file to the mix, which has
a set of existing rules. Notice now there are only 35 unique patterns, and we were
able to parse all of the log messages (no new patterns). There are fewer patterns
because some of the patterns were combined.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence analyze -d ../../patterns -i ../../data/sshd.all  -o sshd.pat
  Analyzed 212897 messages, found 35 unique patterns, 0 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The same log message we saw above now has an entry like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  %createtime% %apphost% %appname% [ %sessionid% ] : %string% ( sshd : %string% ) : %object% %action% for user %dstuser% by ( uid = %integer% )
  # Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_8&#34;&gt;Parse&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence parse [flags]

   Available Flags:
    -h, --help=false: help for parse
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -o, --outfile=&amp;quot;&amp;quot;: output file, if empty, to stdout
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&amp;quot;&amp;quot;: initial pattern file, required
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command parses a file based on existing rules. Note that the
performance number (9570.20 msgs/sec) is mostly due to reading/writing to disk.
To get a more realistic performance number, see the benchmark section below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence parse -d ../../patterns -i ../../data/sshd.all  -o parsed.sshd
  Parsed 212897 messages in 22.25 secs, ~ 9570.20 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is an entry from the output file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
  #   0: { Field=&amp;quot;%createtime%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 15 19:39:26&amp;quot; }
  #   1: { Field=&amp;quot;%apphost%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;jlz&amp;quot; }
  #   2: { Field=&amp;quot;%appname%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;sshd&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;[&amp;quot; }
  #   4: { Field=&amp;quot;%sessionid%&amp;quot;, Type=&amp;quot;%integer%&amp;quot;, Value=&amp;quot;7778&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;]&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pam_unix&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;(&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sshd&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;session&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;)&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #  14: { Field=&amp;quot;%object%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;session&amp;quot; }
  #  15: { Field=&amp;quot;%action%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;opened&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;for&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  18: { Field=&amp;quot;%dstuser%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;jlz&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;by&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;(&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;uid&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  23: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%integer%&amp;quot;, Value=&amp;quot;0&amp;quot; }
  #  24: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;)&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_9&#34;&gt;Benchmark&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence bench [flags]

   Available Flags:
    -c, --cpuprofile=&amp;quot;&amp;quot;: CPU profile filename
    -h, --help=false: help for bench
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&amp;quot;&amp;quot;: pattern file, required
    -w, --workers=1: number of parsing workers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will benchmark the parsing of two files. First file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA
log file, averaging 180 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 2.65 secs, ~ 80449.93 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 4.42 secs, ~ 53081.36 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.52 secs, ~ 140139.27 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 2.51 secs, ~ 93614.09 msgs/sec
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>PingMQ: A SurgeMQ-based ICMP Monitoring Tool</title>
      <link>http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/</link>
      <pubDate>Thu, 25 Dec 2014 00:00:33 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/pingmq-a-surgemq-based-icmp-monitoring-tool/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/surge/surgemq/tree/master/cmd/pingmq&#34;&gt;pingmq&lt;/a&gt; is developed to demonstrate the different use cases one can use &lt;a href=&#34;//surgemq.com&#34;&gt;SurgeMQ&lt;/a&gt;, a high performance MQTT server and client library. In this simplified use case, a network administrator can setup server uptime monitoring system by periodically sending ICMP ECHO_REQUEST to all the IPs in their network, and send the results to SurgeMQ.&lt;/p&gt;

&lt;p&gt;Then multiple clients can subscribe to results based on their different needs. For example, a client maybe only interested in any failed ping attempts, as that would indicate a host might be down. After a certain number of failures the client may then raise some type of flag to indicate host down.&lt;/p&gt;

&lt;p&gt;There are three benefits of using SurgeMQ for this use case.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;First, with all the different monitoring tools out there that wants to know if hosts are up or down, they can all now subscribe to a single source of information. They no longer need to write their own uptime tools.&lt;/li&gt;
&lt;li&gt;Second, assuming there are 5 monitoring tools on the network that wants to ping each and every host, the small packets are going to congest the network. The company can save 80% on their uptime monitoring bandwidth by having a single tool that pings the hosts, and have the rest subscribe to the results.&lt;/li&gt;
&lt;li&gt;Third/last, the company can enhance their security posture by placing tighter restrictions on their firewalls if there&amp;rsquo;s only a single host that can send ICMP ECHO_REQUESTS to all other hosts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following commands will run pingmq as a server, pinging the 8.8.8.0/28 CIDR block, and publishing the results to /ping/success/{ip} and /ping/failure/{ip} topics every 30 seconds. &lt;code&gt;sudo&lt;/code&gt; is needed because we are using RAW sockets and that requires root privilege.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go build
$ sudo ./pingmq server -p 8.8.8.0/28 -i 30
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./pingmq client -t /ping/failure/+
8.8.8.6: Request timed out for seq 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will run pingmq as a client, subscribing to /ping/failure/+ topic and receiving any failed ping attempts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./pingmq client -t /ping/success/+
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One can also subscribe to a specific IP by using the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./pingmq client -t /ping/+/8.8.8.8
8 bytes from 8.8.8.8: seq=1 ttl=56 tos=32 time=21.753711ms
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Commands&lt;/h3&gt;

&lt;p&gt;There are two builtin commands for &lt;code&gt;pingmq&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;pingmq server&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage:
  pingmq server [flags]

 Available Flags:
  -h, --help=false: help for server
  -i, --interval=60: ping interval in seconds
  -p, --ping=[]: Comma separated list of IPv4 addresses to ping
  -q, --quiet=false: print out ping results
  -u, --uri=&amp;quot;tcp://:5836&amp;quot;: URI to run the server on
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;pingmq client&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Usage:
  pingmq client [flags]

 Available Flags:
  -h, --help=false: help for client
  -s, --server=&amp;quot;tcp://127.0.0.1:5836&amp;quot;: PingMQ server to connect to
  -t, --topic=[]: Comma separated list of topics to subscribe to
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;IP Addresses&lt;/h3&gt;

&lt;p&gt;To list IPs you like to use with &lt;code&gt;pingmq&lt;/code&gt;, you can use the following formats:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;10.1.1.1      -&amp;gt; 10.1.1.1
10.1.1.1,2    -&amp;gt; 10.1.1.1, 10.1.1.2
10.1.1,2.1    -&amp;gt; 10.1.1.1, 10.1.2.1
10.1.1,2.1,2  -&amp;gt; 10.1.1.1, 10.1.1.2 10.1.2.1, 10.1.2.2
10.1.1.1-2    -&amp;gt; 10.1.1.1, 10.1.1.2
10.1.1.-2     -&amp;gt; 10.1.1.0, 10.1.1.1, 10.1.1.2
10.1.1.1-10   -&amp;gt; 10.1.1.1, 10.1.1.2 ... 10.1.1.10
10.1.1.1-     -&amp;gt; 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-3.1    -&amp;gt; 10.1.1.1, 10.1.2.1, 10.1.3.1
10.1-3.1-3.1  -&amp;gt; 10.1.1.1, 10.1.2.1, 10.1.3.1, 10.2.1.1, 10.2.2.1, 10.2.3.1, 10.3.1.1, 10.3.2.1, 10.3.3.1
10.1.1        -&amp;gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.254, 10.1.1.255
10.1.1-2      -&amp;gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1-2        -&amp;gt; 10.1.0.0, 10.1.0,1 ... 10.2.255.254, 10..2.255.255
10            -&amp;gt; 10.0.0.0 ... 10.255.255.255
10.1.1.2,3,4  -&amp;gt; 10.1.1.1, 10.1.1.2, 10.1.1.3, 10.1.1.4
10.1.1,2      -&amp;gt; 10.1.1.0, 10.1.1.1 ... 10.1.1.255, 10.1.2.0, 10.1.2.1 ... 10.1.2.255
10.1.1/28     -&amp;gt; 10.1.1.0 ... 10.1.1.255
10.1.1.0/28   -&amp;gt; 10.1.1.0 ... 10.1.1.15
10.1.1.0/30   -&amp;gt; 10.1.1.0, 10.1.1.1, 10.1.1.2, 10.1.1.3
10.1.1.128/25 -&amp;gt; 10.1.1.128 ... 10.1.1.255
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Topic Format&lt;/h3&gt;

&lt;p&gt;TO subscribe to the &lt;code&gt;pingmq&lt;/code&gt; results, you can use the following formats:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/ping/#&lt;/code&gt; will subscribe to both success and failed pings for all IP addresses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping/success/+&lt;/code&gt; will subscribe to success pings for all IP addresses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping/failure/+&lt;/code&gt; will subscribe to failed pings for all IP addresses&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ping/+/8.8.8.8&lt;/code&gt; will subscribe to both success and failed pings for all IP 8.8.8.8&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Building&lt;/h3&gt;

&lt;p&gt;To build &lt;code&gt;pingmq&lt;/code&gt;, you need to have installed &lt;a href=&#34;http://golang.org&#34;&gt;Go 1.3.3 or 1.4&lt;/a&gt;. Then run the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# go get github.com/surge/surgemq
# cd surgemq/examples/pingmq
# go build
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, you should see the &lt;code&gt;pingmq&lt;/code&gt; command in the pingmq directory.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SurgeMQ: High Performance MQTT Server and Client Libraries in Go</title>
      <link>http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/</link>
      <pubDate>Wed, 24 Dec 2014 19:20:40 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/surgemq-high-performance-mqtt-server-and-client-libraries-in-go/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Happy Holidays!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This is more of an announcement post as SurgeMQ is now compatibility-tested with some of the popular MQTT clients out there, and it&amp;rsquo;s reaching &lt;em&gt;playable&lt;/em&gt; state.&lt;/p&gt;

&lt;p&gt;For completeness sake, please bear with some of the duplicate content in this post. The &lt;a href=&#34;//blog/surgemq-mqtt-message-queue-750k-mps/&#34;&gt;last post&lt;/a&gt; made front page of &lt;a href=&#34;https://news.ycombinator.com/item?id=8708921&#34;&gt;Hacker News&lt;/a&gt; and generated some great comments and discussions.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;SurgeMQ is a high performance MQTT broker and client library that aims to be fully compliant with MQTT 3.1 and 3.1.1 specs. The primary package that&amp;rsquo;s of interest is package &lt;a href=&#34;http://godoc.org/github.com/surge/surgemq/service&#34;&gt;service&lt;/a&gt;. It provides the MQTT Server and Client services in a library form.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;SurgeMQ is currently under active development and should be considered unstable until further notice.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;MQTT&lt;/h3&gt;

&lt;p&gt;According to the &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT spec&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.&lt;/p&gt;

&lt;p&gt;The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications.&lt;/li&gt;
&lt;li&gt;A messaging transport that is agnostic to the content of the payload.&lt;/li&gt;
&lt;li&gt;Three qualities of service for message delivery:

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;At most once&amp;rdquo;, where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;At least once&amp;rdquo;, where messages are assured to arrive but duplicates can occur.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Exactly once&amp;rdquo;, where message are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A small transport overhead and protocol exchanges minimized to reduce network traffic.&lt;/li&gt;
&lt;li&gt;A mechanism to notify interested parties when an abnormal disconnection occurs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;There&amp;rsquo;s some very large implementation of MQTT such as &lt;a href=&#34;https://www.facebook.com/notes/facebook-engineering/building-facebook-messenger/10150259350998920&#34;&gt;Facebook Messenger&lt;/a&gt;. There&amp;rsquo;s also an active Eclipse project, &lt;a href=&#34;https://eclipse.org/paho/&#34;&gt;Paho&lt;/a&gt;, that provides scalable open-source client implementations for many different languages, including C/C++, Java, Python, JavaScript, C# .Net and Go.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Features, Limitations, and Future&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Supports QOS 0, 1 and 2 messages&lt;/li&gt;
&lt;li&gt;Supports will messages&lt;/li&gt;
&lt;li&gt;Supports retained messages (add/remove)&lt;/li&gt;
&lt;li&gt;Pretty much everything in the spec except for the list below&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All features supported are in memory only. Once the server restarts everything is cleared.

&lt;ul&gt;
&lt;li&gt;However, all the components are written to be pluggable so one can write plugins based on the Go interfaces defined.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Message redelivery on reconnect is not currently supported.&lt;/li&gt;
&lt;li&gt;Message offline queueing on disconnect is not supported. Though this is also not a specific requirement for MQTT.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Future&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Message re-delivery (DUP)&lt;/li&gt;
&lt;li&gt;$SYS topics&lt;/li&gt;
&lt;li&gt;Server bridge&lt;/li&gt;
&lt;li&gt;Ack timeout/retry&lt;/li&gt;
&lt;li&gt;Session persistence&lt;/li&gt;
&lt;li&gt;Better authentication modules&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;Current performance benchmark of SurgeMQ, running all publishers, subscribers and broker on a single 4-core (2.8Ghz i7) MacBook Pro, is able to achieve:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;over &lt;strong&gt;400,000 MPS&lt;/strong&gt; in a 1:1 single publisher and single producer configuration&lt;/li&gt;
&lt;li&gt;over &lt;strong&gt;450,000 MPS&lt;/strong&gt; in a 20:1 fan-in configuration&lt;/li&gt;
&lt;li&gt;over &lt;strong&gt;750,000 MPS&lt;/strong&gt; in a 1:20 fan-out configuration&lt;/li&gt;
&lt;li&gt;over &lt;strong&gt;700,000 MPS&lt;/strong&gt; in a full mesh configuration with 20 clients&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Compatibility&lt;/h3&gt;

&lt;p&gt;In addition, SurgeMQ has been tested with the following client libraries and it &lt;em&gt;seems&lt;/em&gt; to work:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;libmosquitto 1.3.5 (in C).&lt;/em&gt; Tested with the bundled test programs msgsps_pub and msgsps_sub&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paho MQTT Conformance/Interoperability Testing Suite (in Python).&lt;/em&gt; Tested with all 10 test cases, 3 did not pass. They are

&lt;ol&gt;
&lt;li&gt;&amp;ldquo;offline messages queueing test&amp;rdquo; which is not supported by SurgeMQ&lt;/li&gt;
&lt;li&gt;&amp;ldquo;redelivery on reconnect test&amp;rdquo; which is not yet implemented by SurgeMQ&lt;/li&gt;
&lt;li&gt;&amp;ldquo;run subscribe failure test&amp;rdquo; which is not a valid test&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paho Go Client Library (in Go).&lt;/em&gt; Tested with one of the tests in the library, in fact, that tests is now part of the tests for SurgeMQ.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paho C Client library (in C).&lt;/em&gt; Tested with most of the test cases and failed the same ones as the conformance test because the features are not yet implemented. Actually I think there&amp;rsquo;s a bug in the test suite as it calls the PUBLISH handler function for non-PUBLISH messages.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Documentation&lt;/h3&gt;

&lt;p&gt;Documentation is available at &lt;a href=&#34;http://godoc.org/github.com/surge/surgemq&#34;&gt;godoc&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;More information regarding the design of the SurgeMQ is available at &lt;a href=&#34;http://surgemq.com&#34;&gt;zen 3.1&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;License&lt;/h3&gt;

&lt;p&gt;Copyright &amp;copy; 2014 Dataence, LLC. All rights reserved.&lt;/p&gt;

&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &amp;ldquo;License&amp;rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;ldquo;AS IS&amp;rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.&lt;/p&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;Examples&lt;/h3&gt;

&lt;h4 id=&#34;toc_7&#34;&gt;Server Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;// Create a new server
svr := &amp;amp;service.Server{
    KeepAlive:        300,               // seconds
    ConnectTimeout:   2,                 // seconds
    SessionsProvider: &amp;quot;mem&amp;quot;,             // keeps sessions in memory
    Authenticator:    &amp;quot;mockSuccess&amp;quot;,     // always succeed
    TopicsProvider:   &amp;quot;mem&amp;quot;,             // keeps topic subscriptions in memory
}

// Listen and serve connections at localhost:1883
svr.ListenAndServe(&amp;quot;tcp://:1883&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_8&#34;&gt;Client Example&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;// Instantiates a new Client
c := &amp;amp;Client{}

// Creates a new MQTT CONNECT message and sets the proper parameters
msg := message.NewConnectMessage()
msg.SetWillQos(1)
msg.SetVersion(4)
msg.SetCleanSession(true)
msg.SetClientId([]byte(&amp;quot;surgemq&amp;quot;))
msg.SetKeepAlive(10)
msg.SetWillTopic([]byte(&amp;quot;will&amp;quot;))
msg.SetWillMessage([]byte(&amp;quot;send me home&amp;quot;))
msg.SetUsername([]byte(&amp;quot;surgemq&amp;quot;))
msg.SetPassword([]byte(&amp;quot;verysecret&amp;quot;))

// Connects to the remote server at 127.0.0.1 port 1883
c.Connect(&amp;quot;tcp://127.0.0.1:1883&amp;quot;, msg)

// Creates a new SUBSCRIBE message to subscribe to topic &amp;quot;abc&amp;quot;
submsg := message.NewSubscribeMessage()
submsg.AddTopic([]byte(&amp;quot;abc&amp;quot;), 0)

// Subscribes to the topic by sending the message. The first nil in the function
// call is a OnCompleteFunc that should handle the SUBACK message from the server.
// Nil means we are ignoring the SUBACK messages. The second nil should be a
// OnPublishFunc that handles any messages send to the client because of this
// subscription. Nil means we are ignoring any PUBLISH messages for this topic.
c.Subscribe(submsg, nil, nil)

// Creates a new PUBLISH message with the appropriate contents for publishing
pubmsg := message.NewPublishMessage()
pubmsg.SetPacketId(pktid)
pubmsg.SetTopic([]byte(&amp;quot;abc&amp;quot;))
pubmsg.SetPayload(make([]byte, 1024))
pubmsg.SetQoS(qos)

// Publishes to the server by sending the message
c.Publish(pubmsg, nil)

// Disconnects from the server
c.Disconnect()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>SurgeMQ: MQTT Message Queue @ 750,000 MPS</title>
      <link>http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/</link>
      <pubDate>Thu, 04 Dec 2014 22:44:07 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/surgemq-mqtt-message-queue-750k-mps/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;Wow, this made front page of &lt;a href=&#34;https://news.ycombinator.com/item?id=8708921&#34;&gt;Hacker News&lt;/a&gt;! First for me!&lt;/li&gt;
&lt;li&gt;jacques_chester on HN has an &lt;a href=&#34;https://news.ycombinator.com/item?id=8709146&#34;&gt;EXCELLENT comment&lt;/a&gt; that&amp;rsquo;s definitely worth reading. &lt;a href=&#34;https://news.ycombinator.com/item?id=8709557&#34;&gt;My response&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt; aims to provide a MQTT broker and client library that&amp;rsquo;s fully compliant with &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT spec 3.1.1&lt;/a&gt;. In addition, it tries to be backward compatible with 3.1.&lt;/li&gt;
&lt;li&gt;SurgeMQ is under active development and should be considered unstable. Some of the key MQTT requirements, such as retained messages, still need to be added. The eventual goal is to pass the &lt;a href=&#34;https://eclipse.org/paho/clients/testing/&#34;&gt;MQTT Conformance/Interoperability Testing&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Having said that, running all publishers, subscribers and broker on a single 4-core (2.8Ghz i7) MacBook Pro, SurgeMQ is able to achieve

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;over 400,000&lt;/strong&gt; MPS in a 1:1 single publisher and single producer configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;over 450,000&lt;/strong&gt; MPS in a 20:1 fan-in configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;over 750,000&lt;/strong&gt; MPS in a 1:20 fan-out configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;over 700,000&lt;/strong&gt; MPS in a full mesh configuration with 20 clients&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;In developing SurgeMQ, I improved the performance 15-20X by keeping it simple and serial (KISS), reducing garbage collector pressure, reducing memory copy, and eliminating anything that could potentially introduce latency.&lt;/li&gt;
&lt;li&gt;There are still many areas that can be improved and I look forward to hearing any suggestions you may have.&lt;/li&gt;
&lt;li&gt;I cannot say this enough: &lt;strong&gt;benchmark, profile, optimize, rinse, repeat&lt;/strong&gt;. Go has made testing, benchmarking, and profiling extremely simple. You owe it to yourself to optimize your code using these tools.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 1: Don&amp;rsquo;t be clever. Keep It Simple and Serial (KISS).&lt;/p&gt;

&lt;p&gt;Lesson 2: Reduce or remove memory copying.&lt;/p&gt;

&lt;p&gt;Lesson 3: Race conditions can happen even if you think you followed all the right steps.&lt;/p&gt;

&lt;p&gt;Lesson 4: Use the race detector!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Go Learn Project #8 - Message Queue&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s now been over a year since my last post! Family and work have occupied pretty much all of my time so spare time to learn Go was hard to come by.&lt;/p&gt;

&lt;p&gt;However, I was able to squeeze in an implementation of a &lt;a href=&#34;https://github.com/surge/mqtt&#34;&gt;MQTT encoder/decoder&lt;/a&gt; library in July. The implementation is now outdated and is no longer maintained, but it allowed me to learn about the &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT protocol&lt;/a&gt; and got me thinking about potentially implmenting a broker.&lt;/p&gt;

&lt;p&gt;Now months later, I am finally able spend a few weekends and nights developing &lt;a href=&#34;https://github.com/surge/surgemq&#34;&gt;SurgeMQ&lt;/a&gt;, a (soon to be) full MQTT 3.1.1 compliant message broker.&lt;/p&gt;

&lt;h4 id=&#34;toc_2&#34;&gt;Message Queues&lt;/h4&gt;

&lt;p&gt;According to &lt;a href=&#34;http://en.wikipedia.org/wiki/Message_queue&#34;&gt;Wikipedia&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Message queues provide an asynchronous communications protocol, meaning that the sender and receiver of the message do not need to interact with the message queue at the same time. Messages placed onto the queue are stored until the recipient retrieves them. Message queues have implicit or explicit limits on the size of data that may be transmitted in a single message and the number of messages that may remain outstanding on the queue.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tyler Treat of &lt;a href=&#34;http://www.bravenewgeek.com&#34;&gt;Brave New Geek&lt;/a&gt; also wrote a &lt;a href=&#34;http://www.bravenewgeek.com/tag/message-queues/&#34;&gt;good series on message queues&lt;/a&gt; that went over several of the key MQ implementations. One specific post, &lt;a href=&#34;http://www.bravenewgeek.com/dissecting-message-queues/&#34;&gt;Dissecting Message Queues&lt;/a&gt;, is especially interesting because it benchmarks some of the major message queue implmentations out there, both brokered and brokerless.&lt;/p&gt;

&lt;p&gt;In that post, Tyler found that borkerless queues had the highest throughput, achieving millions of MPS sent and received. Brokered message queue performances ranged from 12,000 MPS (&lt;a href=&#34;nsq.io&#34;&gt;NSQ&lt;/a&gt;) to 195,000 MPS (&lt;a href=&#34;nats.io&#34;&gt;Gnatsd&lt;/a&gt;). While the post showed that the Gnatsd latency to be around 300+ microseconds, in reality it&amp;rsquo;s probably more like the NSQ in terms of latency due to the sender sleeping whenever Gnatsd is 10+ messages behind. Regardless, hats off to Tyler. Great job!&lt;/p&gt;

&lt;h4 id=&#34;toc_3&#34;&gt;MQTT&lt;/h4&gt;

&lt;p&gt;I got interested in MQTT because &amp;ldquo;&lt;a href=&#34;http://mqtt.org&#34;&gt;MQTT&lt;/a&gt; is a machine-to-machine (M2M)/&amp;ldquo;Internet of Things&amp;rdquo; connectivity protocol. It was designed as an extremely lightweight publish/subscribe messaging transport.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;According to the &lt;a href=&#34;http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html&#34;&gt;MQTT spec&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;MQTT is a Client Server publish/subscribe messaging transport protocol. It is light weight, open, simple, and designed so as to be easy to implement. These characteristics make it ideal for use in many situations, including constrained environments such as for communication in Machine to Machine (M2M) and Internet of Things (IoT) contexts where a small code footprint is required and/or network bandwidth is at a premium.&lt;/p&gt;

&lt;p&gt;The protocol runs over TCP/IP, or over other network protocols that provide ordered, lossless, bi-directional connections. Its features include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use of the publish/subscribe message pattern which provides one-to-many message distribution and decoupling of applications.&lt;/li&gt;
&lt;li&gt;A messaging transport that is agnostic to the content of the payload.&lt;/li&gt;
&lt;li&gt;Three qualities of service for message delivery:

&lt;ul&gt;
&lt;li&gt;&amp;ldquo;At most once&amp;rdquo;, where messages are delivered according to the best efforts of the operating environment. Message loss can occur. This level could be used, for example, with ambient sensor data where it does not matter if an individual reading is lost as the next one will be published soon after.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;At least once&amp;rdquo;, where messages are assured to arrive but duplicates can occur.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Exactly once&amp;rdquo;, where message are assured to arrive exactly once. This level could be used, for example, with billing systems where duplicate or lost messages could lead to incorrect charges being applied.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;A small transport overhead and protocol exchanges minimized to reduce network traffic.&lt;/li&gt;
&lt;li&gt;A mechanism to notify interested parties when an abnormal disconnection occurs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;There&amp;rsquo;s some very large implementation of MQTT such as &lt;a href=&#34;https://www.facebook.com/notes/facebook-engineering/building-facebook-messenger/10150259350998920&#34;&gt;Facebook Messenger&lt;/a&gt;. There&amp;rsquo;s also an active Eclipse project, &lt;a href=&#34;https://eclipse.org/paho/&#34;&gt;Paho&lt;/a&gt;, that provides scalable open-source client implementations for many different languages, including C/C++, Java, Python, JavaScript, C# .Net and Go.&lt;/p&gt;

&lt;p&gt;Given the popularity, I decided to implement a MQTT broker in order to learn about message queues.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/images/surgemq-mqtt-message-queue-750k-mps/smqfailedarch.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The above image showed a couple of the architecture approaches I attempted. In them, R is the receiver, which reads from net.Conn, P is the processor, which processes the messages and determines what to do or where to send them, and S is the sender, which sends any messages out to net.Conn. Each R, P, and S are their own goroutines.&lt;/p&gt;

&lt;p&gt;I started the project wanting to be clever, and wanted to dynamically scale up/down a shared pool of processors as the number of messages increase/decrease. As I thought through it, it just got more and more complicated with the logic and coordination. At the end, before I even wrote much of the code, I scraped the idea.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 1: Don&amp;rsquo;t be clever. Keep It Simple and Serial (KISS).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The second architecture approach I took is much simpler and probably much more idiomatic Go.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each connection has their own complete set of R, P and S, instead of sharing P across multiple connections.&lt;/li&gt;
&lt;li&gt;Each R, P and S are their own goroutines.&lt;/li&gt;
&lt;li&gt;Between R and P, and P and S are channels that carry MQTT messages.&lt;/li&gt;
&lt;li&gt;R was using bufio.Reader to read from net.Conn, and S was using bufio.Writer to write to net.Conn.&lt;/li&gt;
&lt;li&gt;sync.Pool was used to help reduce the amount of memory allocation required, thus reducing GC pressure&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This approach worked and I was able to write &lt;a href=&#34;https://github.com/surge/mqtt/commit/1eeba02bb5b7f624fc82a0ca975444944c1ec662&#34;&gt;enough code&lt;/a&gt; to test it. However, the performance was hideoous. In a 1:1 (single publisher and single subscriber) configuration, it was doing about 22,000-25,000 MPS.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go test -vv=3 -logtostderr -run=LotsOf -cpu=2 -v
=== RUN TestServiceLotsOfPublish0Messages-2
1000000 messages, 44297366818 ns, 44297.366818 ns/msg, 22574 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After profiling and looking at the &lt;a href=&#34;/images/surgemq-mqtt-message-queue-750k-mps/2ndfailcpuprof.svg&#34;&gt;CPU profile&lt;/a&gt;, I realized there are a lot of memory copying (io.Copy and io.CopyN), as well as there are still quite a bit GC activities (scanblock). On the memory copying front, there&amp;rsquo;s copying from net.Conn into bufio, then more copying from bufio to the MQTT messages internal buffer, then more copying from MQTT message internal buffers to the outgoing bufio, then to the net.Conn. So lots and lots of memory copying, not a good thing.&lt;/p&gt;

&lt;h4 id=&#34;toc_5&#34;&gt;Buffered Network IO&lt;/h4&gt;

&lt;p&gt;The buffered network IO is a good approach, however, there are two things I wished I had:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;bufio shifts bytes around by copying. For example, whenever it needs to fill the buffer, it copies all the remaining bytes to the front of the buffer, then fill the rest. That&amp;rsquo;s a lot of copying!&lt;/li&gt;
&lt;li&gt;I needed something I can access the bytes directly so I can remove majority of the memory copying.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At this point I decided to try a new technique I learned while doing Go Learn Project #7 - &lt;a href=&#34;/blog/ring-buffer-variable-length-low-latency-disruptor-style/&#34;&gt;Ring Buffer&lt;/a&gt;. The basic idea is that instead of using bufio to read and write to net.Conn, I will implement my own version of that.&lt;/p&gt;

&lt;p&gt;The ring buffer will implement the interfaces ReadFrom(), WriteTo(), Read() and Write().&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The receiver will essentially copy data directly from net.Conn into the ring buffer (technially the ring buffer will ReadFrom() net.Conn and put the read bytes into the internal buffer).&lt;/li&gt;
&lt;li&gt;The processor can &amp;ldquo;peek&amp;rdquo; a byte slice (no copying) from the ring buffer, process it, and then commit the bytes once processing is done.&lt;/li&gt;
&lt;li&gt;If the message needs to be send to other subscribers, the bytes will then be copied into the subscriber&amp;rsquo;s outgoing buffer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While this is not &amp;ldquo;zero-copy&amp;rdquo;, it seems good enough.&lt;/p&gt;

&lt;p&gt;I started by implementing a lock-less ring buffer, and it worked quite well. But as mentioned in the ring buffer article, you really shouldn&amp;rsquo;t use it unless there&amp;rsquo;s plenty of CPU cores lying around. And also calling runtime.Gosched() thousands of times is really not healthy for the Go scheduler.&lt;/p&gt;

&lt;p&gt;So keeping Lesson 1 in mind, I modified the ring buffer to use two sync.Cond (reader sync.Cond and writer sync.Cond) to block (cond.Wait()) when there&amp;rsquo;s not enough bytes to read or when there&amp;rsquo;s not enough space to write. And then unblock (cond.Broadcast()) when bytes are either read from it, or written to it.&lt;/p&gt;

&lt;p&gt;This is a single producer/single consumer ring buffer and is not designed for multiples of anything. The original thought was that since each connection has their own set of R, P and S, there shouldn&amp;rsquo;t really be a need for multiple writers or readers. It turns out I was wrong, at least on the writer front. We will explain this a bit later.&lt;/p&gt;

&lt;p&gt;At the end, this turned out to be the winning combination. I was able to achieve 20X performance increase with this approach after some additional tweaking. Specifically, I tested several buffer block size (the amount of data to read from and write to net.Conn) including 1024, 2048, 4096 and 8192 bytes. The highest performing one is 8192 bytes.&lt;/p&gt;

&lt;p&gt;I also experiemented with different buffer sizes, including 256KB, 512KB and 1024KB. 256KB turned out to be sufficient in that it&amp;rsquo;s the smallest buffer size that doesn&amp;rsquo;t reduce performance by alot, nor higher numbers will help inprove performance.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 2: Reduce or remove memory copying.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;toc_6&#34;&gt;Final Architecture&lt;/h4&gt;

&lt;p&gt;This the final architecture I ended up with and it&amp;rsquo;s working very well. The cost of each client connection are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;3 goroutines: R (receiver), P (processor) and S (sender)&lt;/li&gt;
&lt;li&gt;2 ring buffers of 256K each&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There&amp;rsquo;s very few memory copy operations going on, nor is there much memory allocation. So a good outcome overall.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/surgemq-mqtt-message-queue-750k-mps/finalarch.png&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;Race Conditions&lt;/h3&gt;

&lt;p&gt;With the ring bufer implementation, I was able to achieve 400,000 MPS with a 1:1 configuration. This worked well until I started doing multiple publishers and subscribers. The first problem I ran into was the Processor hanging. &lt;code&gt;go test -race&lt;/code&gt; also didn&amp;rsquo;t show anything that could help me.&lt;/p&gt;

&lt;p&gt;After running tests over and over again, with more and more glog.Debugf() statements, I tracked the problem to the Processor. It was waiting for space in the ring buffer to write the outgoing messages. I know that&amp;rsquo;s not possible as I am blasting messages out to net.Conn as fast as I can, so there&amp;rsquo;s no way that write space is not available.&lt;/p&gt;

&lt;p&gt;After running even more tests, and with even more glog.Debugf() statements, I finally determined the problem to be the way I was using sync.Cond. (I wish I saved the debug output..sigh) In the following code block, I was waiting for the consumer position (cpos) to pass the point in which there will be enough data for writing (wrap).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		this.pcond.L.Lock()
		for cpos = this.cseq.get(); wrap &amp;gt; cpos; cpos = this.cseq.get() {
			this.pcond.Wait()
		}
		this.pcond.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The steps are really quite simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I lock the producer sync.Conn&lt;/li&gt;
&lt;li&gt;I get the consumer position, compare it to wrap (position that I need cpos to pass to indicate there&amp;rsquo;s enough write space)&lt;/li&gt;
&lt;li&gt;If there&amp;rsquo;s not enough space, I wait, otherwise I move on&lt;/li&gt;
&lt;li&gt;I unlock the producer sync.Conn&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Then in the Sender goroutine, I read data from the ring buffer, write to net.Conn, update the consumer position, and call &lt;code&gt;pcond.Broadcast()&lt;/code&gt; to unblock the above &lt;code&gt;pcond.Wait()&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		this.cseq.set(cpos + int64(n))
		this.pcond.Broadcast()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;According to the &lt;a href=&#34;http://golang.org/pkg/sync/#Cond.Broadcast&#34;&gt;Go doc&lt;/a&gt;,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Broadcast wakes all goroutines waiting on c.&lt;/p&gt;

&lt;p&gt;It is allowed but not required for the caller to hold c.L during the call.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So what I have above should work perfectly fine. Except it doesn&amp;rsquo;t. What happens is that I ran into a situation where &lt;code&gt;pcond.Broadcast()&lt;/code&gt; was called after the &lt;code&gt;wrap &amp;gt; cpos&lt;/code&gt; check, but before &lt;code&gt;pcond.Wait()&lt;/code&gt;. In these cases, the &lt;code&gt;wrap &amp;gt; cpos&lt;/code&gt; returned true, which means we need to go wait. But before &lt;code&gt;pcond.Wait()&lt;/code&gt; was called, the Sender goroutine has updated cpos, and called &lt;code&gt;pcond.Broadcast()&lt;/code&gt;. So when &lt;code&gt;pcond.Wait()&lt;/code&gt; is called, there&amp;rsquo;s nothing to wake it up, and thus it hangs forever.&lt;/p&gt;

&lt;p&gt;On the Sender side, because there&amp;rsquo;s no more data to read, it is also just waiting for more data. So both the Sender and Processor are now hung.&lt;/p&gt;

&lt;p&gt;After I finally figured out the root cause, I realized that, unlike what the go doc suggested, the caller should really hold c.L during the call to Broadcast(). So I modified the code to the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;		this.cseq.set(cpos + int64(n))
		this.pcond.L.Lock()
		this.pcond.Broadcast()
		this.pcond.L.Unlock()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What this does is that it ensures I can never call &lt;code&gt;pcond.Broadcast()&lt;/code&gt; after &lt;code&gt;pcond.L.Lock()&lt;/code&gt; (in the Processor goroutine) is called but &lt;code&gt;pcond.Wait()&lt;/code&gt; is not called. When &lt;code&gt;pcond.Wait()&lt;/code&gt; is called, it actually calls &lt;code&gt;pcond.L.Unlock()&lt;/code&gt; internally so it will allow &lt;code&gt;pcond.L.Lock()&lt;/code&gt; in the Sender goroutine to be called.&lt;/p&gt;

&lt;p&gt;In any case, we are finally on our way to working with multiple clients.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 3: Race conditions can happen even if you think you followed all the right steps.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;toc_8&#34;&gt;But Wait, There&amp;rsquo;s More (Race Conditions)&lt;/h4&gt;

&lt;p&gt;As I increase the number of publishers and subscribers, all the sudden I was getting errors about receiving RESERVED messages, and this happens intermittenly, and only when I blast enough messages. Sometimes I have to run the tests many times to catch this from happening.&lt;/p&gt;

&lt;p&gt;It turns out that while I was thinking I only had 1 Publisher per client connection that&amp;rsquo;s writing to the outgoing buffer, I, in fact, had many. This happens when a client is sent a message to a topic that it subscribes to. In this case, the Processor of the publishing client calls the subscriber client&amp;rsquo;s Publish() method, and writes the message to the outgoing ring buffer. At the same time, other publishing clients can be publishing other messages to the subscriber client. When this happens, they could overwrite eachother&amp;rsquo;s message because the ring buffer is NOT designed for multiple writers.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;go test -race&lt;/code&gt; should technically find this race condition (I think). But given that this condition only happens intermittenly and sometimes it only happens when there&amp;rsquo;s a large volume of messages, the race detector was taking too long and I was too impatient.&lt;/p&gt;

&lt;p&gt;Regardless, after identifying the root cause, I added a Mutex to serialize the writes. At some point I may come back and rewrite it without the lock. But for now it&amp;rsquo;s good enough.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lesson 4: Use the race detector!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;toc_9&#34;&gt;Performance Benchmarks&lt;/h3&gt;

&lt;p&gt;These performance numbers are calculated as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sent messages MPS = total messages sent / total elapsed time between 1st and last message sent for all senders&lt;/li&gt;
&lt;li&gt;received messages MPS = total messages received / total elapsed time between 1st and last message received for all receivers&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;toc_10&#34;&gt;Environment&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;$ go version
go version go1.3.3 darwin/amd64

---

Macbook Pro Late 2013
2.8 GHz Intel Core i7
16 GB 1600 MHz DDR3
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_11&#34;&gt;Server&lt;/h4&gt;

&lt;p&gt;To start the server,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd benchmark
$ GOMAXPROCS=2 go test -run=TestServer -vv=2 -logtostderr
server/ListenAndServe: server is ready...
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_12&#34;&gt;1:1&lt;/h4&gt;

&lt;p&gt;To run the single publisher and single subscriber test case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 1 -receivers 1
Total Sent 1000000 messages in 2434523153 ns, 2434 ns/msg, 410758 msgs/sec
Total Received 1000000 messages in 2434523153 ns, 2434 ns/msg, 410758 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_13&#34;&gt;Fan-In&lt;/h4&gt;

&lt;p&gt;To run the Fan-In test with 20 senders and 1 receiver:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 20 -receivers 1
Total Sent 1035436 messages in 2212609304 ns, 2136 ns/msg, 467970 msgs/sec
Total Received 1000022 messages in 2212609304 ns, 2212 ns/msg, 451965 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_14&#34;&gt;Fan-Out&lt;/h4&gt;

&lt;p&gt;To run the Fan-Out test with 1 sender and 20 receivers:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestFan -vv=2 -logtostderr -senders 1 -receivers 20
Total Sent 1000000 messages in 10715317340 ns, 10715 ns/msg, 93324 msgs/sec
Total Received 8180723 messages in 10715317340 ns, 1309 ns/msg, 763460 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_15&#34;&gt;Mesh&lt;/h4&gt;

&lt;p&gt;To run a full mesh test where every client is subscribed to the same topic, thus every message sent w/ the right topic will go to ALL of the other clients:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ GOMAXPROCS=2 go test -run=TestMesh -vv=2 -logtostderr -senders 20 -messages 100000
Total Sent 2000000 messages in 51385336097 ns, 25692 ns/msg, 38921 msgs/sec
Total Received 40000000 messages in 51420975243 ns, 1285 ns/msg, 777892 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_16&#34;&gt;Next Steps&lt;/h3&gt;

&lt;p&gt;There&amp;rsquo;s a lot more to do with SurgeMQ. Given the limited time I have, I expect it will take me a while to get to full compliant with the MQTT spec. But that will be my focus, now that performance is out of the way, as I get time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graceful Shutdown of Go net.Listeners</title>
      <link>http://zhen.org/blog/graceful-shutdown-of-go-net-dot-listeners/</link>
      <pubDate>Thu, 12 Dec 2013 23:33:22 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/graceful-shutdown-of-go-net-dot-listeners/</guid>
      <description>&lt;p&gt;Comments/Feedbacks at &lt;a href=&#34;https://news.ycombinator.com/item?id=6899568&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1ss929/graceful_shutdown_of_go_netlisteners/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The past few evenings I&amp;rsquo;ve been working on a Go program that listens on a TCP socket, accepts new connections, processes some data and then return some data to the client. This is actually fairly simple in Go. The &lt;a href=&#34;http://golang.org/pkg/net&#34;&gt;top&lt;/a&gt; of the &lt;em&gt;net&lt;/em&gt; package has some fairly simple examples of how to do that. Another example can be found about &lt;a href=&#34;http://golang.org/pkg/net/#example_Listener&#34;&gt;mid-section&lt;/a&gt; of the page. You can also find a TON of examples online that shows you how to build a simple TCP listener.&lt;/p&gt;

&lt;p&gt;However, one thing I noticed in all these examples is that none of them shows you how to gracefully shutdown the TCP listener. Most of the examples expect to program to exit so there&amp;rsquo;s no need to clean up anything. However, if you have a program that&amp;rsquo;s a long-running server, and need to, for whatever reason, need to shutdown the TCP listener, you will need to clean up after yourself. Otherwise you may leave a bunch of goroutines behind unintentionally.&lt;/p&gt;

&lt;p&gt;Another reason I wanted a way to shutdown the TCP listener is I want to be able to start a listener in my tests, then start up a bunch of clients, test some stuff, then afterwards shutdown the server. Then I can start another listener in another test for some other tests.&lt;/p&gt;

&lt;p&gt;After some help from jhoto and foobaz on the #go-nuts IRC channel, I wrote the following example to demonstrate the graceful shutdown approach.&lt;/p&gt;

&lt;p&gt;The basic idea is to leverage a quit channel to tell the Accept() goroutine that it&amp;rsquo;s time to quit. Using quit channel is a fairly common practice in Go. However, in this case, the Accept() call is blocking waiting for new connections, so closing the quit channel won&amp;rsquo;t have any effect unless the goroutine actually checks it. So to force Accept() to return from blocking, we can close the net.Listener.&lt;/p&gt;

&lt;p&gt;The order of the operation matters somewhat. We will want to first close the quit channel, then close the net.Listener. If we reverse the order, you will likely see a few more errors from the Accept() call.&lt;/p&gt;

&lt;p&gt;The netgrace_test.go file below shows an example of how to use the quit channel to help gracefully shutdown net.Listeners.&lt;/p&gt;

&lt;p&gt;Hopefully you will find this tip useful. You can find it as a &lt;a href=&#34;https://gist.github.com/zhenjl/7940977&#34;&gt;gist&lt;/a&gt; as well.&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/zhenjl/7940977.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Comments/Feedbacks at &lt;a href=&#34;https://news.ycombinator.com/item?id=6899568&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1ss929/graceful_shutdown_of_go_netlisteners/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ring Buffer - Variable-Length, Low-Latency, Lock-Free, Disruptor-Style</title>
      <link>http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/</link>
      <pubDate>Sat, 30 Nov 2013 23:20:22 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/ring-buffer-variable-length-low-latency-disruptor-style/</guid>
      <description>

&lt;p&gt;Comments/feedbacks on &lt;a href=&#34;https://news.ycombinator.com/item?id=6831293&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1rvvb6/ring_buffer_variablelength_lowlatency_lockfree/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2013-12-04 Update #1: Read a &lt;a href=&#34;https://groups.google.com/forum/#!topic/golang-nuts/7tUShPuPfNM&#34;&gt;very interesting thread on golang-nuts list&lt;/a&gt; on the performance of interface. Seems like using interfaces really affects performance. In Joshua&amp;rsquo;s test he saw a 3-4x performance difference. I decided to try this on the ring buffer implementation since I am currently using interface. A quick test laster, looks like NOT using interface increased performance 2.4x. For now the code is in the &amp;ldquo;&lt;a href=&#34;https://github.com/reducedb/ringbuffer/tree/nointerface&#34;&gt;nointerface&lt;/a&gt;&amp;rdquo; branch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;Benchmark1ProducerAnd1Consumer-3         5000000               353 ns/op
Benchmark1ProducerAnd1ConsumerInBytes-3 10000000               147 ns/op
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/reducedb/ringbuffer&#34;&gt;This project&lt;/a&gt; implements a low-latency lock-free ring buffer for variable length byte slices. It is modeled after the &lt;a href=&#34;https://github.com/LMAX-Exchange/disruptor/&#34;&gt;LMAX Disruptor&lt;/a&gt;, but not a direct port.&lt;/li&gt;
&lt;li&gt;If you have only a single core, don&amp;rsquo;t use this. Use Go channels instead! In fact, unless you can spare as many cores as the number of producers and consumers, don&amp;rsquo;t use this ring buffer.&lt;/li&gt;
&lt;li&gt;In fact, for MOST use cases, Go channel is a better approach. This ring buffer is really a specialized solution for very specific use cases.&lt;/li&gt;
&lt;li&gt;Primary pattern of this ring buffer is single producer and multiple consumer, where the single producer put bytes into the buffer, and each consumer will process ALL of the items in the buffer. (Other patterns can be implemented later but this is what&amp;rsquo;s here now.)&lt;/li&gt;
&lt;li&gt;This ring buffer is designed to deal with situations where we don&amp;rsquo;t know the length of the byte slice before hand. It will write the byte slice to the buffer across multiple slots in the ring if necessary.&lt;/li&gt;
&lt;li&gt;The ring buffer currently employs a lock-free busy-wait strategy, where the producer and consumers will continue to loop until data is available. As such, it performs very well in a multi-core environment (almost twice as fast as Go channels) if you can spare 1 core per produer/consumer, but extremely poorly in a single-core environment (600 times worse compare to Go channels).&lt;/li&gt;
&lt;li&gt;You can find a lot of information on the LMAX Disruptor. The resources I used include &lt;a href=&#34;http://mechanitis.blogspot.com/search/label/disruptor&#34;&gt;Trisha&amp;rsquo;s Disruptor blog series&lt;/a&gt;, &lt;a href=&#34;http://lmax-exchange.github.io/disruptor/&#34;&gt;LMAX Disruptor main page&lt;/a&gt;, &lt;a href=&#34;http://lmax-exchange.github.com/disruptor/files/Disruptor-1.0.pdf&#34;&gt;Disruptor technical whitepaper&lt;/a&gt;, and Martin Fowler&amp;rsquo;s &lt;a href=&#34;http://martinfowler.com/articles/lmax.html&#34;&gt;LMAX Architecture article&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;You can also read more about &lt;a href=&#34;http://en.wikipedia.org/wiki/Circular_buffer&#34;&gt;circular buffer&lt;/a&gt;, &lt;a href=&#34;http://en.wikipedia.org/wiki/Producer-consumer_problem&#34;&gt;producerconsumer problem&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Go Learn Project #7 - Ring Buffer&lt;/h3&gt;

&lt;p&gt;For the past several projects (&lt;a href=&#34;http://zhen.org/blog/benchmarking-integer-compression-in-go/&#34;&gt;#6&lt;/a&gt;, &lt;a href=&#34;http://zhen.org/blog/bitmap-compression-using-ewah-in-go/&#34;&gt;#5&lt;/a&gt;, &lt;a href=&#34;http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/&#34;&gt;#4&lt;/a&gt;), I&amp;rsquo;ve mostly been hacking bits, and optimizing them as much as possible for a single core.&lt;/p&gt;

&lt;p&gt;For project #7, I decided to do something slightly different. This time we will create a ring buffer that can support variable length byte slices, and leverage multi-cores using multiple goroutines.&lt;/p&gt;

&lt;p&gt;Primary target use case is for a producer to read bytes from sockets, ZMQ, files, etc that require process, like JSON, csv, and tsv strings, at a very high speed, such as millions of lines per second, and put these lines into a buffer so other consumers can process these.&lt;/p&gt;

&lt;p&gt;A good example is when we import files with millions of lines of JSON object. These JSON objects are read from files, inserted into the buffer, and then they are unmarshalled into other structures.&lt;/p&gt;

&lt;p&gt;The goal is to process millions of data items per second.&lt;/p&gt;

&lt;h4 id=&#34;toc_2&#34;&gt;Ring Buffer&lt;/h4&gt;

&lt;p&gt;There are several ways to tackle this. A queue or ring buffer is usually a good data structure for this. You can think of a ring buffer is just a special type of queue that&amp;rsquo;s just contiguous by wrapping itself. As Wikipedia said,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A circular buffer, cyclic buffer or ring buffer is a data structure that uses a single, fixed-size buffer as if it were connected end-to-end. This structure lends itself easily to buffering data streams.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, implementation-wise, we can tackle this problem using a standard ring buffer. The most common way of implementing a standard ring buffer is to keep track of a head and a tail pointer, and keep putting items into the buffer but ensuring that head and tail don&amp;rsquo;t cross each other.&lt;/p&gt;

&lt;p&gt;In most implementations, the pointers are mod&amp;rsquo;ed with the ring size to determine the next slot size. Also, mutex is used to ensure only one thread is modifying these pointers at the same time.&lt;/p&gt;

&lt;h4 id=&#34;toc_3&#34;&gt;Go Channels&lt;/h4&gt;

&lt;p&gt;In Go, an idiomatic and fairly common way is to use &lt;a href=&#34;http://golang.org/doc/effective_go.html#channels&#34;&gt;buffered channels&lt;/a&gt; to solve this problem. It is effectively a queue where a producer puts data items into one end of the channel, and the consumer reads the data items on the other end of the channel. There are certainly pros and cons to to this approach.&lt;/p&gt;

&lt;p&gt;First, it is Go idiomatic! Need I say more&amp;hellip; :)&lt;/p&gt;

&lt;p&gt;This is probably the easiest approach since Go Channel is already a battle-tested data structure and it&amp;rsquo;s readily available. An example is provided below in the examples section. Performance-wise it is actually not too bad. In a multi-core environment, it&amp;rsquo;s about 60% of the performance compare to the ring buffer. However, in a single-core environment, it is MUCH faster. In fact, 1300 times faster than my ring buffer!&lt;/p&gt;

&lt;p&gt;There is one major difference between using channels vs this ring buffer. When the channel has multiple consumers, the data is multiplexed to the consumers. So each consumer will get only part of the data rather than going through all the data. This is illustrated by &lt;a href=&#34;http://play.golang.org/p/ACC5LIohIe&#34;&gt;this play&lt;/a&gt;. The current design of the ring buffer allows multiple consumers to go through every item in the queue. A obvious workaround is to send the data items to multiple channels.&lt;/p&gt;

&lt;p&gt;One down side with my channel approach is that I end up creating a lot of garbage over time and will need to be GC&amp;rsquo;ed. Specifically, I am creating a new byte slice for each new data item. Again, there is workaround for this. One can implement a &lt;a href=&#34;http://golang.org/doc/effective_go.html#leaky_buffer&#34;&gt;leaky buffer&lt;/a&gt;. However, because we don&amp;rsquo;t know how big the data items are before hand, it&amp;rsquo;s more difficult to preallocate the buffers up front.&lt;/p&gt;

&lt;p&gt;There might actually be a way to implement the leaky buffer with a big preallocated slice. I may just do that as the next project. The goal is to see if we can avoid having to allocate individual byte slices and leverage CPU caching for the big buffer.&lt;/p&gt;

&lt;h4 id=&#34;toc_4&#34;&gt;Lock-Free Ring Buffer&lt;/h4&gt;

&lt;p&gt;The way that I&amp;rsquo;ve decided to tackle this problem is to model the ring buffer after the LMAX Disruptor. If you haven&amp;rsquo;t read Martin Fowler&amp;rsquo;s &lt;a href=&#34;http://martinfowler.com/articles/lmax.html&#34;&gt;article on LMAX Architecture&lt;/a&gt;, at this time I would recommend that you stop and go read it first. After that, you should go read &lt;a href=&#34;http://mechanitis.blogspot.com/search/label/disruptor&#34;&gt;Trisha&amp;rsquo;s Disruptor blog series&lt;/a&gt; that explains in even more details how the Disruptor works.&lt;/p&gt;

&lt;p&gt;One thing to keep in mind is that the Disruptor-style ring buffer has significant resource requirement, i.e., it requires N cores, where N is the number of producers and consumers, to be performant. And it will keep cores busy by busy waiting (looping). So huge downside. If you don&amp;rsquo;t need this type of low latency architecture, it&amp;rsquo;s much better to stay with channels.&lt;/p&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;Performance Comparison&lt;/h3&gt;

&lt;p&gt;These benchmarks are performed on a MacBook Pro with 2.8GHz Intel Core i7 procesor (Haswell), and 16 GB 1600MHz DDR3 memory. Go version 1.2rc5.&lt;/p&gt;

&lt;p&gt;The 2 channel consumers benchmark is a single producer sending to 2 channels, each channel consumed by a separate goroutine. So it is apples-to-apples compare to the ring buffer 2 consumers benchmark.&lt;/p&gt;

&lt;p&gt;You can see clearly the requirement of 1 core per consumer/producer in the ring buffer implementation (first 6 lines). Without that, performance suffer greately!&lt;/p&gt;

&lt;p&gt;Also notice that the channel benchmark (last 6 lines) is faster for a single core than multi-cores. This is probably due to your friendly cache at play.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go test -bench=. -run=xxx -cpu=1,2,3
PASS
Benchmark1ProducerAnd1Consumer     10000            339807 ns/op
Benchmark1ProducerAnd1Consumer-2        10000000               258 ns/op
Benchmark1ProducerAnd1Consumer-3        10000000               260 ns/op
Benchmark1ProducerAnd2Consumers    10000            341859 ns/op
Benchmark1ProducerAnd2Consumers-2         200000             14967 ns/op
Benchmark1ProducerAnd2Consumers-3        5000000               340 ns/op
BenchmarkChannels1Consumer      10000000               241 ns/op
BenchmarkChannels1Consumer-2     5000000               436 ns/op
BenchmarkChannels1Consumer-3     5000000               446 ns/op
BenchmarkChannels2Consumers      5000000               319 ns/op
BenchmarkChannels2Consumers-2    5000000               647 ns/op
BenchmarkChannels2Consumers-3    5000000               570 ns/op
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;Examples&lt;/h3&gt;

&lt;h4 id=&#34;toc_7&#34;&gt;Go Channel: 1 Producer and 1 Consumer&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;func BenchmarkChannels(b *testing.B) {
	dataSize := 256
	data := make([]byte, dataSize)
	for i := 0; i &amp;lt; dataSize; i++ {
		data[i] = byte(i % 256)
	}

	ch := make(chan []byte, 128)
	go func() {
		for i := 0; i &amp;lt; b.N; i++ {
			// To be fair, we want to make a copy of the data, otherwise we are just
			// sending the same slice header over and over. In the real-world, the
			// original data slice may get over-written by the next set of bytes.
			tmp := make([]byte, dataSize)
			copy(tmp, data)
			ch &amp;lt;- tmp
		}
	}()

	for i := 0; i &amp;lt; b.N; i++ {
		out := &amp;lt;-ch
		if !bytes.Equal(out, data) {
			b.Fatalf(&amp;quot;bytes not the same&amp;quot;)
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_8&#34;&gt;Ring Buffer: 1 Producer and 1 Consumer&lt;/h4&gt;

&lt;p&gt;This test function creates a 256-slot ring buffer, with each slot being 128 bytes long. It also creates 1 producer and 1 consumer, where the producer will put the same byte slice into the buffer 10,000 times, and the consumer will read from the buffer and then make sure we read the correct byte slice.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Test1ProducerAnd1ConsumerAgain(t *testing.T) {
	// Creates a new ring buffer that&#39;s 256 slots and each slot 128 bytes long.
	r, err := New(128, 256)
	if err != nil {
		t.Fatal(err)
	}

	// Gets a single producer from the the ring buffer. If NewProducer() is called
	// the second time, an error will be returned.
	p, err := r.NewProducer()
	if err != nil {
		t.Fatal(err)
	}

	// Gets a singel consumer from the ring buffer. You can call NewConsumer() multiple
	// times and get back a new consumer each time. The consumers are independent and will
	// go through the ring buffers separately. In other words, each consumer will have 
	// their own independent sequence tracker.
	c, err := r.NewConsumer()
	if err != nil {
		t.Fatal(err)
	}

	// We are going to write 10,000 items into the buffer.
	var count int64 = 10000

	// Let&#39;s prepare the data to write. It&#39;s just a basic byte slice that&#39;s 256 bytes long.
	dataSize := 256
	data := make([]byte, dataSize)
	for i := 0; i &amp;lt; dataSize; i++ {
		data[i] = byte(i % 256)
	}

	// Producer goroutine
	go func() {
		// Producer will put the same data slice into the buffer _count_ times
		for i := int64(0); i &amp;lt; count; i++ {
			if _, err := p.Put(data); err != nil {
				// Unfortuantely we have an issue here. If the producer gets an error 
				// and exits, the consumer will continue to wait and not exit. In the
				// real-world, we need to notify all the consumers that there&#39;s been
				// an error and ensure they exit as well.
				t.Fatal(err)
			}
		}
	}()

	var total int64

	// Consumer goroutine
	
	// The consumer will also read from the buffer _count_ times
	for i := int64(0); i &amp;lt; count; i++ {
		if out, err := c.Get(); err != nil {
			t.Fatal(err)
		} else {
			// Check to see if the byte slice we got is the same as the original data
			if !bytes.Equal(out.([]byte), data) {
				t.Fatalf(&amp;quot;bytes not the same&amp;quot;)
			}

			total++
		}
	}

	// Check to make sure the count matches
	if total != count {
		t.Fatalf(&amp;quot;Expected to have read %d items, got %d\n&amp;quot;, count, total)
	}
}

&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_9&#34;&gt;Ring Buffer: 1 Producer and 2 Consumers&lt;/h4&gt;

&lt;p&gt;As mentioned before, the ring buffer supports multiple consumers. &lt;a href=&#34;https://github.com/reducedb/ringbuffer/blob/master/bytebuffer/ringbuffer_test.go#L304&#34;&gt;This example&lt;/a&gt; shows how you would create two consumers.&lt;/p&gt;

&lt;h3 id=&#34;toc_10&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;See tl;dr on top.&lt;/p&gt;

&lt;p&gt;Comments/feedbacks on &lt;a href=&#34;https://news.ycombinator.com/item?id=6831293&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1rvvb6/ring_buffer_variablelength_lowlatency_lockfree/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go vs Java: Decoding Billions of Integers Per Second</title>
      <link>http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/</link>
      <pubDate>Thu, 14 Nov 2013 19:40:22 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/</guid>
      <description>

&lt;p&gt;Comments/feedbacks on &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qquqz/go_vs_java_decoding_billions_of_integers_per/&#34;&gt;reddit&lt;/a&gt;, &lt;a href=&#34;https://news.ycombinator.com/item?id=6743821&#34;&gt;hacker news&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2013-11-17 Update #2: Tried another new trick. This time I updated the leading bit position function, when using the gccgo compiler, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/bitlen_gccgo.go&#34;&gt;to use libgcc&amp;rsquo;s &lt;code&gt;__clzdi2&lt;/code&gt; routine&lt;/a&gt;. This had the same effect as the update #1 except it&amp;rsquo;s for when gccgo is used. Performance increase ranged from 0% to 20% for encoding only. Thanks dgryski on reddit and minux on the golang-nuts mailing list.&lt;/p&gt;

&lt;p&gt;2013-11-16 Update #1: Tried a new trick, which is to use an &lt;a href=&#34;https://github.com/reducedb/encoding/commit/ea080c479fb4994e400ebba021d13f10c4f3fecc&#34;&gt;assembly version of bitlen&lt;/a&gt; to calculate the leading bit position. See the section below on &amp;ldquo;Use Some Assembly&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, before you crucify me for benchmarking Go vs Java, let me just say that I am not trying to discredit Go. I like Go and will use it for more projects. I am simply trying to show the results as objectively as I can, and hope that the community can help me improve my skills as well as the performance of the libraries.&lt;/p&gt;

&lt;p&gt;I don&amp;rsquo;t consider myself a Go expert. I&amp;rsquo;ve been using Go for a few months and have been documenting the projects as I go in this blog. I&amp;rsquo;ve learned a ton and have applied many of the things I learned in optimizing this project. However, I cannot claim that I have done everything possible, so the performance numbers &amp;ldquo;could&amp;rdquo; still be better.&lt;/p&gt;

&lt;p&gt;I would like to ask for your help, if you can spare the time, to share some of your optimization tips and secrets. I would love to make this library even faster if possible.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;tl;dr&lt;/h3&gt;

&lt;p&gt;The following chart shows how much (%) faster Java is compare to Go in decoding integers that are encoded using different codecs. It shows results from processing two different files. See below for more details.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_faster.png&#34;&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Daniel Lemire&amp;rsquo;s &lt;a href=&#34;https://github.com/lemire/JavaFastPFOR&#34;&gt;Java version&lt;/a&gt; is anywhere from 12% to 180% faster than my &lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;Go version&lt;/a&gt; for decoding integers. I didn&amp;rsquo;t compare the &lt;a href=&#34;https://github.com/lemire/fastpfor&#34;&gt;C++ version&lt;/a&gt; but given that the C++ version has access to SIMD operations, it can be much faster.&lt;/li&gt;
&lt;li&gt;I tried many different ways to optimize my Go code for this projects, including using range for looping through slices, inlining simple functions, unrolling simple loops, unrolling even more loops, disabling bound checking (not generally recommended), using &lt;em&gt;gccgo&lt;/em&gt; to compile, and used some assembly.&lt;/li&gt;
&lt;li&gt;Using &lt;em&gt;gccgo -O3&lt;/em&gt; resulted in the highest performance. I tested using standard gc compiler, gc -B, gccgo, and gccgo -O3. The comparison above uses the &lt;em&gt;gccgo -O3&lt;/em&gt; numbers.&lt;/li&gt;
&lt;li&gt;Using a range loop instead of unrolling a loop in one of the often used functions, AND compiling using &lt;em&gt;gccgo -O3&lt;/em&gt;, I was able to get within 6% of Java version for Delta BP32 decoding. However, all of the other Go binaries suffered greatly.&lt;/li&gt;
&lt;li&gt;This benchmark is purely a CPU benchmark. The test environment has enough memory to keep all the arrays in memory without causing swap, and there&amp;rsquo;s no disk IO involved in the actual encoding/decoding functions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Project Review&lt;/h3&gt;

&lt;p&gt;A month ago I wrote about my &amp;ldquo;Go Learn&amp;rdquo; project #6: &lt;a href=&#34;http://zhen.org/blog/benchmarking-integer-compression-in-go/&#34;&gt;Benchmarking Integer Compression in Go&lt;/a&gt; &lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;(github)&lt;/a&gt;. In that project I ported 6 different codecs for encoding and decoding 32-bit integers. Since then, I have ported a couple more codecs, cleaned up the directories, and performed a ton of profiling and optimization to increase performance.&lt;/p&gt;

&lt;p&gt;There are now a total of 8 codecs available:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Binary Packing (BP32), FastPFOR, Variable Byte (varint) (top level directories)

&lt;ul&gt;
&lt;li&gt;Standard codec that encodes/decodes the integers as they are&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Delta BP32, Delta FastPFOR (&lt;strong&gt;new&lt;/strong&gt;), Delta Variable Byte (under &lt;em&gt;delta/&lt;/em&gt;)

&lt;ul&gt;
&lt;li&gt;Encodes/decodes the deltas of the integers&lt;/li&gt;
&lt;li&gt;These codecs generally produce much more compact representations if the integers are sorted&lt;/li&gt;
&lt;li&gt;These codecs generally perform much faster, but there are some exceptions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ZigZag BP32, ZigZag FastPFOR (&lt;strong&gt;new&lt;/strong&gt;) (under &lt;em&gt;zigzag/&lt;/em&gt;)

&lt;ul&gt;
&lt;li&gt;Encode/decodes the deltas of the integers, where the deltas themselves are encoded using Google&amp;rsquo;s zigzag encoding&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition, the &lt;em&gt;benchmark&lt;/em&gt; program under &lt;em&gt;benchmark/&lt;/em&gt; is provided to let users easily test different integer lists and codecs.&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Techniques Tried&lt;/h3&gt;

&lt;p&gt;I cannot say this enough: &lt;strong&gt;benchmark, profile, optimize, rinse, repeat&lt;/strong&gt;. Go has made testing, benchmarking, and profiling extremely simple. You owe it to yourself to optimize your code using these tools. Previously I have written about how I was able to &lt;a href=&#34;http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/&#34;&gt;improve the cityhash Go implementation performance by 3-16X by Go profiling&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To optimize the integer encoding library, I followed the same techniques to profile each codec, and try as much as I can to optimzie the hot spots.&lt;/p&gt;

&lt;p&gt;Below are some of the optimizations I&amp;rsquo;ve tried. Some helped, some didn&amp;rsquo;t.&lt;/p&gt;

&lt;h4 id=&#34;toc_3&#34;&gt;For-Range Through Slices&lt;/h4&gt;

&lt;p&gt;I learned this when Ian Taylor from Google (and others) helped me optimize one of the functions using range to loop through the slices instead of &lt;code&gt;for i := 0; i &amp;lt; b; i++ {}&lt;/code&gt; loops. The for-range method can be 4-7 times faster than the other way. You can see the difference between BenchmarkOffset and BenchmarkRange &lt;a href=&#34;https://gist.github.com/zhenjl/7495442&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also found that &lt;em&gt;gccgo -O3&lt;/em&gt; can do some really good optimizations with simple range loops. You can see the difference with this &lt;a href=&#34;https://gist.github.com/zhenjl/7495442&#34;&gt;gist&lt;/a&gt;. When using &lt;em&gt;gc&lt;/em&gt; the standard Go compiler, BenchmarkRange (31.3 ns/op) is 56% slower than BenchmarkUnrolled (13.9 ns/op). However, then reverse is true when using &lt;em&gt;gccgo -O3&lt;/em&gt;. BenchmarkUnrolled (8.92 ns/op) is 100% slower than BenchmarkRange (4.46 ns/op).&lt;/p&gt;

&lt;p&gt;Side note: this set of benchmarks are courtesy of DisposaBoy and Athiwat in the #go-nuts IRC channel. Thanks for your help guys.&lt;/p&gt;

&lt;h4 id=&#34;toc_4&#34;&gt;Unroll Simple Loops&lt;/h4&gt;

&lt;p&gt;For some simple, known-size, loops, such as initializing a slice with the same initial non-zero value, unrolling the loop makes a big difference. The caveat is that &lt;em&gt;gccgo -O3&lt;/em&gt; does an amazing job of optimizing these simple range loops, so in that case unrolling the loop is actually slower.&lt;/p&gt;

&lt;p&gt;As an example, the &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/bitpacking/delta_bitpacking.go#L242&#34;&gt;following function&lt;/a&gt; is unrolled as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func deltaunpack0(initoffset int32, in []int32, inpos int, out []int32, outpos int) {
    out[outpos+0] = initoffset
    out[outpos+1] = initoffset
    out[outpos+2] = initoffset
    .
    .
    .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It was originally written as&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tmp := out[outpos:outpos+32]
for i, _ := range tmp {
    tmp[i] = initoffset
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When unrolled, AND using the standard &lt;em&gt;gc&lt;/em&gt; compiler, we saw performance increase by almost 45% if this function is called often. However, as we mentioned above, when using &lt;em&gt;gccgo -O3&lt;/em&gt;, the for-range loop is 33% faster than the unrolled method.&lt;/p&gt;

&lt;p&gt;For now, I am keeping the unrolled version of the function.&lt;/p&gt;

&lt;h4 id=&#34;toc_5&#34;&gt;Unroll Even More Loops&lt;/h4&gt;

&lt;p&gt;Given the success of unrolling the above simple loop, I thought I try unrolling &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L150&#34;&gt;even&lt;/a&gt; . &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L281&#34;&gt;more&lt;/a&gt; . &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L412&#34;&gt;loops&lt;/a&gt; to see if it helps.&lt;/p&gt;

&lt;p&gt;It turns out performance actually suffered in some cases. I speculated that the reason may have to do with the bound checking when accessing slices. It turns out I might be right. After I disabled bound checking, performance increased when unrolling these loops. See below regarding disable bound checking.&lt;/p&gt;

&lt;h4 id=&#34;toc_6&#34;&gt;Inline Simple Functions&lt;/h4&gt;

&lt;p&gt;There are &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L53&#34;&gt;several&lt;/a&gt; . &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/util.go#L60&#34;&gt;small&lt;/a&gt; functions that gets called quite often in this encoding library. During profiling I see these functions being on top all the time.&lt;/p&gt;

&lt;p&gt;I decided to try and inline these functions to see if the reduced function call overhead will help. In general I didn&amp;rsquo;t see much performance improvements using this technique. The most I saw was a 1% increase in performance. I contribute that to noise.&lt;/p&gt;

&lt;h4 id=&#34;toc_7&#34;&gt;Disable Bound Checking (Generally NOT Recommended)&lt;/h4&gt;

&lt;p&gt;This integer encoding library operates on large slices of data. There&amp;rsquo;s a TON of slice access using index. Knowing the every slice access using index requires bound checking, I decided to try disabling bound checking using &lt;code&gt;-gcflags -B&lt;/code&gt; to compile the code.&lt;/p&gt;

&lt;p&gt;Disabling the bound checking for the Go compiler didn&amp;rsquo;t help as much as I hoped. For &lt;em&gt;ts.txt&lt;/em&gt;, disabling bound checking increased performance by 10% for Delta BP32 encoding only.&lt;/p&gt;

&lt;p&gt;However, if I disabled bound checking AND unrolled even more loops, we saw decoding performance increase anywhere from 10-40%. Encoding performance didn&amp;rsquo;t see much change. The following chart shows the performance increase (%) from the for-range loops to unrolled some additional loops when I disabled bound checking. This is comparing to the standard &lt;em&gt;gc&lt;/em&gt; compiler.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/no_bound_checking.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The question is, is it worth the removal of bound checking? For the raw results, I kept the numbers from the for-range loops.&lt;/p&gt;

&lt;h4 id=&#34;toc_8&#34;&gt;Using gccgo&lt;/h4&gt;

&lt;p&gt;When I was looking around for Go optimization techniques, I saw several posts on StackOverflow as well as the Go mailing list suggesting people try &lt;em&gt;gccgo&lt;/em&gt; as the compiler if they wanted more performance. So I thought I give it a shot as well. After downloading gcc 4.8.2 and letting it compile overnight, I was finally able to test it out.&lt;/p&gt;

&lt;p&gt;Compiling with &lt;em&gt;gccgo&lt;/em&gt; without any optimization flags actually saw performance drop by 50-60%. The best performance result was achieved when I compiled using &lt;em&gt;gccgo -O3&lt;/em&gt;. The comparison to Java uses the numbers from that binary.&lt;/p&gt;

&lt;p&gt;As mentioned above, &lt;em&gt;gccgo -O3&lt;/em&gt; seems to do a pretty amazing job of optimizing simple range loops. I was able to achieve 33% performance increase using for-range with &lt;em&gt;gccgo -O3&lt;/em&gt; instead of unrolling the simple loop. The final result was within 6% of the Java version for Delta BP32 decoding.&lt;/p&gt;

&lt;h4 id=&#34;toc_9&#34;&gt;Use Some Assembly&lt;/h4&gt;

&lt;p&gt;The final trick I tried is to convert one of the often used functions to assembly language. This was suggested to me almost 6 weeks ago by Dave Andersen on the golang-nuts Google group. He suggested that I steal the bitLen function in the math/big/arith files. That&amp;rsquo;s &lt;a href=&#34;https://github.com/reducedb/encoding/commit/ea080c479fb4994e400ebba021d13f10c4f3fecc&#34;&gt;exactly what I did&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The bitlen function returns the position of the most significant bit that&amp;rsquo;s set to 1. It is most often called by encoding methods to determine how many bits are required to store that integers. So natually one would expect only the encoding functions will be improved.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s exactly what happened. Using the new bitlen assembly function, I was able to improve encoding performance by anywhere from 3% to 40%, depending on the codec. The most significant improvement was saw when Delta FastPFOR encoding was applied on &lt;em&gt;latency.txt&lt;/em&gt;. It consistently saw ~40% performance increase.&lt;/p&gt;

&lt;p&gt;As such, the &lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;code&lt;/a&gt; has been updated to use the assembly version of the bitlen.&lt;/p&gt;

&lt;h3 id=&#34;toc_10&#34;&gt;Benchmark Environment&lt;/h3&gt;

&lt;p&gt;The system I used to run the benchmarks was graciously provided by Dr. Daniel Lemire. Here are the CPU and memory information at the time I ran the benchmarks. As you can see, we have plenty of memory to load the large integer arrays and should cause no swap. (I had this issue running these tests on my little MBA with 4GB of memory. :)&lt;/p&gt;

&lt;p&gt;No disk IO is involved in this benchmark.&lt;/p&gt;

&lt;h4 id=&#34;toc_11&#34;&gt;OS&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;NAME=&amp;quot;Ubuntu&amp;quot;
VERSION=&amp;quot;12.10, Quantal Quetzal&amp;quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&amp;quot;Ubuntu quantal (12.10)&amp;quot;
VERSION_ID=&amp;quot;12.10&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_12&#34;&gt;CPU&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;model name      : Intel(R) Xeon(R) CPU E5-1620 0 @ 3.60GHz
cpu MHz         : 3591.566
cache size      : 10240 KB
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_13&#34;&gt;Memory&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;             total       used       free     shared    buffers     cached
Mem:      32872068   13548960   19323108          0     209416   11517476
-/+ buffers/cache:    1822068   31050000
Swap:     33476604          0   33476604
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_14&#34;&gt;java&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;java version &amp;quot;1.7.0_25&amp;quot;
OpenJDK Runtime Environment (IcedTea 2.3.10) (7u25-2.3.10-1ubuntu0.12.10.2)
OpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_15&#34;&gt;go and gccgo&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;go version go1.2rc4 linux/amd64
gcc version 4.8.2 (GCC)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;toc_16&#34;&gt;Files&lt;/h4&gt;

&lt;p&gt;There are two files used in this benchmark:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;File 1: &lt;em&gt;&lt;a href=&#34;https://github.com/reducedb/encoding/tree/master/benchmark/data&#34;&gt;ts.txt&lt;/a&gt;&lt;/em&gt; contains 144285498 sorted integers. They are timestamps at 1 second precision. There are a lot of repeats as multiple events are recorded for that second.&lt;/li&gt;
&lt;li&gt;File 2: &lt;em&gt;latency.txt&lt;/em&gt; contains 144285498 &lt;strong&gt;unsorted&lt;/strong&gt; integers. An example, lat.txt.gz, can be seen &lt;a href=&#34;https://github.com/reducedb/encoding/tree/master/benchmark/data&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;toc_17&#34;&gt;Codecs&lt;/h4&gt;

&lt;p&gt;For this benchmark, I used 4 different codecs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Delta Binary Packing (BP32) (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/IntegratedBinaryPacking.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/delta/bp32/bp32.go&#34;&gt;Go&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Delta FastPFOR (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/IntegratedFastPFOR.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/delta/fastpfor/fastpfor.go&#34;&gt;Go&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Binary Packing (BP32) (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/BinaryPacking.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/bp32/bp32.go&#34;&gt;Go&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;FastPFOR (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/FastPFOR.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/fastpfor/fastpfor.go&#34;&gt;Go&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because both BP32 and FastPFOR work on 128 integer blocks, the 58 remaining integers from the test files are encoded using Delta VariableByte and Variable Byte codecs, respectively. This is achieved using a &lt;strong&gt;Composition&lt;/strong&gt; (&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/IntegratedComposition.java&#34;&gt;Java&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/encoding/blob/master/composition/composition.go&#34;&gt;Go&lt;/a&gt;) codec.&lt;/p&gt;

&lt;p&gt;The Java and Go versions of the codecs are almost identical, logic-wise, aside from language differences. These codecs operate on arrays (or slices in Go) of integers. There are a lot of bitwise and shift operations, and lots of loops.&lt;/p&gt;

&lt;h3 id=&#34;toc_18&#34;&gt;Benchmark Results&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdDRyNEhPWUlqMHZzMG5FWFQzX1ZoZ1E&amp;amp;output=html&#34;&gt;raw results&lt;/a&gt; from running different Go compilers (and flags) and codes are in the Google spreadsheet at the bottom of the post.&lt;/p&gt;

&lt;p&gt;To be consistent, all the percentage numbers presented below are based on dividing the difference between the larger number (A) and the smaller number (B) by the smaller number (B).&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(A - B) / B&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So you can say that A is faster than B by X%.&lt;/p&gt;

&lt;h4 id=&#34;toc_19&#34;&gt;Go Fastest&lt;/h4&gt;

&lt;p&gt;I compiled 4 different versions of Go binary for this benchmark:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;benchmark.gc&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;This is built using the standard Go compiler, &lt;em&gt;gc&lt;/em&gt;. The command is &lt;code&gt;go build benchmark.o&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;benchmark.gc-B&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;This is built using the standard Go compiler, &lt;em&gt;gc&lt;/em&gt;, and I turned off bound checking for this version since the codecs deals with slices a lot. The command is &lt;code&gt;go build -gcflags -B benchmark.o&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;benchmark.gccgo&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;This is built using the &lt;em&gt;gccgo&lt;/em&gt; compiler with no additional flags. The command is &lt;code&gt;go build -compiler gccgo benchmark.o&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;benchmark.gccgo-O3&lt;/em&gt;

&lt;ul&gt;
&lt;li&gt;This is built using the &lt;em&gt;gccgo&lt;/em&gt; compiler with the &lt;em&gt;-O3&lt;/em&gt; flag. The command is &lt;code&gt;go build -compiler gccgo -gccgoflags &#39;-O3 -static&#39; benchmark.o&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Surprisingly, disabling the bound checking for the Go compiler didn&amp;rsquo;t help as much as I hoped. For &lt;em&gt;ts.txt&lt;/em&gt;, disabling bound checking increased performance by 10-40% for encoding only. For &lt;em&gt;ts.txt&lt;/em&gt; decoding and &lt;em&gt;latency.txt&lt;/em&gt;, it didn&amp;rsquo;t help at all.&lt;/p&gt;

&lt;p&gt;Using the &lt;em&gt;gccgo&lt;/em&gt; compiler with no flags had the worst performance. In general we saw that &lt;em&gt;benchmark.gc&lt;/em&gt; (standard Go version) is about 110%-130% faster than the &lt;em&gt;gccgo&lt;/em&gt; version.&lt;/p&gt;

&lt;p&gt;Lastly, the &lt;em&gt;gccgo-O3&lt;/em&gt; version is the fastest. This is the version that&amp;rsquo;s compiled using &lt;code&gt;-O3&lt;/code&gt; flag for gccgo. We saw that the &lt;em&gt;gccgo-O3&lt;/em&gt; version is anywhere from 10% to 60% faster than the &lt;em&gt;gc&lt;/em&gt; version.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/comparing_go_binaries.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;Above is a chart that shows the decoding performance of the &lt;em&gt;ts.txt&lt;/em&gt; file for the different binaries and codecs.&lt;/p&gt;

&lt;p&gt;For the comparison with Java, I am using the &lt;em&gt;gccgo-O3&lt;/em&gt; numbers.&lt;/p&gt;

&lt;h4 id=&#34;toc_20&#34;&gt;Bits Per Integer (Lower is Better)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/bits_per_integer.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;As you can tell, when the integers are sorted (ts.txt), the compression ratio is VERY good. Delta BP32 achieved 0.25 bits per 32-bit integer, and Delta FastPFOR is even better at 0.13 bits per integer. This is also because there are a lot of repeats in ts.txt.&lt;/p&gt;

&lt;p&gt;Because the timestamps are rather large numbers, e.g., 1375228800, when the non-delta codecs are used, they did not achieve very good compression ratio. We achieved ~31 bits per 32-bit integer using standard FastPFOR and BP32 codecs.&lt;/p&gt;

&lt;p&gt;When the integers are NOT sorted, then we run into trouble. When delta codecs are used, a lot of deltas are negative numbers, which means the MSB for most of the deltas is 1. In this case, it&amp;rsquo;s actually better to use the standard codecs instead of the delta codecs. The standard codecs achieved ~24 bits per 32-bit integer, and the delta codecs were ~32 bits per 32-bit integer.&lt;/p&gt;

&lt;p&gt;I also tested the latency file against the &lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/encoding#varints&#34;&gt;zigzag&lt;/a&gt; delta codecs and achieved ~25 bits per 32-bit integer. So it&amp;rsquo;s not much better than the standard codecs. However, zigzag delta comes in extremely handy when the negative numbers are smaller.&lt;/p&gt;

&lt;h4 id=&#34;toc_21&#34;&gt;Java vs. Go&lt;/h4&gt;

&lt;p&gt;For this section, we are comparing the decoding speed between the fastest Go version and Java. As you saw at the beginning of this post. Daniel Lemire&amp;rsquo;s &lt;a href=&#34;https://github.com/lemire/JavaFastPFOR&#34;&gt;Java version&lt;/a&gt; is anywhere from 12% to 180% faster than my &lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;Go version&lt;/a&gt; for decoding integers.&lt;/p&gt;

&lt;p&gt;The following chart shows how much (%) faster Java is compare to Go in decoding integers that are encoded using different codecs. It shows results from processing two different files. See below for more details.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_faster.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The following chart shows the decoding performance while processing &lt;em&gt;ts.txt&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_tstxt.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;The following chart shows the decoding performance while processing &lt;em&gt;latency.txt&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-11-14-go-vs-java-decoding-billions-of-integers-per-second/java_vs_go_latency.png&#34;&gt;&lt;/p&gt;

&lt;h4 id=&#34;toc_22&#34;&gt;Raw Results&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&#34;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdDRyNEhPWUlqMHZzMG5FWFQzX1ZoZ1E&amp;amp;output=html&#34;&gt;raw results&lt;/a&gt; are in the following Google spreadsheet.&lt;/p&gt;

&lt;iframe width=&#39;800&#39; height=&#39;500&#39; frameborder=&#39;0&#39; src=&#39;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdDRyNEhPWUlqMHZzMG5FWFQzX1ZoZ1E&amp;output=html&amp;widget=true&#39;&gt;&lt;/iframe&gt;

&lt;p&gt;Comments/feedbacks on &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qquqz/go_vs_java_decoding_billions_of_integers_per/&#34;&gt;reddit&lt;/a&gt;, &lt;a href=&#34;https://news.ycombinator.com/item?id=6743821&#34;&gt;hacker news&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving Cityhash Performance by Go Profiling</title>
      <link>http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/</link>
      <pubDate>Sun, 10 Nov 2013 15:04:22 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/</guid>
      <description>

&lt;p&gt;Comments/Feedback on &lt;a href=&#34;https://news.ycombinator.com/item?id=6710115&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qcygc/improving_cityhash_performance_by_go_profiling/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2013-11-19 Update #1: After more profiling (see &lt;em&gt;top&lt;/em&gt; output in the &amp;ldquo;After modification&amp;rdquo; section), I&amp;rsquo;ve found that these 3 functions, unalignedLoad64, fetch64, and uint64InExpectedOrder,  add up to quite a bit of execution time. I looked at the &lt;a href=&#34;https://code.google.com/p/cityhash/source/browse/trunk/src/city.cc&#34;&gt;original cityhash implementation&lt;/a&gt; and realized that the combination of these functions is basically geting a LittleEndian uint64, which we can read by doing LittleEndian.Uint64(). So I updated fetch64 to do just that. Performance increased by ~20% just because of that. Also, the bloom filter test showed that cityhash is now faster than both FNV64 and CRC64.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another month has gone by since the last post. This month has been extremely busy at work in an extraordinary good way. We had a huge POC that went quite succesfully at a large customer site. I also got a chance to visit Lisbon, Portugal as part of this POC. So things overall went pretty well.&lt;/p&gt;

&lt;p&gt;However, since family and work pretty much occupied most of my waking hours over the past few weeks, I haven&amp;rsquo;t made much progress on the &amp;ldquo;Go Learn&amp;rdquo; projects. To keep myself going, I picked a smaller task over the weekend and decided to go back to &amp;ldquo;Go Learn Project #1&amp;rdquo;, my &lt;a href=&#34;https://github.com/reducedb/cityhash&#34;&gt;cityhash&lt;/a&gt; Go implementation.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Go Learn Project #1&lt;/h3&gt;

&lt;p&gt;When I first decided to learn Go, I struggled quite a bit to find a project that I can sink my teeth into. I am not sure if others are the same way, for me to learn a new language, I have to have something meaningful to work on. I can&amp;rsquo;t just write hello world programs or follow tutorials. I can read books and articles, but I will also procrastinate for weeks if I can&amp;rsquo;t find a relevant project.&lt;/p&gt;

&lt;p&gt;Luckily, I was thinking about creating a data generator at work and wanted to write that in Go. But in order to write the data generator in Go, I first have to have a cityhash implementation in Go because our backend (C/C++) is using cityhash.&lt;/p&gt;

&lt;p&gt;Surprisingly, I looked around but couldn&amp;rsquo;t find any Go implementation of cityhash. I would have thunk that given Go and Cityhash are both from Google, some Googler would have already ported cityhash over to Go. But no such luck, or maybe I just didn&amp;rsquo;t look hard enough. In any case, I decided to port cityhash over to Go.&lt;/p&gt;

&lt;p&gt;Porting an existing project in C over to Go has a big advantage in that I don&amp;rsquo;t have to invent any new data structures or algorithms. It will allow me to focus on learning the Go syntax and the standard libraries. Many many moons ago (before I converted to the dark side) I was pretty proficient in C and reading cityhash wasn&amp;rsquo;t too difficult, so porting cityhash over to Go should be relatively straightforward.&lt;/p&gt;

&lt;p&gt;In any case, the porting process wasn&amp;rsquo;t too difficult, as it turned out. Overall I was able to do that over a weekend. I was also able to port the test program (city-test.cc) over as well (vim substitution FTW) to validate that my implementation was functionally correct.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Performance Sucked&lt;/h3&gt;

&lt;p&gt;I had always suspected that my Go cityhash implementation wasn&amp;rsquo;t great performance wise. At the time I hadn&amp;rsquo;t learned how to benchmark or profile Go programs, so I didn&amp;rsquo;t do a whole lot except ensuring functionally the results are correct. Also my data generator at work was working fine so I left the implementation as is.&lt;/p&gt;

&lt;p&gt;In September, I implemented a &lt;a href=&#34;https://github.com/reducedb/bloom&#34;&gt;Bloom Filter package&lt;/a&gt; which required a hash function as part of the implementation. For that package, I &lt;a href=&#34;http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/&#34;&gt;tested different hash functions&lt;/a&gt; to see how they affect the performance of the bloom filter. As you can see from those benchmarks, Cityhash is consistently 3x slower compare to the others. At the time I knew it was because of my implementation but didn&amp;rsquo;t look into it further.&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Profiling Go Cityhash&lt;/h3&gt;

&lt;p&gt;Since that first project, I have learned quite a bit more about benchmarking and profiling. So this weekend I finally took time to profile the Go implementation and found some interesting results. Now Go experts probably will read this and say &amp;ldquo;of course, you had no idea what you were doing.&amp;rdquo; And that would be true. I had no idea at the time. Hopefully this post will make up for it.&lt;/p&gt;

&lt;p&gt;If you haven&amp;rsquo;t read &lt;a href=&#34;http://blog.golang.org/profiling-go-programs&#34;&gt;this blog post on Go profiling&lt;/a&gt;, you should go read it now before continuing.&lt;/p&gt;

&lt;p&gt;In any case, I wrote a &lt;a href=&#34;https://gist.github.com/zhenjl/7405913&#34;&gt;short program&lt;/a&gt; to test cityhash with a big file. This way it can collect enough samples to tell me where the bottleneck is.&lt;/p&gt;

&lt;p&gt;The original implementation took 45 seconds to hash a 1.1G file. Below is the cpu profile output.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;duration = 45401991546ns
Total: 4295 samples
    2766  64.4%  64.4%     2766  64.4% runtime.memmove
     374   8.7%  73.1%      463  10.8% sweepspan
     259   6.0%  79.1%      383   8.9% MHeap_AllocLocked
     123   2.9%  82.0%      123   2.9% runtime.markspan
      95   2.2%  84.2%     1223  28.5% runtime.mallocgc
      74   1.7%  85.9%       74   1.7% runtime.MSpan_Init
      43   1.0%  86.9%     4234  98.6% github.com/reducedb/cityhash.unalignedLoad64
      40   0.9%  87.9%      595  13.9% runtime.MCache_Alloc
      32   0.7%  88.6%       32   0.7% runtime.markallocated
      31   0.7%  89.3%       56   1.3% MHeap_FreeLocked
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, most of the time were spent copying memory (64.4%). And also the sweepspan (part of GC) is also running quite often (8.7%).&lt;/p&gt;

&lt;p&gt;Before this, I had no idea that there&amp;rsquo;s that much memory being copied. So this is definitely interesting. I then looked the &lt;a href=&#34;/images/2013-11-10-improving-cithhash-performance-by-go-profiling/before.svg&#34;&gt;graph of the profile data&lt;/a&gt; using the &amp;ldquo;web&amp;rdquo; command.&lt;/p&gt;

&lt;p&gt;It shows clearly that unalignedLoad64, a function that loads a uint64 from the buffer, is causing most of the memmove. Technically, it&amp;rsquo;s calling binary.Read(), which creates an array of 8 bytes, and passes to another function which eventually calls runtime.copy to copy a few bytes of data from the original buffer into the array.&lt;/p&gt;

&lt;p&gt;So now the reason for the large amount of time spent in memmove is clear. Basically, every time I call binary.Read(), it creates an 8 byte array. Up to 8 bytes of data are copied into it. Then the data in the array gets converted into an uint64. After that, the array is thrown away. And this is done over and over again for the whole 1.1G file, which means 1.1G of memory is being created in tiny 8-byte chunks, copied, and thrown away. It&amp;rsquo;s no wonder the program is slow!&lt;/p&gt;

&lt;p&gt;By this time, some of the readers are probably wondering why the heck I am using binary.Read() if I knew that I will be reading a uint64 from a slice. And they would be right again. Only excuse I have is that I had no clue and that was the first thing I found to work a few months back, so I just used it.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Modifying the Implementation&lt;/h3&gt;

&lt;p&gt;The change turned out to be relatively simple. Instead of using binary.Read(), I used LittleEndian.Uint64() to read the uint64. After the change, I ran the same program again.&lt;/p&gt;

&lt;p&gt;Here are the results from the post-change run. The time it took to hash the 1.1G file is only 2.8 seconds. That&amp;rsquo;s 16X faster than before the change. The &amp;ldquo;top&amp;rdquo; profile output is also a lot more reasonable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;duration = 2718339693ns
Total: 245 samples
      57  23.3%  23.3%       57  23.3% encoding/binary.littleEndian.Uint64
      50  20.4%  43.7%      227  92.7% github.com/reducedb/cityhash.CityHash128WithSeed
      40  16.3%  60.0%       97  39.6% github.com/reducedb/cityhash.unalignedLoad64
      35  14.3%  74.3%      146  59.6% github.com/reducedb/cityhash.fetch64
      28  11.4%  85.7%       28  11.4% github.com/reducedb/cityhash.uint64InExpectedOrder
      27  11.0%  96.7%      132  53.9% github.com/reducedb/cityhash.weakHashLen32WithSeeds_3
       8   3.3% 100.0%        8   3.3% github.com/reducedb/cityhash.weakHashLen32WithSeeds
       0   0.0% 100.0%        1   0.4% MHeap_AllocLarge
       0   0.0% 100.0%      227  92.7% _/Users/jian/Projects/cityhash_test.TestLatencyIntegers
       0   0.0% 100.0%      227  92.7% github.com/reducedb/cityhash.CityHash128
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also, nothing really jumps out when looking at the &lt;a href=&#34;/images/2013-11-10-improving-cithhash-performance-by-go-profiling/after.svg&#34;&gt;post-change profile data graph&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Bloom Filter Benchmarks&lt;/h3&gt;

&lt;p&gt;Now that the changes are in, I went back and re-ran some of the bloom filter benchmarks. They look a lot more reasonable as well. Below is a comparison of the Scalable Bloom Filter. The post-change run is almost 2.5x faster than the pre-change run. Also, the post-change number (1442 ns/op) is a lot closer to some of the other hash functions (~1100 ns/op).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Scalable Bloom Filter
---------------------
BenchmarkBloomCityHash   1000000              1442 ns/op (after cityhash change)
BenchmarkBloomCityHash   1000000              3375 ns/op (before cityhash change)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;The Go authors have made it extermely simple to test, benchmark and profile Go programs so there&amp;rsquo;s really reason for anyone not to do that often. It helps you see how your program works and where the bottlenecks are. It can also help you identify surprises that you may not have though of. A good example is in my case, I had no idea binary.Read() works the way it works until I profiled my program.&lt;/p&gt;

&lt;p&gt;Comments/Feedback on &lt;a href=&#34;https://news.ycombinator.com/item?id=6710115&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qcygc/improving_cityhash_performance_by_go_profiling/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking Integer Compression in Go</title>
      <link>http://zhen.org/blog/benchmarking-integer-compression-in-go/</link>
      <pubDate>Fri, 11 Oct 2013 22:39:22 -0800</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/benchmarking-integer-compression-in-go/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6537688&#34;&gt;comments/feedback&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Updated post: &lt;a href=&#34;http://zhen.org/blog/go-vs-java-decoding-billions-of-integers-per-second/&#34;&gt;Go vs Java: Decoding Billions of Integers Per Second&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;Source code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Let me start by saying that I am not that happy with the performance numbers I got. It probably has more to do with my familiarity and expertise with Go than anything else. But still&amp;hellip;&lt;/li&gt;
&lt;li&gt;Having said that, I am pleasantly surprised how much faster some of the integer compression algorithms are compare to standard compression algorithms such as gzip, LZW and even snappy.&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s no one size fits all solution. There&amp;rsquo;s always tradeoffs between compression ratio and compression/decompression speed. So depending on the type of integer data and how fast you want to encode/decode, you will need to choose the right solution.&lt;/li&gt;
&lt;li&gt;Gzip does probably the best job in compression ratio but worst in terms of encoding/decoding performance.&lt;/li&gt;
&lt;li&gt;Delta BP32 (IntegratedBinaryPacking in JavaFastPFOR) performs the best (ratio, encoding/decoding speed) for sorted integer arrays such as timestamps.&lt;/li&gt;
&lt;li&gt;I would love it if someone can run these tests on a faster machine and send me the results. I would be happy to put them into the spreadsheet.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Go Learn Project #6 - Integer Compression&lt;/h3&gt;

&lt;p&gt;It&amp;rsquo;s been 4 weeks since my last post and I have been BUSY! My day job has been busier than ever (in a good way) and has taken over many of my nights and weekends. However, I&amp;rsquo;ve not forgotten the &amp;ldquo;Go Learn&amp;rdquo; project and continued to tinker with Go as I find time.&lt;/p&gt;

&lt;p&gt;For &amp;ldquo;Go Learn&amp;rdquo; project #6, I decided to port &lt;a href=&#34;https://github.com/lemire/JavaFastPFOR&#34;&gt;JavaFastPFOR&lt;/a&gt; over to Go. The JavaFastPFOR repo actually is a collection of integer encoding/decoding algorithms. However, the more interesting ones are the ones created by Daniel Lemire, including FastPFOR, BP32, BP128, etc. To borrow from the repo&amp;rsquo;s readme:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;[JavaFastPFOR] is a library to compress and uncompress arrays of integers very fast. The assumption is that most (but not all) values in your array use less than 32 bits. These sort of arrays often come up when using differential coding in databases and information retrieval (e.g., in inverted indexes or column stores).&lt;/p&gt;

&lt;p&gt;Some CODECs (&amp;ldquo;integrated codecs&amp;rdquo;) assume that the integers are in sorted orders. Most others do not.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Thank You, Daniel Lemire&lt;/h3&gt;

&lt;p&gt;As you may have noticed, this is the second Daniel Lemire project that I&amp;rsquo;ve ported over. The previous one is &lt;a href=&#34;http://zhen.org/blog/bitmap-compression-using-ewah-in-go/&#34;&gt;Bitmap Compression using EWAH in Go&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://lemire.me/&#34;&gt;Danile Lemire&lt;/a&gt; is a computer science professor at TELUQ (Universit du Qubec) where he teaches primarily online. He specializes in Databases, Data Warehousing and OLAP, Recommender Systems and Collaborative Filtering, and Information Retrieval.&lt;/p&gt;

&lt;p&gt;I won&amp;rsquo;t elaborate on how knowledgeable he is here because you can easily tell by reading his papers and blogs. I do want to mention how Daniel has been extremely helpful on my porting effort. He&amp;rsquo;s provided tremendous guidance and support, and went even as far as providing me access to one of his machines for running performance tests.&lt;/p&gt;

&lt;p&gt;So thank you Daniel!&lt;/p&gt;

&lt;h4 id=&#34;toc_3&#34;&gt;Decoding Billions of Integers per Second Through Vectorization&lt;/h4&gt;

&lt;p&gt;This project is inspired by Danile&amp;rsquo;s blog post, &lt;a href=&#34;http://lemire.me/blog/archives/2012/09/12/fast-integer-compression-decoding-billions-of-integers-per-second/&#34;&gt;Fast integer compression: decoding billions of integers per second&lt;/a&gt;, and &lt;a href=&#34;http://arxiv.org/abs/1209.2137&#34;&gt;paper&lt;/a&gt;. As the paper states:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In many important applicationssuch as search engines and relational database systemsdata is stored in the form of arrays of integers. Encoding and, most importantly, decoding of these arrays consumes considerable CPU time. Therefore, substantial effort has been made to reduce costs associated with compression and decompression&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As part of this research, Daniel also made his code available in &lt;a href=&#34;https://github.com/lemire/fastpfor&#34;&gt;C++&lt;/a&gt; and &lt;a href=&#34;https://github.com/lemire/JavaFastPFOR&#34;&gt;Java&lt;/a&gt;. The Go port is based on JavaFastPFOR.&lt;/p&gt;

&lt;p&gt;However, because Go has no access to the SSE instruction sets (well, not without getting into C or assembly), I was not able to port over the SIMD versions of the algorithms.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;The Port&lt;/h3&gt;

&lt;p&gt;The Go port is available on &lt;a href=&#34;https://github.com/reducedb/encoding&#34;&gt;github&lt;/a&gt;. In this version, I&amp;rsquo;ve proted over six algorithms, including&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FastPFOR&lt;/li&gt;
&lt;li&gt;BP32 (BinaryPacking in JavaFastPFOR)&lt;/li&gt;
&lt;li&gt;Delta BP32 (IntegratedBinaryPacking in JavaFastPFOR)&lt;/li&gt;
&lt;li&gt;ZigZag BP32 (BP32 with Delta encoding using &lt;a href=&#34;https://developers.google.com/protocol-buffers/docs/encoding#types&#34;&gt;ZigZag&lt;/a&gt; encoding method.)&lt;/li&gt;
&lt;li&gt;VariableBytes&lt;/li&gt;
&lt;li&gt;Delta VariableBytes (Integrated VariableBytes in JavaFastPFOR)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I won&amp;rsquo;t go into details of how each of these algorithms work. If you are interested, I strongly encourage you to go read Daniel&amp;rsquo;s paper.&lt;/p&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;The Benchmarks&lt;/h3&gt;

&lt;p&gt;To benchmark these algorithms, I&amp;rsquo;ve created 5 sets of data.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/synth/ClusteredDataGenerator.java&#34;&gt;Clustered&lt;/a&gt; - This is a generated list of random and sorted integers based on the clustered model.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/lemire/JavaFastPFOR/blob/master/src/main/java/me/lemire/integercompression/synth/UniformDataGenerator.java&#34;&gt;Uniform&lt;/a&gt; - This will generate a &amp;ldquo;uniform&amp;rdquo; list of sorted integers.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/reducedb/encoding/tree/master/samples&#34;&gt;Timestamps&lt;/a&gt; - This is a list of timestamps in sorted order, extracted from another data set that&amp;rsquo;s mainly network monitoring data. The timestamps are epoch time (seconds since 1970). It has long runs of the same timestamp because the dataset contains many entries per second.&lt;/li&gt;
&lt;li&gt;IP Addresses - This is a data set that contains a 32-bit integer representation of IPv4 addresses. This data set is NOT sorted.

&lt;ul&gt;
&lt;li&gt;In the real-world, one probably wouldn&amp;rsquo;t compress IP addresses like that. One would probably use dictionary encoding for the IPs, then encode the dictionary keys.&lt;/li&gt;
&lt;li&gt;The dictionary keys will likely be much smaller numbers, which means it will compress fairly well.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Latencies - This list of integers represent latencies on the network. It is also unsorted.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Timestamps, IP addresses, and Latencies are essentially 3 columns from another dataset.&lt;/p&gt;

&lt;p&gt;Along with benchmarking the integer encoding algorithms, I also benchmarked the Go implementations of Gzip, LZW and Snappy. Both Gzip and LZW are part of the Go standard library. Snappy is a &lt;a href=&#34;https://code.google.com/p/snappy-go/&#34;&gt;separate project&lt;/a&gt; implemented by the Go developers.&lt;/p&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;The Results&lt;/h3&gt;

&lt;p&gt;All benchmarks are performed on a machine with Intel&amp;reg; Xeon&amp;reg; CPU E5-2609 0 @ 2.40GHz.&lt;/p&gt;

&lt;h4 id=&#34;toc_7&#34;&gt;Compression Ratio (Lower is Better)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-10-11-benchmarking-integer-compression-in-go/Integer-Compression-Ratio.png&#34; alt=&#34;Compression Ratio&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Compression ratio is measured in bits/integer. Before compression, the integers we are compressing are all 32-bit integers. This chart shows us how many bits are used for each integer after compression.&lt;/p&gt;

&lt;p&gt;There are a few things you can observe from this chart:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sorted integer lists ALWAYS perform better (ratio and speed) than unsorted integer lists.&lt;/li&gt;
&lt;li&gt;Sometimes the compressed size is LARGER than the uncompressed size. This is because some of these algorithms use extra space to store encoding meta information.&lt;/li&gt;
&lt;li&gt;Gzip, in general, has the best compression ratio.&lt;/li&gt;
&lt;li&gt;Delta BP32 performs the best for sorted lists, but really has problems with unsorted lists.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;toc_8&#34;&gt;Compression Speed (Higher is Better)&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-10-11-benchmarking-integer-compression-in-go/Integer-Compression-Speed.png&#34; alt=&#34;Compression Speed&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Compression speed is measured in millions of integers per second (MiS). Note that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Delta BP32 by far has the best compression speed across different data sets.&lt;/li&gt;
&lt;li&gt;BP32 does a fairly decent job compressing as well, but its compression ratio is fairly poor.&lt;/li&gt;
&lt;li&gt;Gzip and LZW perfom the most poorly.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;toc_9&#34;&gt;Decompression Speed&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;/images/2013-10-11-benchmarking-integer-compression-in-go/Integer-Decompression-Speed.png&#34; alt=&#34;Decompression Ratio&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;Decompression speed is measured in millions of integers per second (MiS). Note that:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Again, Delta BP32 does the best across different data sets, and gzip/LZW did the poorest.&lt;/li&gt;
&lt;li&gt;BP32 decoded extremely fast for sorted timestamps that have large runs of the same timestamps.&lt;/li&gt;
&lt;li&gt;FastPFOR seems to perform the most consistently for integer compression algorithms. (For compression as well.)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_10&#34;&gt;The Conclusions&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;For encoding sorted numbers, Delta BP32 has the best combination of compression ratio, and encoding/decoding speed.&lt;/li&gt;
&lt;li&gt;For encoding unsorted numbers, Snappy seems like a good alternative.&lt;/li&gt;
&lt;li&gt;For long term storage, it might be worth considering gzip. It provides the best compression ratio for data that may not be accessed for a long while.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_11&#34;&gt;The Raw Result&lt;/h3&gt;

&lt;p&gt;The following is a Google spreadsheet that contains the raw result set.&lt;/p&gt;

&lt;iframe width=&#39;800&#39; height=&#39;500&#39; frameborder=&#39;0&#39; src=&#39;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdEwzMkJBNVQzWkxkOExTLThIbGlCSkE&amp;output=html&amp;widget=true&#39;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6537688&#34;&gt;comments/feedback&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bitmap Compression using EWAH in Go</title>
      <link>http://zhen.org/blog/bitmap-compression-using-ewah-in-go/</link>
      <pubDate>Sun, 15 Sep 2013 23:08:22 -0800</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/bitmap-compression-using-ewah-in-go/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6392197&#34;&gt;comments/suggestions&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;tl;dr&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;If you are interested in database bitmap index, then it&amp;rsquo;s definitely worth learning more about bitmap compression.&lt;/li&gt;
&lt;li&gt;EWAH is a very interesting way of compressing bitmaps and this article talks about my port of it to Go.&lt;/li&gt;
&lt;li&gt;Sorry I think you should read the rest of the article. :)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;The Real Deal&lt;/h3&gt;

&lt;p&gt;For &amp;ldquo;Go Learn&amp;rdquo; project #5, I&amp;rsquo;ve decided to continue implementing bit-related data structures. This time I decided to port a bitmap (not image) compression data structure to Go. Unlike bloom filters, which had quite a few implementations, I couldn&amp;rsquo;t really find any compressed bitmap implementations in Go. Most of them are in C/C++ or Java. (For previous projects please see previous blog posts.)&lt;/p&gt;

&lt;p&gt;Btw, if you have any suggestions on what project I should do next, feel free to drop me a comment.&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Bitmaps&lt;/h3&gt;

&lt;p&gt;Bitmap compression is often used for database &lt;a href=&#34;http://en.wikipedia.org/wiki/Bitmap_index&#34;&gt;bitmap indexing&lt;/a&gt;. To quote Wikipedia again:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A bitmap index is a special kind of database index that uses bitmaps. [&amp;hellip;]&lt;/p&gt;

&lt;p&gt;Bitmap indexes use bit arrays (commonly called bitmaps) and answer queries by performing bitwise logical operations on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional B-tree indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for online transaction processing applications. [&amp;hellip;]&lt;/p&gt;

&lt;p&gt;Some researchers argue that Bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are quite a few approaches to bitmap compression. You will find most of the work listed in the wikipedia page. The two I found to be most interesting are &lt;a href=&#34;http://ricerca.mat.uniroma3.it/users/colanton/docs/concise.pdf&#34;&gt;CONCISE&lt;/a&gt; and &lt;a href=&#34;https://code.google.com/p/javaewah/&#34;&gt;Enhanced Word-Aligned Hybrid (EWAH)&lt;/a&gt;. After reading throught both papers and looking at their Java implementations, I decided to start with EWAH. There&amp;rsquo;s no primary reason for starting with EWAH first. But Daniel Lemire&amp;rsquo;s comment intrigued me, &amp;ldquo;Out of the bitmap formats that I have often tested, Concise is the most concise, but JavaEWAH is often faster. So there is a trade-off.&amp;rdquo; I wanted to see how fast the Go implementation can be so I figure I start with EWAH. I may still port CONCISE later. (Note: I am not the best Go programmer nor do I claim that my code is the most performant. This is a learning project so there will definitely be room for improvement.)&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;EWAH&lt;/h3&gt;

&lt;p&gt;Enhanced Word-Aligned Hybrid (EWAH) is a bitmap compression algorithm created by Daniel Lemire, Owen Kaser, and Kamel Aouiche. (&lt;a href=&#34;http://arxiv.org/pdf/0901.3751v4.pdf&#34;&gt;paper&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;EWAH uses a method called &lt;a href=&#34;http://en.wikipedia.org/wiki/Run-length_encoding&#34;&gt;Run-Length Encoding&lt;/a&gt; to perform compression. RLE is extremely useful for dataset that tend to have long blocks of the same data. In the case of database bitmap indices, there maybe long blocks of 1&amp;rsquo;s or 0&amp;rsquo;s, depending on the sparsity of the data. In cases where sparsity is low, compression can be extremely high. We will show some data points on this later.&lt;/p&gt;

&lt;p&gt;EWAH uses two types of words where the first type is a 64-bit verbatim word. The second type of word is a marker word: the first bit indicates which clean word will follow, half the bits (32 bits) are used to store the number of clean words, and the rest of the bits (31 bits) are used to store the number of dirty words following the clean words. EWAH bitmaps begin with a marker word.&lt;/p&gt;

&lt;p&gt;Clean (empty) words are either all 0&amp;rsquo;s or all 1&amp;rsquo;s. Dirty (literal) words are verbatim words that have 1&amp;rsquo;s and 0&amp;rsquo;s. Blocks of clean words can be compressed using RLE. Dirty words cannot be compressed so they are represented vertatim.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;3210987654321098765432109876543210987654321098765432109876543210
0000000000000000000000000000010000000000000000000000000000000100
0000000000000000000001000000000000000000000000000000000000000001
0000000000000000000000000000000000000000001000000000000000000100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For example, the above bits show a marker word. Bits go from right to left. So the first bit being 0 means the clean/empty words all have their bits as 0. The next 32 bits indicate the number of clean words this is encoding. &amp;ldquo;10&amp;rdquo; in this case means this marker word encodes 2 clean words that are all 0&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;The final 31 bits (starting at position 33 from the right) indicates the number of literal words that will follow this marker word. In thise case, there are also 2 dirty (literal) words that follow the marker word.&lt;/p&gt;

&lt;p&gt;As you can see, the overall bitmap contains 3 64-bit words. However, it encodes 4 64-bit words. So there&amp;rsquo;s a space saving of 25%.&lt;/p&gt;

&lt;p&gt;For more detailed information please read the paper. It&amp;rsquo;s definitely worth the read if you are trying to understand bitmap compression.&lt;/p&gt;

&lt;p&gt;There are a few caveats to EWAH:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;There is no random access to any single bit, unlike a regular bit array. To access a random bit, you need to walk through the whole bitmap and find the word that contains the bit.&lt;/li&gt;
&lt;li&gt;You cannot set a bit that&amp;rsquo;s earlier than the last set bit. For example, if you have already set bit 100, you can not go back and set bit 99.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Technically you can solve both of these problems, but the cost of doing such operations will be extremely high.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Go Port&lt;/h3&gt;

&lt;p&gt;Most of my nights and weekends over the past couple of weeks have gone into the porting of JavaEWAH to Go. There&amp;rsquo;s also a C++ version of EWAH which I&amp;rsquo;ve also gone through. Code structure wise it&amp;rsquo;s very similar to the Java version. You can find the &lt;a href=&#34;https://github.com/reducedb/bitmap&#34;&gt;Go EWAH port on Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is one of the more inovled ports I&amp;rsquo;ve ported to Go because it involved quite a few more classes and also the logic required much more understanding. With previous projects like &lt;a href=&#34;https://github.com/reducedb/cityhash&#34;&gt;Cityhash&lt;/a&gt; and &lt;a href=&#34;https://github.com/reducedb/bloom&#34;&gt;Bloom Filter&lt;/a&gt;, I can get away with either translating directly without complete understanding (in the case of Cityhash) or the logic is fairly simple and can be coded up pretty easily (bloom filters.)&lt;/p&gt;

&lt;p&gt;I wanted to be able to implement multiple bitmap compression algorithms down the road, so I started with a simple &lt;a href=&#34;https://github.com/reducedb/bitmap/blob/master/bitmap.go&#34;&gt;bitmap interface&lt;/a&gt;. It contains most the basic bit operations such as Set, Get, And, Not, Or, Xor, AndNot, Cardinality, etc. I then went through the Java source code and ported everything that&amp;rsquo;s required to implement these functions.&lt;/p&gt;

&lt;p&gt;What this means is that my Go version is NOT a complete port of JavaEWAH. There are some java classes and methods I chose not to port as they weren&amp;rsquo;t required for satisfying the bit operations.&lt;/p&gt;

&lt;p&gt;Initially I ported the classes over exactly as Java version had them. This included classes such as RunningLengthWord, BufferedRunningLengthWord, EWAHIterator, IteratingBufferedRunningLengthWord. These classes formed the bases for iterating over the bitmap and they are heavily used throughout the implementation for all the bit operations. Iterating over a bitmap usually involves multiple layers of nested iterators.&lt;/p&gt;

&lt;p&gt;What I found is that in my Go port using this same approach was running fairly slow as there are multiple nested iterators. (Results later.) Now to be completely honest and fair, this does not mean the Java version is also slow. It&amp;rsquo;s just the &lt;strong&gt;My Go version&lt;/strong&gt; is slow. As I have learned over the past few projects, there are always ways to make things faster. I tried to apply what I&amp;rsquo;ve learned in previous projects in these new projects but it certainly doesn&amp;rsquo;t mean I always succeed.&lt;/p&gt;

&lt;h4 id=&#34;toc_5&#34;&gt;Cursors&lt;/h4&gt;

&lt;p&gt;After porting the Java version to Go directly, I learned a ton about the structure of the algorithm and how the different bit operations are performed. Given that my Go version was running fairly slow, I decided to try a different approach. Instead of the nested iterator approach, I implemented a Cursor structure that basically collapsed RunningLengthWord, BufferedRunningLengthWord, IteratingBufferedRunningLengthWord, and EWAHIterator into a single structure.&lt;/p&gt;

&lt;p&gt;Whenever I need to iterate over the bitmap, I create a cursor (you can call it a iterator if you like but it&amp;rsquo;s just a single layer with no nesting) and use that to keep track of where I am. Then I loop over the bitmap and perform the necessary operations.&lt;/p&gt;

&lt;p&gt;In many cases during my benchmarks, the cursor-based performance is 2-4x faster over the previous approach.  I took great care to make sure the results of the two different implementations return exactly the same thing. You can see some of that in my tests.&lt;/p&gt;

&lt;h4 id=&#34;toc_6&#34;&gt;Get&lt;/h4&gt;

&lt;p&gt;The original Java implementation did not have a Get function, which gets the bit at any random location. This is because bitmap implementation such as CONCISE and EWAH are meant to be used with bitmap indices and in most cases you don&amp;rsquo;t need to check a random bit. The common use cases are to perform bit operations on two bitmaps, then find all the bits that are set.&lt;/p&gt;

&lt;p&gt;Regardless, I wanted to have a Get function duing my testing so I can check to see the bits I set are indeed set correctly. So I did a quick Get implemention by looping over the bitmap. Daniel was gracious enough to convert my Go implementation back into Java and &lt;a href=&#34;https://github.com/lemire/javaewah/blob/master/src/main/java/com/googlecode/javaewah/EWAHCompressedBitmap.java#L1015&#34;&gt;incorporated it into JavaEWAH&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However, walking the bitmap each and every time we want to check a bit is way way way too slow. In some of the use cases I am thinking about I tend to sequentially check bits. For example, I will check bits 100, 159, 302, 405, etc. The bits are almost always increasing.&lt;/p&gt;

&lt;p&gt;So taking a page from the &lt;a href=&#34;https://github.com/reducedb/skiplist&#34;&gt;skiplist&lt;/a&gt; implementation I did a few weeks back, I implemented Get with Fingers. The Finger concept is fairly simple. We basically keep a finger on the last checked word, and if the new bit to check is in or after the last checked word, we just move forward from there. If the new bit is BEFORE the current word, we restart from the beginning. The actually Get implementation keeps a getCursor as a finger to the last checked word.&lt;/p&gt;

&lt;p&gt;The result is quite an improvement for sequential checks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Get with Finger and Cursor - 125 ns/op&lt;/li&gt;
&lt;li&gt;Get without Finger but uses Cursor - 60542 ns/op&lt;/li&gt;
&lt;li&gt;Get using the nested iterators approach - 822530 ns/op&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;Results&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/reducedb/bitmap&#34;&gt;final product&lt;/a&gt; of this project is available on Github. I ran quite a few benchmarks with different bitmap density (1-sparsity) as well as running bit operations with two bitmaps of varying density. Here are some points to note.&lt;/p&gt;

&lt;h4 id=&#34;toc_8&#34;&gt;Density vs Space Saving&lt;/h4&gt;

&lt;p&gt;The whole reason for bitmap compression to exist is to save space. So here are some results from that test. Probably not surprisingly, the higher the density, the lower the space savings. The charte below shows how density and space savings correlate for a bitmap that had a cardinality of 10,000.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docs.google.com/spreadsheet/oimg?key=0ApDLtJuUH-1rdHh2bzY1d0h3U042UXNkLUlzaENLMUE&amp;oid=1&amp;zx=510m225jyk7b&#34; /&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Space Saving&lt;/th&gt;
&lt;th&gt;Bit Density&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;-0.62%&lt;/td&gt;
&lt;td&gt;50.39%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;-0.04%&lt;/td&gt;
&lt;td&gt;6.46%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;32.68%&lt;/td&gt;
&lt;td&gt;0.67%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;91.61%&lt;/td&gt;
&lt;td&gt;0.07%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;99.15%&lt;/td&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;My test data set is generated fairly uniformly. To get different density, I generate a random number between 1-N, where N is the number at the left-most column in the table. So on average the distance between bits is N/2, well, approximately. This may not be the best real-world scenario so take it with a grain of salt. But it does demonstrate how well EWAH can compress.&lt;/p&gt;

&lt;p&gt;At the bottom, you will find the full list of bitmaps I generated for the tests. (Look at the bitmaps tab)&lt;/p&gt;

&lt;h4 id=&#34;toc_9&#34;&gt;Cursor vs Nested Iterators&lt;/h4&gt;

&lt;p&gt;Another interesting benchmark I did was test the performance between the cursor-based implementation and the nested-iterators implementation. I tested the two implementations using a wide variaty of combinations, totaling 225 benchmarks per bit operation (AND, OR, XOR, ANDNOT).&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/zhenjl/6577789.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The above gist is the shell script that generates the wrappers for calling a benchmark function in Go. Btw, I love the testing package for Go as it makes it really easy for me to create tests and benchmarks. However, not having the ability to run benchmarks with different parameters without creating wrappers like these is a huge pain the butt.&lt;/p&gt;

&lt;p&gt;The result is quite telling. The cursor-based implementation is 2-4 times faster than the nested-iterator version. See the &amp;ldquo;Performance Results&amp;rdquo; tab below.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;AND/OR/XOR/ANDNOT 1 - This is the cursor-based implementation&lt;/li&gt;
&lt;li&gt;AND/OR/XOR/ANDNOT 2 - This is the nested-iterator-based implementation&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;toc_10&#34;&gt;Embedded Spreadsheet&lt;/h4&gt;

&lt;iframe width=&#39;800&#39; height=&#39;500&#39; frameborder=&#39;0&#39; src=&#39;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdHh2bzY1d0h3U042UXNkLUlzaENLMUE&amp;output=html&amp;widget=true&#39;&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6392197&#34;&gt;comments/suggestions&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking Bloom Filters and Hash Functions in Go</title>
      <link>http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/</link>
      <pubDate>Wed, 04 Sep 2013 00:23:22 -0800</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6329616&#34;&gt;discussion/comments&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Update: @bradfitz &lt;a href=&#34;https://news.ycombinator.com/item?id=6329616&#34;&gt;commented&lt;/a&gt; that allocating a new slice during each add/check is probably not good for performance. And he is in fact correct. After removing the allocation for each add/check and doing a single slice make during New(), the performance increased by ~27%!! Here&amp;rsquo;s the gist containing the new ns/op results.&lt;/p&gt;

&lt;p&gt;{% gist 6515577 %}&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Another week, another &amp;ldquo;Go Learn&amp;rdquo; project. This time, in project #4, I implemented a &lt;a href=&#34;https://github.com/reducedb/bloom&#34;&gt;Go package&lt;/a&gt; with several bloom filters and &lt;a href=&#34;https://gist.github.com/zhenjl/6433634&#34;&gt;benchmarked&lt;/a&gt; them with various hash functions. (For previous projects, see &lt;a href=&#34;http://zhen.org/blog/go-skiplist/&#34;&gt;#3&lt;/a&gt;, &lt;a href=&#34;http://zhen.org/blog/testing-msgpack-and-bson/&#34;&gt;#2&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/cityhash&#34;&gt;#1&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;The goal of this project, for me at least, is to implement a pure Go package that doesn&amp;rsquo;t rely on wrappers to other langagues. There&amp;rsquo;s already &lt;a href=&#34;https://github.com/search?l=Go&amp;amp;q=bloom&amp;amp;ref=cmdform&amp;amp;type=Repositories&#34;&gt;quite a few&lt;/a&gt; bloom filters implemented in Go. But hey, in the name of learning, why not implement another one!&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Bloom Filters&lt;/h3&gt;

&lt;p&gt;Wikipedia says,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A Bloom filter, conceived by Burton Howard Bloom in 1970 is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not; i.e. a query returns either &amp;ldquo;inside set (may be wrong)&amp;rdquo; or &amp;ldquo;definitely not in set&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In my little project, I implemented the following three variants:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Standard - This is the classic bloom filter as described on &lt;a href=&#34;http://en.wikipedia.org/wiki/Bloom_filter&#34;&gt;Wikipedia&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Partitioned - This is a variant described by &lt;a href=&#34;http://www.ieee-infocom.org/2004/Papers/45_3.PDF&#34;&gt;these&lt;/a&gt; &lt;a href=&#34;http://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf&#34;&gt;papers&lt;/a&gt;. Basically instead of having a single big bit array, partitioned bloom filter breaks it into &lt;em&gt;k&lt;/em&gt; partitions (or slices) s.t. each partition is &lt;em&gt;m/k&lt;/em&gt; size, where &lt;em&gt;m&lt;/em&gt; is the total number of bits, and &lt;em&gt;k&lt;/em&gt; is the number of hashes. Then each hash function is assigned to a single partition.&lt;/li&gt;
&lt;li&gt;Scalable - This is yet another variant described by &lt;a href=&#34;http://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf&#34;&gt;here&lt;/a&gt;). The idea is that the standard bloom filter requires that you know &lt;em&gt;m&lt;/em&gt;, or the size of the filter a priori. This is not possible for use cases where data continue to come in without bound. So the Scalable bloom filter utilizes multiple bloom filters, each with incresing k, but decreasing &lt;em&gt;P&lt;/em&gt; where &lt;em&gt;P&lt;/em&gt; is the desired error probability. This bloom filter also introduces &lt;em&gt;r&lt;/em&gt;, which is the error tightening ratio, and it&amp;rsquo;s 0 &amp;lt; r &amp;lt; 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are a ton more variants for bloom filters. You can raed all about them in &lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=5&amp;amp;cad=rja&amp;amp;ved=0CE4QFjAE&amp;amp;url=http%3A%2F%2Fwww.tribler.org%2Ftrac%2Fraw-attachment%2Fwiki%2FJelleRoozenburg%2Fresearch_assignment_jroozenburg_20051108.pdf&amp;amp;ei=JHInUt6oO6GTiQK-44DYDg&amp;amp;usg=AFQjCNG057WVJ2m2QYPuqWzCZ0Vn4JnOug&amp;amp;sig2=NVlad7xGO4S_hFpMU9apGA&amp;amp;bvm=bv.51773540,d.cGE&#34;&gt;this paper&lt;/a&gt; and &lt;a href=&#34;http://www.dca.fee.unicamp.br/~chesteve/pubs/bloom-filter-ieee-survey-preprint.pdf&#34;&gt;this paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Hash Functions&lt;/h3&gt;

&lt;p&gt;To add an element, bloom filters hashes the element using &lt;em&gt;k&lt;/em&gt; hashing functions, identifying &lt;em&gt;k&lt;/em&gt; positions in the bit array, and setting those bits to 1. To check for an element, you essentially do the same thing (hash the element with &lt;em&gt;k&lt;/em&gt; hash functions) and then check to see if all the bits at those positions are set. If all set, then most likely the element is available. If any of them are not set, then the element is definitely not avaiable. So bloom filters can have false positives, but not false negatives.&lt;/p&gt;

&lt;p&gt;However, actually running &lt;em&gt;k&lt;/em&gt; hash functions is quite expensive. So &lt;a href=&#34;http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/rsa.pdf&#34;&gt;Kirsch and Mitzenmacher&lt;/a&gt; determined that by using a single hash function, but using the formula, &lt;em&gt;gi(x) = h1(x) + i * h2(x)&lt;/em&gt;, to calculate &lt;em&gt;k&lt;/em&gt; hash values, the performance is actually similar. So this is what I used here.&lt;/p&gt;

&lt;p&gt;For the values &lt;em&gt;h1&lt;/em&gt; and &lt;em&gt;h2&lt;/em&gt;, we used the first 4 bytes returned from each hash function for &lt;em&gt;h1&lt;/em&gt;, and second 4 bytes for &lt;em&gt;h2&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The following hash functions are used for this little experiement:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FNV-64 - There&amp;rsquo;s a built-in Go package for this. So that&amp;rsquo;s what I used.&lt;/li&gt;
&lt;li&gt;CRC-64 - Again, using the built-in Go package.&lt;/li&gt;
&lt;li&gt;Murmur3-64 - There&amp;rsquo;s no built-in package so I used &lt;a href=&#34;https://github.com/spaolacci/murmur3&#34;&gt;this one&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CityHash-64 - Again, no built-in so I am using the one &lt;a href=&#34;https://github.com/reducedb/cityhash&#34;&gt;I implemented&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;MD5 - Using the built-in Go implementation.&lt;/li&gt;
&lt;li&gt;SHA1 - Using the built-in Go implementation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a side note, MD5 and SHA1 return 128 bit hash values. Since we only use the first 64 bits, we throw away the last 64 bits.&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Test Setup&lt;/h3&gt;

&lt;p&gt;The machine that ran these tests is a Macbook Air 10.8.4 1.8GHz Intel Core i5 4GB 1600MHz DDR3.&lt;/p&gt;

&lt;p&gt;For the false positive test, I added all the words in /usr/share/dict/web2 (on my Mac) into each of the bloom filters. To check for false positives, I check for all the words in /usr/share/dict/web2a in each of the bloom filters. The two files should have completely different set of words (AFAICT).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  235886 /usr/share/dict/web2
   76205 /usr/share/dict/web2a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each bloom filter, I ran tests using the following combinations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i in element_count[236886, 200000, 100000, 50000]
    for j in hash_functions[fnv64, crc64, murmur3-64, cityhash-64, md5, sha1]
        run test with hash_functions[j] and element_count[i]
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Element count in this is the initial size I set for the bloom filter. The bit array size, &lt;em&gt;m&lt;/em&gt;, and number of hash values, &lt;em&gt;k&lt;/em&gt;, are then calculated from there. I also set &lt;em&gt;P&lt;/em&gt; (error probability) to 0.001 (0.1%) and &lt;em&gt;p&lt;/em&gt; (fill ratio, or how full should the bit array be) to 0.5. The idea for the element count is to see how the bloom filters will perform when it has a high fill ratio.&lt;/p&gt;

&lt;p&gt;For the scalable bloom filter test, I also needed to add another dimension since it uses either standard or partitioned bloom filter internally. So&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i in element_count[236886, 200000, 100000, 50000]
    for j in hash_functions[fnv64, crc64, murmur3-64, cityhash-64, md5, sha1]
        for k in bloom_filter[standard, partitioned]
            run test with hash_functions[j] and element_count[i] and bloom_filter[k]
        end
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the performance test, I added all the words in web2 into the bloom filters. I continue to loop through the file until b.N (Go benchmark framework) is met. So some of the words will be re-added, which should not skew our test results since the operations are the same.&lt;/p&gt;

&lt;p&gt;You can see the tests in the &lt;a href=&#34;https://github.com/reducedb/bloom&#34;&gt;github repo&lt;/a&gt;. Just look for all the _test.go files.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Test Results&lt;/h3&gt;

&lt;p&gt;The following is a summary of the test results. You can also feel free to look at all the &lt;a href=&#34;https://gist.github.com/zhenjl/6433634&#34;&gt;gory details&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note: FR = Fill Ratio, FP = False Positive&lt;/p&gt;

&lt;p&gt;For the spreadsheet embedded below, here are some observations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The MD5 Go implementation I used maybe broken, or I am not using it correctly, or MD5 is just bad. You will see that the fill ration is VERY low, 1-6%. So the false positive rate is very high (89+%).&lt;/li&gt;
&lt;li&gt;The CityHash Go implementation seems very slow. Could just be my implementation (anyone want to venture some optimization?). But functionally it&amp;rsquo;s correct.&lt;/li&gt;
&lt;li&gt;Both standard and partitioned bloom filters use the same number of bits and there&amp;rsquo;s not a huge difference in fill ratio and false positive rate. (Ignoring the MD5 anomaly.)&lt;/li&gt;
&lt;li&gt;Predictably, as fill ratio goes up, so does the false positive rate for both standard and partitioned bloom filters.&lt;/li&gt;
&lt;li&gt;Scalable bloom filter uses more bits as it contines to add new base bloom filters when the estimated fill ratio reaches &lt;em&gt;P&lt;/em&gt; which is set to 0.5 (50%).&lt;/li&gt;
&lt;li&gt;Probably not surprisingly, Scalable bloom filter maintains a fairly low false positive rate.&lt;/li&gt;
&lt;li&gt;However, you will also notice that the Scalable FP increases as the total number of base filters increase. This suggests that I may want to try a lower &lt;em&gt;r&lt;/em&gt; (error tightening ratio). Currently I use 0.9, but maybe 0.8 is more appropriate.&lt;/li&gt;
&lt;li&gt;Overall it seems FNV is probably good enough for most scenarios.&lt;/li&gt;
&lt;li&gt;Also Scalable+Standard might be a good choice for anyone doing stream data processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe width=&#39;800&#39; height=&#39;600&#39; frameborder=&#39;0&#39; src=&#39;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdEEwZ3JwaVVTZGxBX1g1NkthSFVVTXc&amp;single=true&amp;gid=0&amp;output=html&amp;widget=true&#39;&gt;&lt;/iframe&gt;

&lt;p&gt;This chart shows the performance (ns/op) for adding elements to the bloom filters. Overall the performance is very similar for the different bloom filter implementations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docs.google.com/spreadsheet/oimg?key=0ApDLtJuUH-1rdEEwZ3JwaVVTZGxBX1g1NkthSFVVTXc&amp;oid=1&amp;zx=jbizsaoepc5x&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Feedback&lt;/h3&gt;

&lt;p&gt;Feel free to send me any feedback via &lt;a href=&#34;https://github.com/reducedb/bloom/issues&#34;&gt;github issues&lt;/a&gt; or on &lt;a href=&#34;https://news.ycombinator.com/item?id=6329616&#34;&gt;Hacker News&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;During this process I referenced and leveraged some of these other projects, so thank you all!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Referenced

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/willf/bloom&#34;&gt;willf/bloom&lt;/a&gt; - Go package implementing Bloom filters&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitly/dablooms&#34;&gt;bitly/dablooms&lt;/a&gt; - scaling, counting, bloom filter library&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s also a bunch of papers, some of which I linked above.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Leveraged

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/willf/bitset&#34;&gt;willf/bitset&lt;/a&gt; - Go package implementing bitsets&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spaolacci/murmur3&#34;&gt;spaolacci/murmur3&lt;/a&gt; - Native MurmurHash3 Go implementation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6329616&#34;&gt;discussion/comments&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go Skiplist</title>
      <link>http://zhen.org/blog/go-skiplist/</link>
      <pubDate>Mon, 02 Sep 2013 13:09:22 -0800</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/go-skiplist/</guid>
      <description>

&lt;p&gt;Most of my nights and weekends in the past week have been immersed in my &lt;a href=&#34;https://github.com/reducedb/skiplist&#34;&gt;new skiplist library&lt;/a&gt;, even sacrificing some family time (yikes, not good!). It&amp;rsquo;s one of those projects I picked up in order to learn &lt;a href=&#34;http://golang.org&#34;&gt;Go&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Wikipedia defines a skip list as&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A skip list is a data structure for storing a sorted list of items using a hierarchy of linked lists that connect increasingly sparse subsequences of the items. These auxiliary lists allow item lookup with efficiency comparable to balanced binary search trees (that is, with number of probes proportional to log n instead of n).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You can imagine using a skiplist data structure whenver you want to use a binary search tree.&lt;/p&gt;

&lt;p&gt;This skiplist implementation also uses search fingers, based on William Pugh&amp;rsquo;s work, &lt;a href=&#34;http://drum.lib.umd.edu/bitstream/1903/544/2/CS-TR-2286.1.pdf&#34;&gt;A Skiplist Cookbook&lt;/a&gt;. So the efficiency is O(log k) where k is the distance between the last searched/updated item and the current one.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;{% gist e8779545c102c69aae10 %}&lt;/p&gt;

&lt;p&gt;These numbers are from my Macbook Air 10.8.4 1.8GHz Intel Core i5 4GB 1600MHz DDR3.&lt;/p&gt;

&lt;p&gt;Notice the fastest time is BenchmarkInsertTimeDescending. This is because the keys for that test is generated using time.Now().UnixNano(), which is always ascending. And because the sort order of the skiplist is descending, so the new item is ALWAYS inserted at the front of the list. This happens to have the best case of O(1).&lt;/p&gt;

&lt;p&gt;The next best time is BenchmarkInsertTimeAscending. This is still pretty good, but because the sort order is ascending, so the new items are ALWAYS inserted at the end. This required the skiplist to walk all the levels so it took a bit longer.&lt;/p&gt;

&lt;p&gt;The other benchmarks should have the average O(log k) efficiency.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Example (Int)&lt;/h3&gt;

&lt;p&gt;You can see additional examples in the &lt;a href=&#34;https://github.com/reducedb/skiplist/blob/master/skiplist_test.go&#34;&gt;test file&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Creating a new skiplist, using the built-in Less Than function as the comparator.
// There are also two other built in comparators: BuiltinGreaterThan, BuiltinEqual
list := New(skiplist.BuiltinLessThan)

// Inserting key, value pairs into the skiplist. The skiplist is sorted by key,
// using the comparator function to determine order
list.Insert(1,1)
list.Insert(1,2)
list.Insert(2,1)
list.Insert(2,2)
list.Insert(2,3)
list.Insert(2,4)
list.Insert(2,5)
list.Insert(1,3)
list.Insert(1,4)
list.Insert(1,5)

// Selecting items that have the key == 1. Select returns a Skiplist.Iterator
rIter, err := list.Select(1)

// Iterate through the list of items. Keys and Values are turned as interface{}, so you
// need to type assert them to your type
for rIter.Next() {
	fmt.Println(rIter.Key().(int), rIter.Value().(int))
}

// Delete the items that match key. An iterator is returned with the list of deleted items.
rIter, err = list.Delete(1)

// You can also SelectRange or DeleteRange
rIter, err = list.SelectRange(1, 2)

rIter, err = list.DeleteRange(1, 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Bultin Comparators&lt;/h3&gt;

&lt;p&gt;There are three built-in comparator functions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;BuiltinLessThan: if you want to sort the skiplist in ascending order&lt;/li&gt;
&lt;li&gt;BuiltinGreaterThan: if you want to sort the skiplist in descending order&lt;/li&gt;
&lt;li&gt;BuiltinEqual: just to compare&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Currently these built-in comparator functions work for all built-in Go types, including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;string&lt;/li&gt;
&lt;li&gt;uint64, uint32, uint16, uint8, uint&lt;/li&gt;
&lt;li&gt;int64, int32, int16, int8, int&lt;/li&gt;
&lt;li&gt;float32, float64&lt;/li&gt;
&lt;li&gt;unitptr&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6317109&#34;&gt;discussion/comments&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Testing MsgPack and BSON</title>
      <link>http://zhen.org/blog/testing-msgpack-and-bson/</link>
      <pubDate>Wed, 28 Aug 2013 14:20:22 -0800</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/testing-msgpack-and-bson/</guid>
      <description>&lt;p&gt;In one of the projects I am working on we are trying to select a message encoding scheme. The use case is pretty simple. We have a server that accepts data records from different clients, and our goal is support 100K messages per second per node. Our selection criteria are pretty simple as well:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compactness: how compact is the post-encoding byte array&lt;/li&gt;
&lt;li&gt;Performance: how fast is marshalling and unmarshalling of data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Out of the various encoding schemes out there, including &lt;a href=&#34;http://bsonspec.org&#34;&gt;BSON&lt;/a&gt;, &lt;a href=&#34;http://msgpack.org&#34;&gt;MessagePack&lt;/a&gt;, ProtoBuf, Thrift, etc etc, we decided to test BSON and MessagePack due to their closeness to JSON (we use JSON format quite a bit internally).&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a very short and simple &lt;a href=&#34;http://golang.org&#34;&gt;Go&lt;/a&gt; program we wrote to test the performance. The libraries I am using are &lt;a href=&#34;http://github.com/ugorji/go/codec&#34;&gt;Go codec&lt;/a&gt; and &lt;a href=&#34;http://labix.org/gobson&#34;&gt;Go BSON library&lt;/a&gt;. (I am sure one can argue that the library used affects the result, which I would agree. However, these are the best libraries for Go AFAICT.)&lt;/p&gt;

&lt;p&gt;The results are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For marshalling, BSON is about 15-20% slower than MsgPack.&lt;/li&gt;
&lt;li&gt;For Unmarshalling, BSON is 300% slower than MsgPack.&lt;/li&gt;
&lt;li&gt;Size-wise, BSON is about 20% more than MsgPack.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So looks like msgpack is the way to go!&lt;/p&gt;

&lt;p&gt;Special thanks(!) to realrocker in #go-nuts and &lt;a href=&#34;https://github.com/ugorji&#34;&gt;Ugorji&lt;/a&gt; for their help in cleaning up my code and helping me figure out my problems.&lt;/p&gt;

&lt;p&gt;Some notes about the codec library:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;According to Ugorji, decoding without schema in the codec library useslargest type for decoding, i.e. int64, uint64, float64. So if you don&amp;rsquo;t have a schema, the result will not pass reflect.DeepEqual test unless you initially type convert to those types. This is documented &lt;a href=&#34;https://github.com/ugorji/go/commit/7d844bb938783105c48aa9f4a34663f7d4190c32&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ugorji also quickly fixed a bug where &amp;ldquo;codec should be able to decode into a byte slice from both msgpack str/raw format or msgpack bin format (new in finalized spec).&amp;rdquo; AWESOME response time!&lt;/li&gt;
&lt;li&gt;To get string support (encode/decode strings), make sure RawToString is set to true (see below)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6293642&#34;&gt;discussion&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;{% gist 6371433 %}&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>