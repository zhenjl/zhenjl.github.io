<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Papers on Zen 3.1</title>
    <link>http://localhost:1313/categories/papers/</link>
    <description>Recent content in Papers on Zen 3.1</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Jian Zhen. All Rights Reserved.</copyright>
    <lastBuildDate>Sun, 22 Feb 2015 19:57:56 -0800</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/papers/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Papers I Read: 2015 Week 8</title>
      <link>http://localhost:1313/blog/papers-i-read-2015-week-8/</link>
      <pubDate>Sun, 22 Feb 2015 19:57:56 -0800</pubDate>
      
      <guid>http://localhost:1313/blog/papers-i-read-2015-week-8/</guid>
      <description>

&lt;h3 id=&#34;random-ramblings:29724b4ec2d333ad12e89fd98257dccd&#34;&gt;Random Ramblings&lt;/h3&gt;

&lt;p&gt;Another week, another report of hacks. This time, &lt;a href=&#34;http://securelist.com/blog/research/68732/the-great-bank-robbery-the-carbanak-apt/&#34;&gt;The Great Bank Robbery&lt;/a&gt;, where up to 100 financial institutions have been hit.Total financial losses could be as a high as $1bn. You can download the &lt;a href=&#34;http://25zbkz3k00wn2tp5092n6di7b5k.wpengine.netdna-cdn.com/files/2015/02/Carbanak_APT_eng.pdf&#34;&gt;full report&lt;/a&gt; and learn all about it.&lt;/p&gt;

&lt;p&gt;Sony spent $15M to clean up and remediate their hack. I wonder how much these banks are going to spend on tracing the footsteps of their intruders and trying to figure out exactly where they have gone, what they have done and what they have taken.&lt;/p&gt;

&lt;p&gt;I didn&amp;rsquo;t make much progress this week on either &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; or &lt;a href=&#34;https://github.com/surgemq/surgemq&#34;&gt;surgemq&lt;/a&gt; because of busy work schedule and my son getting sick AGAIN!! But I did merge the few surgemq &lt;a href=&#34;https://github.com/surgemq/surgemq/pulls?q=is%3Apr+is%3Aclosed&#34;&gt;pull requests&lt;/a&gt; that the community has graciously contributed. One of them actually got it tested on Raspberry! That&amp;rsquo;s pretty cool.&lt;/p&gt;

&lt;p&gt;I also did manage to finish up the &lt;a href=&#34;https://github.com/strace/sequence/commit/713979f70d6025308e434205973249aa3138e58e&#34;&gt;experimental json scanner&lt;/a&gt; that I&amp;rsquo;ve been working on for the past couple of weeks. I will write more about it in the next &lt;a href=&#34;http://strace.io/sequence&#34;&gt;sequence&lt;/a&gt; article.&lt;/p&gt;

&lt;p&gt;Actually I am starting to feel a bit overwhelmed by having both projects. Both of them are very interesting and I can see both move forward in very positive ways. Lots of ideas in my head but not enough time to do them. Now that I am getting feature requests, issues and pull requests, I feel even worse because I haven&amp;rsquo;t spent enough time on them. &amp;lt;sigh&amp;gt;&lt;/p&gt;

&lt;h3 id=&#34;papers-i-read:29724b4ec2d333ad12e89fd98257dccd&#34;&gt;Papers I Read&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.pdl.cmu.edu/PDL-FTP/HECStorage/git-cercs-12-08.pdf&#34;&gt;Memory-Efficient GroupBy-Aggregate using Compressed Buffer Trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Memory is rapidly becoming a precious resource in many data processing environments. This paper introduces
a new data structure called a Compressed Buffer Tree (CBT). Using a combination of buffering, compression,
and lazy aggregation, CBTs can improve the memoryefficiency of the GroupBy-Aggregate abstraction which
forms the basis of many data processing models like MapReduce and databases. We evaluate CBTs in the
context of MapReduce aggregation, and show that CBTs can provide significant advantages over existing hashbased
aggregation techniques: up to 2× less memory and 1.5× the throughput, at the cost of 2.5× CPU.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.usenix.org/system/files/conference/atc14/atc14-paper-hu.pdf&#34;&gt;ELF: Efficient Lightweight Fast Stream Processing at Scale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Stream processing has become a key means for gaining rapid insights from webserver-captured data. Challenges
include how to scale to numerous, concurrently running streaming jobs, to coordinate across those jobs to share
insights, to make online changes to job functions to adapt to new requirements or data characteristics, and for each job, to efficiently operate over different time windows. The ELF stream processing system addresses these new challenges. Implemented over a set of agents enriching the web tier of datacenter systems, ELF obtains scalability by using a decentralized “many masters” architecture where for each job, live data is extracted directly from webservers, and placed into memory-efficient compressed buffer trees (CBTs) for local parsing and temporary storage, followed by subsequent aggregation using shared reducer trees (SRTs) mapped to sets of worker processes. Job masters at the roots of SRTs can dynamically customize worker actions, obtain aggregated results for end user delivery and/or coordinate with other jobs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html&#34;&gt;Is Parallel Programming Hard, And, If So, What Can You Do About It?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not just a paper, it&amp;rsquo;s a whole book w/ 800+ pages.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The purpose of this book is to help you program shared-memory parallel machines without risking your sanity.1 We hope that this book’s design principles will help you avoid at least some parallel-programming pitfalls. That said, you should think of this book as a foundation on which to build, rather than as a completed cathedral. Your mission, if you choose to accept, is to help make further progress in the exciting field of parallel programming—progress that will in time render this book obsolete. Parallel programming is not as hard as some say, and we hope that this book makes your parallel-programming projects easier and more fun.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Papers I Read: 2015 Week 7</title>
      <link>http://localhost:1313/blog/papers-i-read-2015-week-7/</link>
      <pubDate>Sun, 15 Feb 2015 19:57:56 -0800</pubDate>
      
      <guid>http://localhost:1313/blog/papers-i-read-2015-week-7/</guid>
      <description>

&lt;h2 id=&#34;random-ramblings:10248ddcf524aa32198bc5cded1aa098&#34;&gt;Random Ramblings&lt;/h2&gt;

&lt;p&gt;Well, another week, &lt;a href=&#34;http://www.nytimes.com/2015/02/05/business/hackers-breached-data-of-millions-insurer-says.html&#34;&gt;another big data breach&lt;/a&gt;. This time is Anthem, one of the nation’s largest health insurers. Ok, maybe it was last week that it happend. But this week they revealed that &lt;a href=&#34;http://consumerist.com/2015/02/13/anthem-says-data-from-as-far-back-as-2004-exposed-during-hack-offering-free-identity-theft-protection/&#34;&gt;hackers had access &amp;hellip; going back as far as 2004&lt;/a&gt;. WSJ blamed Anthem for &lt;a href=&#34;http://www.wsj.com/articles/investigators-eye-china-in-anthem-hack-1423167560&#34;&gt;not encrypting the data&lt;/a&gt;. Though I have to agree with Rich Mogull over at Securosis that &amp;ldquo;&lt;a href=&#34;https://securosis.com/blog/even-if-anthem-encrypted-it-probably-wouldnt-have-mattered&#34;&gt;even if Anthem had encrypted, it probably wouldn’t have helped&lt;/a&gt;&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;I feel bad for saying this but there&amp;rsquo;s one positive side effect from all these data breaches. Security is now officially a boardroom topic. Anthem&amp;rsquo;s CEO, Joseph Swedish, is now &lt;a href=&#34;http://www.latimes.com/business/la-fi-anthem-hack-ceo-20150213-story.html#page=1&#34;&gt;under the gun&lt;/a&gt; because top level executives are no longer immune to major security breaches that affect the company&amp;rsquo;s top line. Just ask &lt;a href=&#34;http://www.forbes.com/sites/ericbasu/2014/06/15/target-ceo-fired-can-you-be-fired-if-your-company-is-hacked/&#34;&gt;Target’s CEO Gregg Steinhafel&lt;/a&gt;, or &lt;a href=&#34;http://abcnews.go.com/Entertainment/wireStory/sony-chief-amy-pascal-acknowledges-fired-28918607&#34;&gt;Sony&amp;rsquo;s Co-Chairwoman Amy Pascal&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Brian Krebs wrote a &lt;a href=&#34;http://krebsonsecurity.com/2015/02/anthem-breach-may-have-started-in-april-2014/&#34;&gt;detailed piece&lt;/a&gt; analyzing the various pieces of information available relating to the Anthem hack. Quite an interesting read.&lt;/p&gt;

&lt;p&gt;One chart in the artile that Brian referred to is the time difference between the “time to compromise” and the “time to discovery&amp;rdquo;, taken from &lt;a href=&#34;http://www.verizonenterprise.com/DBIR/2014/&#34;&gt;Verizon’s 2014 Data Breach Investigations Report&lt;/a&gt;. As Brian summaries, &amp;ldquo;TL;DR: That gap is not improving, but instead is widening.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;What this really says is that, &lt;strong&gt;you will get hacked&lt;/strong&gt;. So how do you shorten the time between getting hacked, and finding out that you are hacked so you can quickly remediate the problem before worse things happen?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://krebsonsecurity.com/wp-content/uploads/2015/02/timetocompromise.png&#34; alt=&#34;The time difference between the “time to compromise” and the “time to discovery.”&#34; /&gt;
&lt;/p&gt;

&lt;p&gt;With all these data breaches as backdrop, this week we also saw &amp;ldquo;President Barack Obama signed an executive order on Friday designed to spur businesses and the Federal Government to share with each other information related to cybersecurity, hacking and data breaches for the purpose of safeguarding U.S. infrastructure, economics and citizens from cyber attacks.&amp;rdquo; (&lt;a href=&#34;https://gigaom.com/2015/02/13/obamas-executive-order-calls-for-sharing-of-security-data/&#34;&gt;Gigaom&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;In general I don&amp;rsquo;t really think government mandates like this will work. The industry has to feel the pain enough that they are willing to participate, otherwise it&amp;rsquo;s just a waste of paper and ink. Facebook seems to be taking a lead in security information sharing and &lt;a href=&#34;https://www.facebook.com/notes/protect-the-graph/threatexchange-sharing-for-a-safer-internet/1566584370248375&#34;&gt;launched their ThreatExchange security framework&lt;/a&gt; this week. along with Pinterest, Tumblr, Twitter, and Yahoo. Good for them! I hope this is not a temporary PR thing, and that they keep funding and supporting the framework.&lt;/p&gt;

&lt;h2 id=&#34;papers-i-read:10248ddcf524aa32198bc5cded1aa098&#34;&gt;Papers I Read&lt;/h2&gt;

&lt;p&gt;Another great resource of computer science papers is Adrian Coyler&amp;rsquo;s &lt;a href=&#34;http://blog.acolyer.org/&#34;&gt;the morning paper&lt;/a&gt;. He selects and summarizes &amp;ldquo;an interesting/influential/important paper from the world of CS every weekday morning&amp;rdquo;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cs.put.poznan.pl/dweiss/site/publications/download/fsacomp.pdf&#34;&gt;Smaller Representation of Finite State Automata&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I read this paper when I was trying to figure out how to make the FSAs smaller for the &lt;a href=&#34;https://github.com/surge/xparse/tree/master/etld&#34;&gt;Effective TLD matcher&lt;/a&gt; I created. The FSM I generated is 212,294 lines long. That&amp;rsquo;s just absolutely crazy. This paper seems to present an interesting way of compressing them.&lt;/p&gt;

&lt;p&gt;I am not exactly sure if &lt;a href=&#34;https://godoc.org/golang.org/x/net/publicsuffix&#34;&gt;PublicSuffix&lt;/a&gt; uses a similar representation but it basically represents a FSA as an array of bytes, and then walk the bytes like a binary search tree. It&amp;rsquo;s interesting for sure.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This paper is a follow-up to Jan Daciuk’s experiments on space-efficient finite state automata representation that can be used directly for traversals in main memory [4]. We investigate several techniques of reducing the memory footprint of minimal automata, mainly exploiting the fact that transition labels and transition pointer offset values are not evenly distributed and so are suitable for compression. We achieve a size gain of around 20–30% compared to the original representation given in [4]. This result is comparable to the state-of-the-art dictionary compression techniques like the LZ-trie [12] method, but remains
memory and CPU efficient during construction.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://arxiv.org/pdf/1409.5942v1.pdf&#34;&gt;IP Tracing and Active Network Response&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;This work presents integrated model for active security response model. The proposed model introduces Active Response Mechanism (ARM) for tracing anonymous attacks in the network back to their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or “spoofed”, source addresses. This paper presents within the proposed model two tracing approaches based on:
• Sleepy Watermark Tracing (SWT) for unauthorized access attacks.
• Probabilistic Packet Marking (PPM) in the network for Denial of Service
(DoS) and Distributed Denial of Service (DDoS) attacks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36356.pdf&#34;&gt;Dapper, a Large-Scale Distributed Systems Tracing Infrastructure&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Here we introduce the design of Dapper, Google’s production distributed systems tracing infrastructure, and describe how our design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie [3] and X-Trace [12], but certain design
choices were made that have been key to its success in our environment, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.confluent.io/2015/01/29/making-sense-of-stream-processing/&#34;&gt;STREAM PROCESSING, EVENT SOURCING, REACTIVE, CEP… AND MAKING SENSE OF IT ALL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not a paper, but a good write up nonetheless.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Some people call it stream processing. Others call it Event Sourcing or CQRS. Some even call it Complex Event Processing. Sometimes, such self-important buzzwords are just smoke and mirrors, invented by companies who want to sell you stuff. But sometimes, they contain a kernel of wisdom which can really help us design better systems. In this talk, we will go in search of the wisdom behind the buzzwords. We will discuss how event streams can help make your application more scalable, more reliable and more maintainable.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Papers I Read: 2015 Week 6</title>
      <link>http://localhost:1313/blog/papers-i-read-2015-week-6/</link>
      <pubDate>Sun, 08 Feb 2015 19:57:56 -0800</pubDate>
      
      <guid>http://localhost:1313/blog/papers-i-read-2015-week-6/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://paperswelove.org/&#34;&gt;Papers We Love&lt;/a&gt; has been making rounds lately and a lot of people are excited about it. I also think it&amp;rsquo;s kind of cool since I&amp;rsquo;ve been reading a lot of research papers over the past year or so. I have been killing some trees because of that.&lt;/p&gt;

&lt;p&gt;My interests have been mostly around data analytics, but the specific focus areas have changed a few times. I have read papers on data structures (bloom filters, skiplist, bitmap compression, etc), security analytics, consumer behavioral analysis, loyalty analytics, and now back to security analytics. In fact, recently I started reading a few security research papers that I found on &lt;a href=&#34;http://www.covert.io/&#34;&gt;covert.io&lt;/a&gt;, put together by Jason Trost.&lt;/p&gt;

&lt;p&gt;In any case, I thought it might be an interesting idea to share some of the papers I read/scan/skim on weekly basis. This way I can also track what I read over time.&lt;/p&gt;

&lt;h3 id=&#34;random-ramblings:e2137c343047358d9912399632750231&#34;&gt;Random Ramblings&lt;/h3&gt;

&lt;p&gt;This week has been a disaster. I was the last one in the family to catch the cold, but probably lasted the longest. In fact I am still only about 50%. This whole week I have been having headaches, body aches, and haven&amp;rsquo;t been able to concentrate. My body must be trying to catch up on sleep or something. For a couple days I actually slept for almost 12 hours a night!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been meaning to work on &lt;a href=&#34;https://github.com/strace/sequence&#34;&gt;sequence&lt;/a&gt; and finish updating the analyzer, but really had a hard time concentrating. Any non-working hours are basically spent in bed if I could.&lt;/p&gt;

&lt;p&gt;So this is probably the worst week to start the &amp;ldquo;Papers I Read&amp;rdquo; series since I only technically read 1 paper. But I am going to cheat a little, and list the papers I read over the past couple of weeks, pretty much all in my spare time.&lt;/p&gt;

&lt;p&gt;This week we also saw Sony&amp;rsquo;s accouncement that last year&amp;rsquo;s hack cost them &lt;a href=&#34;http://www.sony.net/SonyInfo/IR/financial/fr/150204_sony.pdf&#34;&gt;$15 million&lt;/a&gt; to investigate and remediate. It&amp;rsquo;s pretty crazy if you think about it.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s assume that they hired a bunch of high-priced consultants, say $250/hour, to help comb through the logs and clean the systems. And let&amp;rsquo;s say &lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; of the $15m is spent on these consultants. That&amp;rsquo;s &lt;code&gt;$10m / $250 = 40,000 hours&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say these consultants worked full time, non-stop, no weekends, no breaks, for 2 months since the announcement on Nov 24, 2014, that would be a team of 56 people (&lt;code&gt;40,000 hours / 60 days / 12 hours/day = 56&lt;/code&gt;) working 12 hour days!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll tell ya, these security guys are raking it in. They make money upfront by selling products/services to protect the company, then they make money in the back by selling forensic services to clean up after the hack.&lt;/p&gt;

&lt;p&gt;[Disclaimer: any mistake in my calculations/assumptions I blame on my drugged brain cells.]&lt;/p&gt;

&lt;h3 id=&#34;papers-i-read:e2137c343047358d9912399632750231&#34;&gt;Papers I Read&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/Beehive%20-%20Large-Scale%20Log%20Analysis%20for%20Detecting%20Suspicious%20Activity%20in%20Enterprise%20Networks.pdf&#34;&gt;Beehive: Large-Scale Log Analysis for Detecting Suspicious Activity in Enterprise Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;We present a novel system, Beehive, that attacks the problem of automatically mining and extracting knowledge from the dirty log data produced by a wide variety of security products in a large enterprise. We improve on signature-based approaches to detecting security incidents and instead identify suspicious host behaviors that Beehive reports as potential security incidents.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://minds.cs.umn.edu/publications/chapter.pdf&#34;&gt;Data Mining for Cyber Security&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;This chapter provides an overview of the Minnesota Intrusion Detection System (MINDS), which uses a suite of data mining based algorithms to address different aspects of cyber security. The various components of MINDS such as the scan detector, anomaly detector and the profiling module detect different types of attacks
and intrusions on a computer network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/VAST-%20Network%20Visibility%20Across%20Space%20and%20Time.pdf&#34;&gt;VAST: Network Visibility Across Space and Time&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Key operational networking tasks, such as troubleshooting and defending against attacks, greatly benefit from attaining views of network activity that are unified across space and time. This means that data from heterogeneous devices and systems is treated in a uniformfashion, and that analyzing past activity and detecting future instances follow the same procedures. Based on previous ideas that formulated principles for comprehensive
network visibility [AKP+08], we present the design and architecture of Visibility Across Space and Time (VAST), an intelligent database that serves as a single vantage point into the network. The system is based on a generic event model to handle network data from disparate sources and provides a query architecture that allows operators or remote applications to extract events matching a given condition. We implemented a proof-of-principle prototype that can archive and index events from a wide range of sources. Moreover, we conducted a preliminary performance evaluation to verify that our implementation works efficient and as expected.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.covert.io/research-papers/security/Finding%20The%20Needle-%20Suppression%20of%20False%20Alarms%20in%20Large%20Intrusion%20Detection%20Data%20Sets.pdf&#34;&gt;Finding The Needle: Suppression of False Alarms in Large Intrusion Detection Data Sets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Managed security service providers (MSSPs) must manage and monitor thousands of intrusion detection sensors.
The sensors often vary by manufacturer and software version, making the problem of creating generalized tools to separate true attacks from false positives particularly difficult. Often times it is useful from an operations perspective to know if a particular sensor is acting out of character. We propose a solution to this problem using anomaly detection techniques over the set of alarms produced by the sensors. Similar to the manner in which an anomaly based sensor detects deviations from normal user or system behavior, we establish the baseline
behavior of a sensor and detect deviations from this baseline. We show that departures from this profile by a sensor have a high probability of being artifacts of genuine attacks. We evaluate a set of time-based Markovian heuristics against a simple compression algorithm and show that we are able to detect the existence of all attacks which were manually identified by security personnel, drastically reduce the number of false positives, and identify attacks which were overlooked during manual evaluation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://user.informatik.uni-goettingen.de/~krieck/docs/2013a-aisec.pdf&#34;&gt;A Close Look on n-Grams in Intrusion Detection: Anomaly Detection vs. Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Detection methods based on n-gram models have been widely studied for the identification of attacks and malicious software. These methods usually build on one of two learning schemes: anomaly detection, where a model of normality is constructed from n-grams, or classification, where a discrimination between benign and malicious n-grams is learned. Although successful in many security domains, previous work falls short of explaining why a particular scheme is used and more importantly what renders one favorable over the other for a given type of data. In this paper we provide a close look on n-gram models for intrusion detection. We specifically study anomaly detection and classification using n-grams and develop criteria for data being used in one or the other
scheme. Furthermore, we apply these criteria in the scope of web intrusion detection and empirically validate their effectiveness with different learning-based detection methods for client-side and service-side attacks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.scalyr.com/2014/05/searching-20-gbsec-systems-engineering-before-algorithms/&#34;&gt;Searching 20 GB/sec: Systems Engineering Before Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ok, this is a blog post, not a research paper, but it&amp;rsquo;s somewhat interesting nonetheless.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This article describes how we met that challenge using an “old school”, brute-force approach, by eliminating layers and avoiding complex data structures. There are lessons here that you can apply to your own engineering challenges.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>