<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hashing on Zen 3.1</title>
    <link>http://zhen.org/categories/hashing/</link>
    <description>Recent content in Hashing on Zen 3.1</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Jian Zhen. All Rights Reserved.</copyright>
    <lastBuildDate>Sun, 10 Nov 2013 15:04:22 -0800</lastBuildDate>
    <atom:link href="http://zhen.org/categories/hashing/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Improving Cityhash Performance by Go Profiling</title>
      <link>http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/</link>
      <pubDate>Sun, 10 Nov 2013 15:04:22 -0800</pubDate>
      
      <guid>http://zhen.org/blog/improving-cityhash-performance-by-go-profiling/</guid>
      <description>

&lt;p&gt;Comments/Feedback on &lt;a href=&#34;https://news.ycombinator.com/item?id=6710115&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qcygc/improving_cityhash_performance_by_go_profiling/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2013-11-19 Update #1: After more profiling (see &lt;em&gt;top&lt;/em&gt; output in the &amp;ldquo;After modification&amp;rdquo; section), I&amp;rsquo;ve found that these 3 functions, unalignedLoad64, fetch64, and uint64InExpectedOrder,  add up to quite a bit of execution time. I looked at the &lt;a href=&#34;https://code.google.com/p/cityhash/source/browse/trunk/src/city.cc&#34;&gt;original cityhash implementation&lt;/a&gt; and realized that the combination of these functions is basically geting a LittleEndian uint64, which we can read by doing LittleEndian.Uint64(). So I updated fetch64 to do just that. Performance increased by ~20% just because of that. Also, the bloom filter test showed that cityhash is now faster than both FNV64 and CRC64.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another month has gone by since the last post. This month has been extremely busy at work in an extraordinary good way. We had a huge POC that went quite succesfully at a large customer site. I also got a chance to visit Lisbon, Portugal as part of this POC. So things overall went pretty well.&lt;/p&gt;

&lt;p&gt;However, since family and work pretty much occupied most of my waking hours over the past few weeks, I haven&amp;rsquo;t made much progress on the &amp;ldquo;Go Learn&amp;rdquo; projects. To keep myself going, I picked a smaller task over the weekend and decided to go back to &amp;ldquo;Go Learn Project #1&amp;rdquo;, my &lt;a href=&#34;https://github.com/reducedb/cityhash&#34;&gt;cityhash&lt;/a&gt; Go implementation.&lt;/p&gt;

&lt;h3 id=&#34;go-learn-project-1:c82123e7d16007176a816d99942a250f&#34;&gt;Go Learn Project #1&lt;/h3&gt;

&lt;p&gt;When I first decided to learn Go, I struggled quite a bit to find a project that I can sink my teeth into. I am not sure if others are the same way, for me to learn a new language, I have to have something meaningful to work on. I can&amp;rsquo;t just write hello world programs or follow tutorials. I can read books and articles, but I will also procrastinate for weeks if I can&amp;rsquo;t find a relevant project.&lt;/p&gt;

&lt;p&gt;Luckily, I was thinking about creating a data generator at work and wanted to write that in Go. But in order to write the data generator in Go, I first have to have a cityhash implementation in Go because our backend (C/C++) is using cityhash.&lt;/p&gt;

&lt;p&gt;Surprisingly, I looked around but couldn&amp;rsquo;t find any Go implementation of cityhash. I would have thunk that given Go and Cityhash are both from Google, some Googler would have already ported cityhash over to Go. But no such luck, or maybe I just didn&amp;rsquo;t look hard enough. In any case, I decided to port cityhash over to Go.&lt;/p&gt;

&lt;p&gt;Porting an existing project in C over to Go has a big advantage in that I don&amp;rsquo;t have to invent any new data structures or algorithms. It will allow me to focus on learning the Go syntax and the standard libraries. Many many moons ago (before I converted to the dark side) I was pretty proficient in C and reading cityhash wasn&amp;rsquo;t too difficult, so porting cityhash over to Go should be relatively straightforward.&lt;/p&gt;

&lt;p&gt;In any case, the porting process wasn&amp;rsquo;t too difficult, as it turned out. Overall I was able to do that over a weekend. I was also able to port the test program (city-test.cc) over as well (vim substitution FTW) to validate that my implementation was functionally correct.&lt;/p&gt;

&lt;h3 id=&#34;performance-sucked:c82123e7d16007176a816d99942a250f&#34;&gt;Performance Sucked&lt;/h3&gt;

&lt;p&gt;I had always suspected that my Go cityhash implementation wasn&amp;rsquo;t great performance wise. At the time I hadn&amp;rsquo;t learned how to benchmark or profile Go programs, so I didn&amp;rsquo;t do a whole lot except ensuring functionally the results are correct. Also my data generator at work was working fine so I left the implementation as is.&lt;/p&gt;

&lt;p&gt;In September, I implemented a &lt;a href=&#34;https://github.com/reducedb/bloom&#34;&gt;Bloom Filter package&lt;/a&gt; which required a hash function as part of the implementation. For that package, I &lt;a href=&#34;http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/&#34;&gt;tested different hash functions&lt;/a&gt; to see how they affect the performance of the bloom filter. As you can see from those benchmarks, Cityhash is consistently 3x slower compare to the others. At the time I knew it was because of my implementation but didn&amp;rsquo;t look into it further.&lt;/p&gt;

&lt;h3 id=&#34;profiling-go-cityhash:c82123e7d16007176a816d99942a250f&#34;&gt;Profiling Go Cityhash&lt;/h3&gt;

&lt;p&gt;Since that first project, I have learned quite a bit more about benchmarking and profiling. So this weekend I finally took time to profile the Go implementation and found some interesting results. Now Go experts probably will read this and say &amp;ldquo;of course, you had no idea what you were doing.&amp;rdquo; And that would be true. I had no idea at the time. Hopefully this post will make up for it.&lt;/p&gt;

&lt;p&gt;If you haven&amp;rsquo;t read &lt;a href=&#34;http://blog.golang.org/profiling-go-programs&#34;&gt;this blog post on Go profiling&lt;/a&gt;, you should go read it now before continuing.&lt;/p&gt;

&lt;p&gt;In any case, I wrote a &lt;a href=&#34;https://gist.github.com/zhenjl/7405913&#34;&gt;short program&lt;/a&gt; to test cityhash with a big file. This way it can collect enough samples to tell me where the bottleneck is.&lt;/p&gt;

&lt;p&gt;The original implementation took 45 seconds to hash a 1.1G file. Below is the cpu profile output.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;duration = 45401991546ns
Total: 4295 samples
    2766  64.4%  64.4%     2766  64.4% runtime.memmove
     374   8.7%  73.1%      463  10.8% sweepspan
     259   6.0%  79.1%      383   8.9% MHeap_AllocLocked
     123   2.9%  82.0%      123   2.9% runtime.markspan
      95   2.2%  84.2%     1223  28.5% runtime.mallocgc
      74   1.7%  85.9%       74   1.7% runtime.MSpan_Init
      43   1.0%  86.9%     4234  98.6% github.com/reducedb/cityhash.unalignedLoad64
      40   0.9%  87.9%      595  13.9% runtime.MCache_Alloc
      32   0.7%  88.6%       32   0.7% runtime.markallocated
      31   0.7%  89.3%       56   1.3% MHeap_FreeLocked
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As you can see, most of the time were spent copying memory (64.4%). And also the sweepspan (part of GC) is also running quite often (8.7%).&lt;/p&gt;

&lt;p&gt;Before this, I had no idea that there&amp;rsquo;s that much memory being copied. So this is definitely interesting. I then looked the &lt;a href=&#34;http://zhen.org/images/2013-11-10-improving-cithhash-performance-by-go-profiling/before.svg&#34;&gt;graph of the profile data&lt;/a&gt; using the &amp;ldquo;web&amp;rdquo; command.&lt;/p&gt;

&lt;p&gt;It shows clearly that unalignedLoad64, a function that loads a uint64 from the buffer, is causing most of the memmove. Technically, it&amp;rsquo;s calling binary.Read(), which creates an array of 8 bytes, and passes to another function which eventually calls runtime.copy to copy a few bytes of data from the original buffer into the array.&lt;/p&gt;

&lt;p&gt;So now the reason for the large amount of time spent in memmove is clear. Basically, every time I call binary.Read(), it creates an 8 byte array. Up to 8 bytes of data are copied into it. Then the data in the array gets converted into an uint64. After that, the array is thrown away. And this is done over and over again for the whole 1.1G file, which means 1.1G of memory is being created in tiny 8-byte chunks, copied, and thrown away. It&amp;rsquo;s no wonder the program is slow!&lt;/p&gt;

&lt;p&gt;By this time, some of the readers are probably wondering why the heck I am using binary.Read() if I knew that I will be reading a uint64 from a slice. And they would be right again. Only excuse I have is that I had no clue and that was the first thing I found to work a few months back, so I just used it.&lt;/p&gt;

&lt;h3 id=&#34;modifying-the-implementation:c82123e7d16007176a816d99942a250f&#34;&gt;Modifying the Implementation&lt;/h3&gt;

&lt;p&gt;The change turned out to be relatively simple. Instead of using binary.Read(), I used LittleEndian.Uint64() to read the uint64. After the change, I ran the same program again.&lt;/p&gt;

&lt;p&gt;Here are the results from the post-change run. The time it took to hash the 1.1G file is only 2.8 seconds. That&amp;rsquo;s 16X faster than before the change. The &amp;ldquo;top&amp;rdquo; profile output is also a lot more reasonable.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;duration = 2718339693ns
Total: 245 samples
      57  23.3%  23.3%       57  23.3% encoding/binary.littleEndian.Uint64
      50  20.4%  43.7%      227  92.7% github.com/reducedb/cityhash.CityHash128WithSeed
      40  16.3%  60.0%       97  39.6% github.com/reducedb/cityhash.unalignedLoad64
      35  14.3%  74.3%      146  59.6% github.com/reducedb/cityhash.fetch64
      28  11.4%  85.7%       28  11.4% github.com/reducedb/cityhash.uint64InExpectedOrder
      27  11.0%  96.7%      132  53.9% github.com/reducedb/cityhash.weakHashLen32WithSeeds_3
       8   3.3% 100.0%        8   3.3% github.com/reducedb/cityhash.weakHashLen32WithSeeds
       0   0.0% 100.0%        1   0.4% MHeap_AllocLarge
       0   0.0% 100.0%      227  92.7% _/Users/jian/Projects/cityhash_test.TestLatencyIntegers
       0   0.0% 100.0%      227  92.7% github.com/reducedb/cityhash.CityHash128
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also, nothing really jumps out when looking at the &lt;a href=&#34;http://zhen.org/images/2013-11-10-improving-cithhash-performance-by-go-profiling/after.svg&#34;&gt;post-change profile data graph&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;bloom-filter-benchmarks:c82123e7d16007176a816d99942a250f&#34;&gt;Bloom Filter Benchmarks&lt;/h3&gt;

&lt;p&gt;Now that the changes are in, I went back and re-ran some of the bloom filter benchmarks. They look a lot more reasonable as well. Below is a comparison of the Scalable Bloom Filter. The post-change run is almost 2.5x faster than the pre-change run. Also, the post-change number (1442 ns/op) is a lot closer to some of the other hash functions (~1100 ns/op).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Scalable Bloom Filter
---------------------
BenchmarkBloomCityHash   1000000              1442 ns/op (after cityhash change)
BenchmarkBloomCityHash   1000000              3375 ns/op (before cityhash change)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;summary:c82123e7d16007176a816d99942a250f&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;The Go authors have made it extermely simple to test, benchmark and profile Go programs so there&amp;rsquo;s really reason for anyone not to do that often. It helps you see how your program works and where the bottlenecks are. It can also help you identify surprises that you may not have though of. A good example is in my case, I had no idea binary.Read() works the way it works until I profiled my program.&lt;/p&gt;

&lt;p&gt;Comments/Feedback on &lt;a href=&#34;https://news.ycombinator.com/item?id=6710115&#34;&gt;Hacker News&lt;/a&gt;, &lt;a href=&#34;http://www.reddit.com/r/golang/comments/1qcygc/improving_cityhash_performance_by_go_profiling/&#34;&gt;Reddit&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Benchmarking Bloom Filters and Hash Functions in Go</title>
      <link>http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/</link>
      <pubDate>Wed, 04 Sep 2013 00:23:22 -0800</pubDate>
      
      <guid>http://zhen.org/blog/benchmarking-bloom-filters-and-hash-functions-in-go/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6329616&#34;&gt;discussion/comments&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Update: @bradfitz &lt;a href=&#34;https://news.ycombinator.com/item?id=6329616&#34;&gt;commented&lt;/a&gt; that allocating a new slice during each add/check is probably not good for performance. And he is in fact correct. After removing the allocation for each add/check and doing a single slice make during New(), the performance increased by ~27%!! Here&amp;rsquo;s the gist containing the new ns/op results.&lt;/p&gt;

&lt;p&gt;{% gist 6515577 %}&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Another week, another &amp;ldquo;Go Learn&amp;rdquo; project. This time, in project #4, I implemented a &lt;a href=&#34;https://github.com/reducedb/bloom&#34;&gt;Go package&lt;/a&gt; with several bloom filters and &lt;a href=&#34;https://gist.github.com/zhenjl/6433634&#34;&gt;benchmarked&lt;/a&gt; them with various hash functions. (For previous projects, see &lt;a href=&#34;http://zhen.org/blog/go-skiplist/&#34;&gt;#3&lt;/a&gt;, &lt;a href=&#34;http://zhen.org/blog/testing-msgpack-and-bson/&#34;&gt;#2&lt;/a&gt;, &lt;a href=&#34;https://github.com/reducedb/cityhash&#34;&gt;#1&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;The goal of this project, for me at least, is to implement a pure Go package that doesn&amp;rsquo;t rely on wrappers to other langagues. There&amp;rsquo;s already &lt;a href=&#34;https://github.com/search?l=Go&amp;amp;q=bloom&amp;amp;ref=cmdform&amp;amp;type=Repositories&#34;&gt;quite a few&lt;/a&gt; bloom filters implemented in Go. But hey, in the name of learning, why not implement another one!&lt;/p&gt;

&lt;h3 id=&#34;bloom-filters:75bc04a36b7f857175f963f46bbada5d&#34;&gt;Bloom Filters&lt;/h3&gt;

&lt;p&gt;Wikipedia says,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A Bloom filter, conceived by Burton Howard Bloom in 1970 is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not; i.e. a query returns either &amp;ldquo;inside set (may be wrong)&amp;rdquo; or &amp;ldquo;definitely not in set&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In my little project, I implemented the following three variants:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Standard - This is the classic bloom filter as described on &lt;a href=&#34;http://en.wikipedia.org/wiki/Bloom_filter&#34;&gt;Wikipedia&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Partitioned - This is a variant described by &lt;a href=&#34;http://www.ieee-infocom.org/2004/Papers/45_3.PDF&#34;&gt;these&lt;/a&gt; &lt;a href=&#34;http://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf&#34;&gt;papers&lt;/a&gt;. Basically instead of having a single big bit array, partitioned bloom filter breaks it into &lt;em&gt;k&lt;/em&gt; partitions (or slices) s.t. each partition is &lt;em&gt;m/k&lt;/em&gt; size, where &lt;em&gt;m&lt;/em&gt; is the total number of bits, and &lt;em&gt;k&lt;/em&gt; is the number of hashes. Then each hash function is assigned to a single partition.&lt;/li&gt;
&lt;li&gt;Scalable - This is yet another variant described by &lt;a href=&#34;http://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf&#34;&gt;here&lt;/a&gt;). The idea is that the standard bloom filter requires that you know &lt;em&gt;m&lt;/em&gt;, or the size of the filter a priori. This is not possible for use cases where data continue to come in without bound. So the Scalable bloom filter utilizes multiple bloom filters, each with incresing k, but decreasing &lt;em&gt;P&lt;/em&gt; where &lt;em&gt;P&lt;/em&gt; is the desired error probability. This bloom filter also introduces &lt;em&gt;r&lt;/em&gt;, which is the error tightening ratio, and it&amp;rsquo;s 0 &amp;lt; r &amp;lt; 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are a ton more variants for bloom filters. You can raed all about them in &lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=5&amp;amp;cad=rja&amp;amp;ved=0CE4QFjAE&amp;amp;url=http%3A%2F%2Fwww.tribler.org%2Ftrac%2Fraw-attachment%2Fwiki%2FJelleRoozenburg%2Fresearch_assignment_jroozenburg_20051108.pdf&amp;amp;ei=JHInUt6oO6GTiQK-44DYDg&amp;amp;usg=AFQjCNG057WVJ2m2QYPuqWzCZ0Vn4JnOug&amp;amp;sig2=NVlad7xGO4S_hFpMU9apGA&amp;amp;bvm=bv.51773540,d.cGE&#34;&gt;this paper&lt;/a&gt; and &lt;a href=&#34;http://www.dca.fee.unicamp.br/~chesteve/pubs/bloom-filter-ieee-survey-preprint.pdf&#34;&gt;this paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;hash-functions:75bc04a36b7f857175f963f46bbada5d&#34;&gt;Hash Functions&lt;/h3&gt;

&lt;p&gt;To add an element, bloom filters hashes the element using &lt;em&gt;k&lt;/em&gt; hashing functions, identifying &lt;em&gt;k&lt;/em&gt; positions in the bit array, and setting those bits to 1. To check for an element, you essentially do the same thing (hash the element with &lt;em&gt;k&lt;/em&gt; hash functions) and then check to see if all the bits at those positions are set. If all set, then most likely the element is available. If any of them are not set, then the element is definitely not avaiable. So bloom filters can have false positives, but not false negatives.&lt;/p&gt;

&lt;p&gt;However, actually running &lt;em&gt;k&lt;/em&gt; hash functions is quite expensive. So &lt;a href=&#34;http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/rsa.pdf&#34;&gt;Kirsch and Mitzenmacher&lt;/a&gt; determined that by using a single hash function, but using the formula, &lt;em&gt;gi(x) = h1(x) + i * h2(x)&lt;/em&gt;, to calculate &lt;em&gt;k&lt;/em&gt; hash values, the performance is actually similar. So this is what I used here.&lt;/p&gt;

&lt;p&gt;For the values &lt;em&gt;h1&lt;/em&gt; and &lt;em&gt;h2&lt;/em&gt;, we used the first 4 bytes returned from each hash function for &lt;em&gt;h1&lt;/em&gt;, and second 4 bytes for &lt;em&gt;h2&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The following hash functions are used for this little experiement:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FNV-64 - There&amp;rsquo;s a built-in Go package for this. So that&amp;rsquo;s what I used.&lt;/li&gt;
&lt;li&gt;CRC-64 - Again, using the built-in Go package.&lt;/li&gt;
&lt;li&gt;Murmur3-64 - There&amp;rsquo;s no built-in package so I used &lt;a href=&#34;https://github.com/spaolacci/murmur3&#34;&gt;this one&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CityHash-64 - Again, no built-in so I am using the one &lt;a href=&#34;https://github.com/reducedb/cityhash&#34;&gt;I implemented&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;MD5 - Using the built-in Go implementation.&lt;/li&gt;
&lt;li&gt;SHA1 - Using the built-in Go implementation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a side note, MD5 and SHA1 return 128 bit hash values. Since we only use the first 64 bits, we throw away the last 64 bits.&lt;/p&gt;

&lt;h3 id=&#34;test-setup:75bc04a36b7f857175f963f46bbada5d&#34;&gt;Test Setup&lt;/h3&gt;

&lt;p&gt;The machine that ran these tests is a Macbook Air 10.8.4 1.8GHz Intel Core i5 4GB 1600MHz DDR3.&lt;/p&gt;

&lt;p&gt;For the false positive test, I added all the words in /usr/share/dict/web2 (on my Mac) into each of the bloom filters. To check for false positives, I check for all the words in /usr/share/dict/web2a in each of the bloom filters. The two files should have completely different set of words (AFAICT).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  235886 /usr/share/dict/web2
   76205 /usr/share/dict/web2a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each bloom filter, I ran tests using the following combinations:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i in element_count[236886, 200000, 100000, 50000]
    for j in hash_functions[fnv64, crc64, murmur3-64, cityhash-64, md5, sha1]
        run test with hash_functions[j] and element_count[i]
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Element count in this is the initial size I set for the bloom filter. The bit array size, &lt;em&gt;m&lt;/em&gt;, and number of hash values, &lt;em&gt;k&lt;/em&gt;, are then calculated from there. I also set &lt;em&gt;P&lt;/em&gt; (error probability) to 0.001 (0.1%) and &lt;em&gt;p&lt;/em&gt; (fill ratio, or how full should the bit array be) to 0.5. The idea for the element count is to see how the bloom filters will perform when it has a high fill ratio.&lt;/p&gt;

&lt;p&gt;For the scalable bloom filter test, I also needed to add another dimension since it uses either standard or partitioned bloom filter internally. So&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for i in element_count[236886, 200000, 100000, 50000]
    for j in hash_functions[fnv64, crc64, murmur3-64, cityhash-64, md5, sha1]
        for k in bloom_filter[standard, partitioned]
            run test with hash_functions[j] and element_count[i] and bloom_filter[k]
        end
    end
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the performance test, I added all the words in web2 into the bloom filters. I continue to loop through the file until b.N (Go benchmark framework) is met. So some of the words will be re-added, which should not skew our test results since the operations are the same.&lt;/p&gt;

&lt;p&gt;You can see the tests in the &lt;a href=&#34;https://github.com/reducedb/bloom&#34;&gt;github repo&lt;/a&gt;. Just look for all the _test.go files.&lt;/p&gt;

&lt;h3 id=&#34;test-results:75bc04a36b7f857175f963f46bbada5d&#34;&gt;Test Results&lt;/h3&gt;

&lt;p&gt;The following is a summary of the test results. You can also feel free to look at all the &lt;a href=&#34;https://gist.github.com/zhenjl/6433634&#34;&gt;gory details&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note: FR = Fill Ratio, FP = False Positive&lt;/p&gt;

&lt;p&gt;For the spreadsheet embedded below, here are some observations&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The MD5 Go implementation I used maybe broken, or I am not using it correctly, or MD5 is just bad. You will see that the fill ration is VERY low, 1-6%. So the false positive rate is very high (89+%).&lt;/li&gt;
&lt;li&gt;The CityHash Go implementation seems very slow. Could just be my implementation (anyone want to venture some optimization?). But functionally it&amp;rsquo;s correct.&lt;/li&gt;
&lt;li&gt;Both standard and partitioned bloom filters use the same number of bits and there&amp;rsquo;s not a huge difference in fill ratio and false positive rate. (Ignoring the MD5 anomaly.)&lt;/li&gt;
&lt;li&gt;Predictably, as fill ratio goes up, so does the false positive rate for both standard and partitioned bloom filters.&lt;/li&gt;
&lt;li&gt;Scalable bloom filter uses more bits as it contines to add new base bloom filters when the estimated fill ratio reaches &lt;em&gt;P&lt;/em&gt; which is set to 0.5 (50%).&lt;/li&gt;
&lt;li&gt;Probably not surprisingly, Scalable bloom filter maintains a fairly low false positive rate.&lt;/li&gt;
&lt;li&gt;However, you will also notice that the Scalable FP increases as the total number of base filters increase. This suggests that I may want to try a lower &lt;em&gt;r&lt;/em&gt; (error tightening ratio). Currently I use 0.9, but maybe 0.8 is more appropriate.&lt;/li&gt;
&lt;li&gt;Overall it seems FNV is probably good enough for most scenarios.&lt;/li&gt;
&lt;li&gt;Also Scalable+Standard might be a good choice for anyone doing stream data processing.&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe width=&#39;800&#39; height=&#39;600&#39; frameborder=&#39;0&#39; src=&#39;https://docs.google.com/spreadsheet/pub?key=0ApDLtJuUH-1rdEEwZ3JwaVVTZGxBX1g1NkthSFVVTXc&amp;single=true&amp;gid=0&amp;output=html&amp;widget=true&#39;&gt;&lt;/iframe&gt;

&lt;p&gt;This chart shows the performance (ns/op) for adding elements to the bloom filters. Overall the performance is very similar for the different bloom filter implementations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://docs.google.com/spreadsheet/oimg?key=0ApDLtJuUH-1rdEEwZ3JwaVVTZGxBX1g1NkthSFVVTXc&amp;oid=1&amp;zx=jbizsaoepc5x&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;feedback:75bc04a36b7f857175f963f46bbada5d&#34;&gt;Feedback&lt;/h3&gt;

&lt;p&gt;Feel free to send me any feedback via &lt;a href=&#34;https://github.com/reducedb/bloom/issues&#34;&gt;github issues&lt;/a&gt; or on &lt;a href=&#34;https://news.ycombinator.com/item?id=6329616&#34;&gt;Hacker News&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;references:75bc04a36b7f857175f963f46bbada5d&#34;&gt;References&lt;/h3&gt;

&lt;p&gt;During this process I referenced and leveraged some of these other projects, so thank you all!&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Referenced

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/willf/bloom&#34;&gt;willf/bloom&lt;/a&gt; - Go package implementing Bloom filters&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bitly/dablooms&#34;&gt;bitly/dablooms&lt;/a&gt; - scaling, counting, bloom filter library&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s also a bunch of papers, some of which I linked above.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Leveraged

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/willf/bitset&#34;&gt;willf/bitset&lt;/a&gt; - Go package implementing bitsets&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spaolacci/murmur3&#34;&gt;spaolacci/murmur3&lt;/a&gt; - Native MurmurHash3 Go implementation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=6329616&#34;&gt;discussion/comments&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>