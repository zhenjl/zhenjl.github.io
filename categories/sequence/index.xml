<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Sequence on Zen 3.1 </title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://zhen.org/categories/sequence/index.xml/</link>
    <language>en-us</language>
    <author>Jian Zhen</author>
    <copyright>Jian Zhen</copyright>
    <updated>Mon, 05 Jan 2015 22:40:20 PST</updated>
    
    <item>
      <title>Sequence: A High Performance Sequential Semantic Log Analyzer Parser</title>
      <link>http://zhen.org/blog/sequence-high-performance-sequential-semantic-log-analyzer-parser/</link>
      <pubDate>Mon, 05 Jan 2015 22:40:20 PST</pubDate>
      <author>Jian Zhen</author>
      <guid>http://zhen.org/blog/sequence-high-performance-sequential-semantic-log-analyzer-parser/</guid>
      <description>

&lt;p&gt;&lt;code&gt;sequence&lt;/code&gt; is a &lt;em&gt;high performance sequential semantic log message analyzer and parser&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;It is &lt;em&gt;sequential&lt;/em&gt; because it goes through a log message sequentially and does not use regular expressions.&lt;/li&gt;
&lt;li&gt;It is &lt;em&gt;semantic&lt;/em&gt; because it tries to extract meaningful information out of the log messages and give them semantic indicators, e.g., src IPv4 or dst IPv4.&lt;/li&gt;
&lt;li&gt;It is an &lt;em&gt;analyzer&lt;/em&gt; because analyzes a large corpus of text-based log messages and try to determine the unique patterns that would represent all of them.&lt;/li&gt;
&lt;li&gt;It is a &lt;em&gt;parser&lt;/em&gt; because it will take a message and parses out the meaningful parts.&lt;/li&gt;
&lt;li&gt;It is &lt;em&gt;high performance&lt;/em&gt; because it can parse 100K+ messages per second without the need to separate parsing rules by log source type.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;code&gt;sequence&lt;/code&gt; is currently under active development and should be considered unstable until further notice.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;Motivation&lt;/h3&gt;

&lt;p&gt;Log messages are notoriusly difficult to parse because they all have different formats. Industries (see Splunk, ArcSight, Tibco LogLogic, Sumo Logic, Logentries, Loggly, LogRhythm, etc etc etc) have been built to solve the problems of parsing, understanding and analyzing log messages.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say you have a bunch of log files you like to parse. The first problem you will typically run into is you have no way of telling how many DIFFERENT types of messages there are, so you have no idea how much work there will be to develop rules to parse all the messages. Not only that, you have hundreds of thousands, if not  millions of messages, in front of you, and you have no idea what messages are worth parsing, and what&amp;rsquo;s not.&lt;/p&gt;

&lt;p&gt;The typical workflow is develop a set of regular expressions and keeps testing against the logs until some magical moment where all the logs you want parsed are parsed. Ask anyone who does this for a living and they will tell you this process is long, frustrating and error-prone.&lt;/p&gt;

&lt;p&gt;Even after you have developed a set of regular expressions that match the original set of messages, if new messages come in, you will have to determine which of the new messages need to be parsed. And if you develop a new set of regular expressions to parse those new messages, you still have no idea if the regular expressions will conflict with the ones you wrote before. If you write your regex parsers too liberally, it can easily parse the wrong messages.&lt;/p&gt;

&lt;p&gt;After all that, you will end up finding out the regex parsers are quite slow. It can typically parse several thousands messages per second. Given enough CPU resources on a large enough machine, regex parsers can probably parse tens of thousands of messages per second. Even to achieve this type of performance, you will likely need to limit the number of regular expressions the parser has. The more regex rules, the slower the parser will go.&lt;/p&gt;

&lt;p&gt;To work around this performance issue, companies have tried to separate the regex rules for different log message types into different parsers. For example, they will have a parser for Cisco ASA logs, a parser for sshd logs, a parser for Apache logs, etc etc. And then they will require the users to tell them which parser to use (usually by indicating the log source type of the originating IP address or host.)&lt;/p&gt;

&lt;p&gt;Sequence is developed to make analyzing and parsing log messages a lot easier and faster.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Performance&lt;/h3&gt;

&lt;p&gt;The following performance benchmarks are run on a single 4-core (2.8Ghz i7) MacBook Pro. The first file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA log file, averaging 180 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ go version
  go version go1.4 darwin/amd64

  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 2.65 secs, ~ 80449.93 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 4.42 secs, ~ 53081.36 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.52 secs, ~ 140139.27 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 2.51 secs, ~ 93614.09 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;License&lt;/h3&gt;

&lt;p&gt;Copyright &amp;copy; 2014 Dataence, LLC. All rights reserved.&lt;/p&gt;

&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &amp;ldquo;License&amp;rdquo;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &amp;ldquo;AS IS&amp;rdquo; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Roadmap / Futures&lt;/h3&gt;

&lt;p&gt;There are some pattern files developed for ASA, Sudo and SSH in the &lt;code&gt;patterns&lt;/code&gt; directory. The goal is to continue to develop a set of patterns for the various log messages, and along the way add additional features to the parser that can help make it even easier to parse log messages. So currently there&amp;rsquo;s not a set roadmap.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Concepts&lt;/h2&gt;

&lt;p&gt;The following concepts are part of the package:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A &lt;em&gt;Token&lt;/em&gt; is a piece of information extracted from the original log message. It is a struct that contains fields for &lt;em&gt;TokenType&lt;/em&gt;, &lt;em&gt;FieldType&lt;/em&gt;, &lt;em&gt;Value&lt;/em&gt;, and indicators of whether it&amp;rsquo;s a key or value in the key=value pair.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;TokenType&lt;/em&gt; indicates whether the token is a literal string (one that does not change), a variable string (one that could have different values), an IPv4 or IPv6 address, a MAC address, an integer, a floating point number, or a timestamp.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;FieldType&lt;/em&gt; indicates the semantic meaning of the token. For example, a token could be a source IP address (%srcipv4%), or a user (%srcuser% or %dstuser%), an action (%action%) or a status (%status%).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Sequence&lt;/em&gt; is a list of Tokens. It is returned by the &lt;em&gt;Scanner&lt;/em&gt;, the &lt;em&gt;Analyzer&lt;/em&gt;, and the &lt;em&gt;Parser&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Scanner&lt;/em&gt; is a sequential lexical analyzer that breaks a log message into a sequence of tokens. It is sequential because it goes through log message sequentially tokentizing each part of the message, without the use of regular expressions. The scanner currently recognizes time stamps, IPv4 addresses, URLs, MAC addresses,
integers and floating point numbers. It also recgonizes key=value or key=&amp;ldquo;value&amp;rdquo; or key=&amp;lsquo;value&amp;rsquo; or key=&lt;value&gt; pairs.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Analyzer&lt;/em&gt; builds an analysis tree that represents all the Sequences from messages. It can be used to determine all of the unique patterns for a large body of messages.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A &lt;em&gt;Parser&lt;/em&gt; is a tree-based parsing engine for log messages. It builds a parsing tree based on pattern sequence supplied, and for each message sequence, returns the matching pattern sequence. Each of the message tokens will be marked with the semantic field types.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Sequence Command&lt;/h2&gt;

&lt;p&gt;The typical workflow of using sequence is to first analyze all of the log messages to determine the unique patterns. This could easily reduce millions of log messages down to maybe 30-50 formats.&lt;/p&gt;

&lt;p&gt;Then the analyst can look through these formats and annotate the patterns with the semantic meanings. Once that&amp;rsquo;s done, the analyst can run the parser with these annotated rules and outcome the parsed tokens.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;sequence&lt;/code&gt; command is developed to demonstrate the use of this package. You can find it in the cmd/sequence directory.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   Usage:
     sequence [command]

   Available Commands:
     scan                      scan will tokenize a log file or message and output a list of tokens
     analyze                   analyze will analyze a log file and output a list of patterns that will match all the log messages
     parse                     parse will parse a log file and output a list of parsed tokens for each of the log messages
     bench                     benchmark the parsing of a log file, no output is provided
     help [command]            Help about any command
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;Scan&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence scan [flags]

   Available Flags:
    -h, --help=false: help for scan
    -m, --msg=&amp;quot;&amp;quot;: message to tokenize
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence scan -m &amp;quot;jan 14 10:15:56 testserver sudo:    gonner : tty=pts/3 ; pwd=/home/gonner ; user=root ; command=/bin/su - ustream&amp;quot;
  #   0: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 14 10:15:56&amp;quot; }
  #   1: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;testserver&amp;quot; }
  #   2: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sudo&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   4: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;gonner&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;tty&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pts/3&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;pwd&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/home/gonner&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  14: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  15: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;root&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;;&amp;quot; }
  #  18: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;command&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;/bin/su&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;-&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;ustream&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;Analyze&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence analyze [flags]

   Available Flags:
    -h, --help=false: help for analyze
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -o, --outfile=&amp;quot;&amp;quot;: output file, if empty, to stdout
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used, optional
    -p, --patfile=&amp;quot;&amp;quot;: initial pattern file, optional
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command analyzes a set of sshd log messages, and output the
patterns to the sshd.pat file. In this example, &lt;code&gt;sequence&lt;/code&gt; analyzed over 200K
messages and found 45 unique patterns. Notice we are not supplying an existing
pattern file, so it treats all the patters as new.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence analyze -i ../../data/sshd.all  -o sshd.pat
  Analyzed 212897 messages, found 45 unique patterns, 45 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the output file has entries such as:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  %ts% %string% sshd [ %integer% ] : %string% ( sshd : %string% ) : session %string% for user %string% by ( uid = %integer% )
  # Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the following command, we added an existing pattern file to the mix, which has
a set of existing rules. Notice now there are only 35 unique patterns, and we were
able to parse all of the log messages (no new patterns). There are fewer patterns
because some of the patterns were combined.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence analyze -d ../../patterns -i ../../data/sshd.all  -o sshd.pat
  Analyzed 212897 messages, found 35 unique patterns, 0 are new.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The same log message we saw above now has an entry like the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  %createtime% %apphost% %appname% [ %sessionid% ] : %string% ( sshd : %string% ) : %object% %action% for user %dstuser% by ( uid = %integer% )
  # Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_8&#34;&gt;Parse&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence parse [flags]

   Available Flags:
    -h, --help=false: help for parse
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -o, --outfile=&amp;quot;&amp;quot;: output file, if empty, to stdout
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&amp;quot;&amp;quot;: initial pattern file, required
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command parses a file based on existing rules. Note that the
performance number (9570.20 msgs/sec) is mostly due to reading/writing to disk.
To get a more realistic performance number, see the benchmark section below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence parse -d ../../patterns -i ../../data/sshd.all  -o parsed.sshd
  Parsed 212897 messages in 22.25 secs, ~ 9570.20 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is an entry from the output file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  Jan 15 19:39:26 jlz sshd[7778]: pam_unix(sshd:session): session opened for user jlz by (uid=0)
  #   0: { Field=&amp;quot;%createtime%&amp;quot;, Type=&amp;quot;%ts%&amp;quot;, Value=&amp;quot;jan 15 19:39:26&amp;quot; }
  #   1: { Field=&amp;quot;%apphost%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;jlz&amp;quot; }
  #   2: { Field=&amp;quot;%appname%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;sshd&amp;quot; }
  #   3: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;[&amp;quot; }
  #   4: { Field=&amp;quot;%sessionid%&amp;quot;, Type=&amp;quot;%integer%&amp;quot;, Value=&amp;quot;7778&amp;quot; }
  #   5: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;]&amp;quot; }
  #   6: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #   7: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;pam_unix&amp;quot; }
  #   8: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;(&amp;quot; }
  #   9: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;sshd&amp;quot; }
  #  10: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #  11: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;session&amp;quot; }
  #  12: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;)&amp;quot; }
  #  13: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;:&amp;quot; }
  #  14: { Field=&amp;quot;%object%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;session&amp;quot; }
  #  15: { Field=&amp;quot;%action%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;opened&amp;quot; }
  #  16: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;for&amp;quot; }
  #  17: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;user&amp;quot; }
  #  18: { Field=&amp;quot;%dstuser%&amp;quot;, Type=&amp;quot;%string%&amp;quot;, Value=&amp;quot;jlz&amp;quot; }
  #  19: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;by&amp;quot; }
  #  20: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;(&amp;quot; }
  #  21: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;uid&amp;quot; }
  #  22: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;=&amp;quot; }
  #  23: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%integer%&amp;quot;, Value=&amp;quot;0&amp;quot; }
  #  24: { Field=&amp;quot;%funknown%&amp;quot;, Type=&amp;quot;%literal%&amp;quot;, Value=&amp;quot;)&amp;quot; }
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_9&#34;&gt;Benchmark&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;  Usage:
    sequence bench [flags]

   Available Flags:
    -c, --cpuprofile=&amp;quot;&amp;quot;: CPU profile filename
    -h, --help=false: help for bench
    -i, --infile=&amp;quot;&amp;quot;: input file, required
    -d, --patdir=&amp;quot;&amp;quot;: pattern directory,, all files in directory will be used
    -p, --patfile=&amp;quot;&amp;quot;: pattern file, required
    -w, --workers=1: number of parsing workers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The following command will benchmark the parsing of two files. First file is a
bunch of sshd logs, averaging 98 bytes per message. The second is a Cisco ASA
log file, averaging 180 bytes per message.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  $ ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all
  Parsed 212897 messages in 2.65 secs, ~ 80449.93 msgs/sec

  $ ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log
  Parsed 234815 messages in 4.42 secs, ~ 53081.36 msgs/sec
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Performance can be improved by adding more cores:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  GOMAXPROCS=2 ./sequence bench -p ../../patterns/sshd.txt -i ../../data/sshd.all -w 2
  Parsed 212897 messages in 1.52 secs, ~ 140139.27 msgs/sec

  $ GOMAXPROCS=2 ./sequence bench -p ../../patterns/asa.txt -i ../../data/allasa.log -w 2
  Parsed 234815 messages in 2.51 secs, ~ 93614.09 msgs/sec
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>